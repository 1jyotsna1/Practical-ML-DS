{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVED TEXT\n",
    "\n",
    "## Unit-vector representation\n",
    "Since we can add, subtract and scale vectors, we can represent any vector in vector space as the sum of its unit vectors, each multiplied with their own scalar. This gives us another mean of representing vector. In the 2-D case, the $x_{1}$ component is represented by a scalar multiple of $\\hat{\\mathbf{i}}$, and the $x_{2}$ component is represented by a scalar multiple of $\\hat{\\mathbf{j}}$. For example, the two representations below correspond to the same vector:\n",
    "\n",
    "$$\\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} = 2\\mathbf{\\hat{i}} +  3\\mathbf{\\hat{j}}$$\n",
    "\n",
    "These concepts in scalar multiplication extend to the N-D case. In general, the unit-vector representation becomes innefficient in representing high-dimensional vectors.\n",
    "\n",
    "\n",
    "## Geometric Inner Product\n",
    "This becomes even clearer when we use the geometric definition of the innner produt, also applicable to vectors of any dimensions:\n",
    "\n",
    "$$\\mathbf{\\vec{x}}\\cdot \\mathbf{\\vec{y}} = |\\mathbf{\\vec{x}}||\\mathbf{\\vec{y}}|\\cos(\\theta)$$\n",
    "\n",
    "Both the algebraic and geometric definition are equivalent. We can now define different cases: \n",
    "- If $\\theta = 0$, the vectors are perfectly alligned and have the same direction, meaning the vectors are __parallel__. This is always the orientation that returns the __greatest__ inner product.\n",
    "- If  $\\theta = \\pi$, the vectors have exactly the opposite orientation, meaning the vectors are __anti-parallel__. This is always the orientation that returns the __lowest__ inner product.\n",
    "- If $\\theta =  \\frac{\\pi}{2}$, the vectors are __orthogonal__, meaning the inner product is 0.\n",
    "\n",
    "Sometimes, parallel and anti-parallel vectors are referred to within the same category: __collinear__.\n",
    "\n",
    "Another important property to understand, which some of you may have already realised from the diagrams above, is that if two vectors are parallel, they are positive scalar multiples of each other, whereas if two vectors are anti-parallel, they are negative scalar multiples of each other. This also means collinear vectors share the same unit vector! We can generalize this notion as shown below, where if $\\vec{\\mathbf{x}}$ and $\\vec{\\mathbf{y}}$ are collinear:\n",
    "\n",
    "$$ \\vec{\\mathbf{x}} = a\\vec{\\mathbf{y}} $$\n",
    "for a given scalar, a, where:\n",
    "- the vectors are parallel if a>0\n",
    "- the vectors are anti-parallel if a<0\n",
    "\n",
    "We now have simple means of classification of a pair of vectors. If their inner-product is 0, the two vectors are orthogonal. If one is the scalar multiple of the other, it is either paralle or anti-parallel, depending on the sign of the scalar. Another distinction between a pair/set of vectors is that, if all vectors in a set are orthogonal and have unit length, then they are further classified __orthonormal vectors__.\n",
    "\n",
    "\n",
    "## Cosine Similarity\n",
    "We now understand that the inner-product of two vectors represents how similar two vectors are, proportionally to their magnitudes. But what if we are uninterested in their magnitudes, and only interested in their allignment? Well, now we know that every vector has a respective unit vector to represent its direction with a magnitude of 1. To calculate the unit vector, we can simply divide the vector by its length. So, to calculate the similarity of two vectors, we can simply calculate the inner product of their unit vectors. A neat trick is that when we combine the geometric and algebraic definitions of the inner-product, we get what is known as __cosine similarity__:\n",
    "\n",
    "$$\\text{similarity} = \\cos(\\theta) = \\frac{\\vec{\\mathbf{x}}\\cdot \\vec{\\mathbf{y}}} {||\\vec{\\mathbf{x}}|||\\vec{\\mathbf{y}}||}$$\n",
    "\n",
    "Meaning that the cosine of the angle between the two vectors is a useful measure of allingment of two vectors irrespective of their lengths, where:\n",
    "- $\\cos(\\theta) = 1 $: parallel vectors\n",
    "- $\\cos(\\theta) = -1 $: anti-parallel vectors\n",
    "- $\\cos(\\theta) = 0 $: orthogonal vectors\n",
    "\n",
    "\n",
    "## Vector Norms\n",
    "Before, to calculate the length, or magnitude of a vector, we calculated what is known as __Euclidian distance__, which is the most common metric for vector magnitude. However, there are infinitely many other ways to measure the length of a vector, known as __vector norms__. Generally, for an N-D vector, we define vector norms as:\n",
    "\n",
    "$$||\\vec{\\mathbf{x}}||_{p} = \\left[ \\sum_{i=1}^{N}|x_{i}|^{p} \\right] ^{\\frac{1}{p}}$$\n",
    "\n",
    "Where $p=1,2,3,...,\\infty$.\n",
    "\n",
    "As you can probably tell from the definition, Euclidian distance is formally known as the 2-norm, denoted as $||\\mathbf{\\vec{x}}||_{2}$. However, since it is so common, we normally drop the '2'. Sometimes, the 1-norm is also used, although less frequently. So what are the differences between the different norms? By looking at the general equation for the p-norm, we see that we take the $p^{th}$ root of the sum of the vector values to the power of p. This means that the higher the value of p, the less weight smaller valued components will take. In fact, when we reach the $\\infty$-norm, we simply get the largest component value of our vector.\n",
    "\n",
    "Below, we compute some vector norms in Standard Python and in NumPy. As we cannot physically compute the $\\infty$-norm, we will approximate it with a large enough value for p.\n",
    "\n",
    "\n",
    "\n",
    "## Gram Schmidt Orthonormalization Process Intuition\n",
    "\n",
    "### make our own version of this diagrams (this one has the right idea but isn't _exactly_ what we want)!\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Gram%E2%80%93Schmidt_process.svg/1280px-Gram%E2%80%93Schmidt_process.svg.png\" width=\"500px\" height =\"500px\">\n",
    "\n",
    "The goal is to create a vector orthogonal to the normalized one ($\\mathbf{\\hat{e_{1}}}$) based on another vector ($\\mathbf{\\vec{v_{2}}}$). This process works because the the inner-product gives the magnitude of the projection of one vector onto another, and multiplying with the vector ($\\mathbf{\\hat{e_{1}}}$) gives the direction of the projection. As shown above, by subtracting the projection, we make our vector orthogonal to the normalized vector ($\\mathbf{\\hat{e_{1}}}$). We then normalize the vector that is orthogonal to the first to get $\\mathbf{\\hat{e_{2}}}$. To make the 3rd vector orthonormal to the first two, we must subtract both their projections from the 3rd vector, $\\mathbf{\\vec{v_{3}}}$, then normalize it to get $\\mathbf{\\hat{e_{3}}}$. In general terms, to obtain the respective vector that is orthonormal to the already orthonormalized vectors in our set, we subtract their projections from the vector at hand, then normalize it.\n",
    "\n",
    "\n",
    "# Rotations and Scaling Content\n",
    "Generally, two of the most common types of linear transformations are __rotation__ and __scaling__ transformations. Scaling is a transformation that returns a scalar product of the input vector, whereas rotation is a transformation that does not affect the length of the vector, but rotates it in space. Examples of these are shown below.\n",
    "\n",
    "$$\\text{Scaling: } \\begin{bmatrix}2 & 0 \\\\ 0 & 2\\end{bmatrix} \\times \\begin{bmatrix}1 \\\\ 1\\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2\\end{bmatrix}$$\n",
    "\n",
    "$$\\text{Rotation: } \\begin{bmatrix}0 & -1 \\\\ 1 & 0\\end{bmatrix} \\times \\begin{bmatrix}2 \\\\ 0\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Vectors Plotly\n",
    "\n",
    "# Vector components\n",
    "x1 = [0,1,2]\n",
    "x2 = [0,1,2]\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=x1, y=x2,\n",
    "    mode='markers',\n",
    "    marker = dict(size=[10,40,40], \n",
    "                  color=[\"black\",\"orange\",\"orange\"]),\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Parallel Vectors ($x \\cdot y = 4$)\",\n",
    "    xaxis_title=\"$x_{1}$\",\n",
    "    yaxis_title=\"$x_{2}$\",\n",
    ")\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], marker_color=\"black\"))\n",
    "fig.add_trace(go.Scatter(x=[0, 2], y=[0, 2], marker_color=\"black\"))\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine simarility functions\n",
    "\n",
    "def cosine_similarity(v1,v2):\n",
    "    product = inner_product(v1,v2)\n",
    "    similarity = product/(vector_length(v1)*vector_length(v2))\n",
    "    return similarity\n",
    "\n",
    "def cosine_similarity(v1,v2):\n",
    "    similarity = np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Norms\n",
    "\n",
    "# Defining our vector\n",
    "vector1 = [1,4,9,15]\n",
    "\n",
    "# Standard Python\n",
    "def p_norm(v,p):\n",
    "    norm = 0\n",
    "    for val in v:\n",
    "        norm += val**p\n",
    "    norm = norm ** (1/p)\n",
    "    return norm\n",
    "print(\"Standard Python norms:\")\n",
    "print(\"1-norm:\",p_norm(vector1,1))\n",
    "print(\"2-norm:\",p_norm(vector1,2))\n",
    "print(\"3-norm:\",p_norm(vector1,3))\n",
    "print(\"inf-norm:\",p_norm(vector1,100))\n",
    "print()\n",
    "\n",
    "\n",
    "# NumPy\n",
    "print(\"NumPy norms:\")\n",
    "print(\"1-norm:\",np.linalg.norm(vector1,1))\n",
    "print(\"2-norm:\",np.linalg.norm(vector1,2))\n",
    "print(\"3-norm:\",np.linalg.norm(vector1,3))\n",
    "print(\"inf-norm:\",np.linalg.norm(vector1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Addition\n",
    "\n",
    "# def vec_add(v1,v2):\n",
    "#     resultant_vector = []\n",
    "#     for v1_val,v2_val in zip(v1,v2):\n",
    "#         resultant_vector.append(v1_val + v2_val)\n",
    "#     return resultant_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Animation\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib nbagg\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10,4))\n",
    "plt.suptitle(\"Vector Example\")\n",
    "\n",
    "def update_plot(x1,x2):\n",
    "    \"\"\"\n",
    "    This function updates our plot when we use the interactive widgets\n",
    "    \"\"\"\n",
    "    ax.clear()\n",
    "    # Adding points\n",
    "    x = np.array([0, x1])\n",
    "    y = np.array([0, x2])\n",
    "    \n",
    "    ax.plot(x,y,'-o',C='orange')#,label=units.format(x1,x2))\n",
    "    ax.set_xlim([0,5])\n",
    "    ax.set_ylim([0,5])\n",
    "    plt.xlabel(\"$x_{1}$\")\n",
    "    plt.ylabel(\"$x_{2}$\")\n",
    "    plt.show()\n",
    "    \n",
    "x1 = widgets.FloatSlider(min=0, max=5, value=1, description=\"$x_{1}$\")\n",
    "x2 = widgets.FloatSlider(min=0, max=5, value=1, description=\"$x_{2}$\")\n",
    "    \n",
    "widgets.interactive(update_plot, x1=x1, x2=x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Python Addition and Subtraction\n",
    "\n",
    "A = [[1,2],[3,2]]\n",
    "B = [[3,1],[1,1]]\n",
    "C = [[4,2],[2,2]]\n",
    "\n",
    "# Standard Python\n",
    "def mat_add(matrix1,matrix2):\n",
    "    result = []\n",
    "    for row1,row2 in zip(matrix1,matrix2): # iterating for row\n",
    "        new_row = []\n",
    "        for val1,val2 in zip(row1,row2): # iterating for each column\n",
    "            new_row.append(val1+val2)\n",
    "        result.append(new_row) # adding row after operation is done\n",
    "    return result\n",
    "\n",
    "def mat_subtract(matrix1,matrix2):\n",
    "    result = []\n",
    "    for row1,row2 in zip(matrix1,matrix2): # iterating for row\n",
    "        new_row = []\n",
    "        for val1,val2 in zip(row1,row2): # iterating for each column\n",
    "            new_row.append(val1-val2)\n",
    "        result.append(new_row) # adding row after operation is done\n",
    "    return result\n",
    "\n",
    "print(\"Standard Python:\")\n",
    "print(\"Addition:\",mat_add(A,B))\n",
    "print(\"Subtraction:\",mat_subtract(C,B))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Python Transpose\n",
    "\n",
    "A = [[1,2],[2,3],[3,4]]\n",
    "B = [[2,2],[1,1]]\n",
    "\n",
    "# Standard Python\n",
    "def transpose(mat):\n",
    "    mat_transpose = [] # initialise transposed matrix\n",
    "    for idx2 in range(len(mat[0])): # iterate through columns\n",
    "        new_row = []\n",
    "        for idx1 in range(len(mat)): # iterate through rows\n",
    "            new_row.append(mat[idx1][idx2])\n",
    "        mat_transpose.append(new_row)\n",
    "    return mat_transpose\n",
    "\n",
    "print(\"Standard Python:\")\n",
    "print(\"A' =\",transpose(A))\n",
    "print(\"B' =\",transpose(B))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity Matrix\n",
    "\n",
    "A = [[1,2],[2,1]]\n",
    "\n",
    "# Standard Python\n",
    "def identity(dim):\n",
    "    identity_matrix = []\n",
    "    for i in range(dim):\n",
    "        new_row = []\n",
    "        for j in range(dim):\n",
    "            if i==j:\n",
    "                new_row.append(1)\n",
    "            else:\n",
    "                new_row.append(0)\n",
    "        identity_matrix.append(new_row)\n",
    "    return identity_matrix\n",
    "I = identity(2)\n",
    "print(\"Standard Python:\")\n",
    "print(\"2x2 identity matrix:\",I)\n",
    "print(\"Matrix product:\",matrix_product(A,I))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "\n",
    "x,y = np.array([0,0]),np.array([0,0])\n",
    "u = np.array([1,2])\n",
    "v = np.array([1,2])\n",
    "\n",
    "fig = ff.create_quiver(x, y, u, v,\n",
    "                       scale=1,\n",
    "                       arrow_scale=.2,\n",
    "                       name='quiver',\n",
    "                       line_width=3\n",
    "                      )\n",
    "\n",
    "# # updating layout for a larger range of values and displaying axes\n",
    "fig.update_layout(yaxis=dict(range=[0,5]),\n",
    "                  xaxis=dict(range=[0,5]),\n",
    "                  title=\"Scaling\",\n",
    "                  xaxis_title=\"x1\",\n",
    "                  yaxis_title=\"x2\"\n",
    "                 )\n",
    "# fig.update_layout(xaxis=dict(range=[0,5]))\n",
    "\n",
    "fig.update_yaxes(nticks=20)\n",
    "fig.update_xaxes(nticks=20)\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x,y = np.array([0,0]),np.array([0,0])\n",
    "u = np.array([2,0])\n",
    "v = np.array([0,2])\n",
    "\n",
    "fig = ff.create_quiver(x, y, u, v,\n",
    "                       scale=1,\n",
    "                       arrow_scale=.2,\n",
    "                       name='quiver',\n",
    "                       line_width=3\n",
    "                      )\n",
    "\n",
    "# # updating layout for a larger range of values and displaying axes\n",
    "fig.update_layout(yaxis=dict(range=[0,5]),\n",
    "                  xaxis=dict(range=[0,5]),\n",
    "                  title=\"Rotation\",\n",
    "                  xaxis_title=\"x1\",\n",
    "                  yaxis_title=\"x2\"\n",
    "                 )\n",
    "# fig.update_layout(xaxis=dict(range=[0,5]))\n",
    "\n",
    "fig.update_yaxes(nticks=20)\n",
    "fig.update_xaxes(nticks=20)\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orthogonal Matrix Checker\n",
    "\n",
    "# Defining our matrix\n",
    "A = np.array([[0.5**0.5,0.5**0.5],[-0.5**0.5,0.5**0.5]]) # same matrix as in the example above\n",
    "B = np.array([[1,1],[1,2]])\n",
    "\n",
    "def is_orthogonal(matrix):\n",
    "    for i in range(len(matrix)-1):\n",
    "        for j in range(i+1,len(matrix)):\n",
    "            product = np.dot(matrix[:,i],matrix[:,j])\n",
    "            if product != 0:\n",
    "                return False\n",
    "    return True\n",
    "print(is_orthogonal(A))\n",
    "print(is_orthogonal(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
