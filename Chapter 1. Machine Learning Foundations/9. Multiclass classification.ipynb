{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "## Learning objectives\n",
    "- Understand how classification can be implemented when there are more than 2 classes\n",
    "- Implement a multiclass classifier from scratch\n",
    "\n",
    "## Intro - Binary classification vs multiclass classification\n",
    "\n",
    "In binary classification the output must be either true or false. Either the example falls into this class, or it doesn't. We have seen that we can represent this by our model having a single output node whose value is forced between 0 and 1, and as such represents a confidence in the fact that the example belongs to the positive class. Alternatively, still for binary classification, we could have two output nodes, where the value of the first represents the confidence that the input belongs to the positive class (true/class 1) and the value of the second represents the confidence that the input belongs to the negative class (false/class 2). In this case, the values of each output node must be positive and they must sum to 1, because this output layer represents a probability distribution over the output classes. \n",
    "\n",
    "# Softmax\n",
    "\n",
    "![](./images/binary-class.jpg)\n",
    "\n",
    "In the case where we have two nodes to represent true and false, we can think about it as having trained two models.\n",
    "\n",
    "Treating true and false as separate classes with separate output nodes shows us how we can extend this idea to do multiclass classification; we simply add more nodes and ensure that their values are positive and sum to one.\n",
    "\n",
    "![](./images/multiclass.jpg)\n",
    "\n",
    "### What function can we use to convert the output layer into a distribution over classes?\n",
    "\n",
    "The **softmax function** exponentiates each value in a vector to make it positive and then divides each of them by their sum to normalise them (make them sum to 1). This ensures that the vector then can be interpreted as a probability distribution.\n",
    "\n",
    "![](./images/softmax.jpg)\n",
    "\n",
    "## Differentiating the softmax\n",
    "\n",
    "# show differentiation of softmax here\n",
    "\n",
    "### Properties of softmax\n",
    "- increasing the value of any entry decreases the value of all of the others, because the whole vector must always sum to one. \n",
    "\n",
    "Let's implement our own softmax function, and again include a boolean flag that will return the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.46523752 0.05120626 0.74960415]\n[0.33445931 0.22107101 0.44446968]\n1.0\n[[ 0.22259628 -0.07393926 -0.14865702]\n [-0.07393926  0.17219862 -0.09825936]\n [-0.14865702 -0.09825936  0.24691638]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z, label=None, grad=False):\n",
    "    if grad:\n",
    "        num_classes = len(z)\n",
    "        g = np.zeros((num_classes, num_classes))\n",
    "        for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                if j == i:\n",
    "                    g[i][j] = softmax(z)[i] * (1 - softmax(z)[i])\n",
    "                else:\n",
    "                    g[i][j] = - softmax(z)[i] * softmax(z)[j]\n",
    "        return g\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "x = np.random.rand(3)\n",
    "print(x)\n",
    "print(softmax(x))\n",
    "print('softmax sums up to:', sum(softmax(x)))\n",
    "print(softmax(x, label=2, grad=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cross entropy loss function\n",
    "\n",
    "In the BCE loss function, the one line equation contained all of the \"switches\" that it needs to turn on or off certain terms of the equation.\n",
    "This was possible because in binary classification, the labels will certainly either be 0 or 1.\n",
    "\n",
    "In multiclass classification however, these switches cannot be contained in a single line.\n",
    "\n",
    "An appropriate loss function to use for multiclass classification is the cross entropy loss function.\n",
    "Like BCE loss, cross entropy uses the same term: the negative natural log of the output probability to penalise outputs exponentially as they stray from the ground truth.\n",
    "Not all of the terms are needed.\n",
    "By increasing the value of one element of the output of a softmax, the others must decrease, because the whole vector has to sum to 1.\n",
    "So if we focus on increasing the correct class likelihood, then we will implicitly be decreasing the incorrect class likelihood.\n",
    "\n",
    "Let's implement the cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(z, label, grad=False):\n",
    "    if grad:\n",
    "        return - 1 / z\n",
    "    return np.sum(- np.log(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the final layer of the model where the softmax is applied and the loss function, the model and algorithm stay the same. \n",
    "\n",
    "Of course however, changing the model changes the gradient of the loss function with respect to the model parameters.\n",
    "So we'll need to change the code that performs the parameter updates.\n",
    "\n",
    "Below is the same code we wrote to perform binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Classifier' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3f49b4f6d967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_binary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from get_colors import colors\n",
    "\n",
    "\n",
    "def make_binary_data(m=50): \n",
    "    X = 3*np.random.rand(m, 2) #sample from a normal distribution\n",
    "    # X = np.sort(X, axis=0)\n",
    "    # print(X.shape)\n",
    "    X = X[np.argsort(X[:, 0], axis=0)] # sort by first feature value so that i can plot hypothesis boundaries later\n",
    "    w = np.array([1, -2])\n",
    "    Xw = np.matmul(X, w.T)\n",
    "    Y = np.zeros(X.shape[0])\n",
    "    Y = Y.astype(int)\n",
    "    Y[Xw > -2] = 1 #np.sum(Wx,axis=1)\n",
    "    Y[Xw > 0.5] = 2\n",
    "    return X, Y #returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    for y in range(3):\n",
    "        x = X[Y == y]\n",
    "        ax.scatter(x[:, 0], x[:, 1], c=colors[y], marker='x', s=100)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "\n",
    "\n",
    "    # ax1.set_zlabel('$x_3$')\n",
    "\n",
    "    # ax1.ylabel('Y')\n",
    "    # # ax1.grid()\n",
    "    # plt.ion()\n",
    "    # plt.show()\n",
    "    return fig, ax\n",
    "\n",
    "def plot_hypothesis(X, H, ax=None):\n",
    "    ax = fig.add_subplot(111)\n",
    "    x = X[:, 0]\n",
    "    w0 = H.w[:, 0] # w for hypothesis 0\n",
    "    y0 = (w0[0] * x + H.b[0]) / w0[1]\n",
    "    w1 = H.w[:, 1] # w for hypothesis 1\n",
    "    y1 = (w1[0] * x + H.b[1]) / w1[1]\n",
    "    ax.plot(x, y0)\n",
    "    ax.plot(x, y1)\n",
    "\n",
    "    # l = H(X)\n",
    "    # other_w_b = np.matmul(X[:, 1:], H.w[1:]) + H.b\n",
    "    # l += other_w_b\n",
    "    # l /= H.w[1]\n",
    "    # r = other_w_b\n",
    "    # r /= H.w[1]\n",
    "    # # r -= 0.5\n",
    "    # ax.plot(l, r)\n",
    "\n",
    "    # h += H.b\n",
    "    # h += H.w[1]\n",
    "    # h = np.matmul(X, H.w.T)\n",
    "    # for y in range(3):\n",
    "    #     # print(h)\n",
    "    #     x = X[Y == y]\n",
    "    #     ax.plot(h[:, 0], h[:, 1], c=colors[y], marker='x')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    \n",
    "X, Y = make_binary_data()\n",
    "\n",
    "H = Classifier(n_features=2, n_classes=3)\n",
    "\n",
    "fig, ax = plot_data(X, Y)\n",
    "plot_hypothesis(X, H, ax)\n",
    "fig.canvas.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3, 3)\n(2,)\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ba6ab5da70ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mdhdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_deriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;31m# print(prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dhdw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ba6ab5da70ad>\u001b[0m in \u001b[0;36mcalc_deriv\u001b[0;34m(self, x, y_hat, label)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhdz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdzdw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdhdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhdz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdzdw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdhdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdhdz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdzdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dhdw:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhdw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        self.w = np.random.rand(n_features, n_classes)\n",
    "        self.b = np.random.rand(n_classes)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = np.matmul(x, self.w) + self.b\n",
    "        x = softmax(x)\n",
    "        return x\n",
    "\n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w\n",
    "        self.b = new_b    \n",
    "\n",
    "    def calc_deriv(self, x, y_hat, label):\n",
    "        m = len(Y) # m = number of examples\n",
    "        diffs = y_hat - label # calculate errors\n",
    "        dzdb = 1\n",
    "        dzdw = x # needs to be distributed\n",
    "        dhdz = softmax(y_hat, label, grad=True)\n",
    "        print(dhdz.shape)\n",
    "        print(dzdw.shape)\n",
    "        dhdw = np.matmul(dhdz, dzdw)\n",
    "        dhdb = dhdz * dzdb\n",
    "        print('dhdw:', dhdw)\n",
    "        ssdc\n",
    "        return dhdw, dhdb\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "H = Classifier(n_features=2, n_classes=3)\n",
    "\n",
    "# PLOT OUR HYPOTHESIS BEFORE TRAINING\n",
    "# plt.figure()\n",
    "# plt.title('Before training')\n",
    "# plt.ylabel('Label')\n",
    "# plt.xlabel('Features')\n",
    "# plt.scatter(X, H(X), label='predictions')\n",
    "# plt.scatter(X, Y, c='r', marker='x', label='ground truth')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "fig, ax = plot_data(X, Y)\n",
    "plot_hypothesis(X, H, ax)\n",
    "\n",
    "epochs = 1000\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []\n",
    "    for x, y in zip(X, Y):\n",
    "        # jnd\n",
    "        prediction = H(x)\n",
    "        # print(y)\n",
    "        loss = CrossEntropyLoss(prediction, y)\n",
    "        epoch_losses.append(loss)\n",
    "        dhdw, dhdb = H.calc_deriv(x, prediction, y)\n",
    "        # print(prediction)\n",
    "        print('dhdw')\n",
    "        print(dhdw)\n",
    "        print(dhdw.shape)\n",
    "\n",
    "        print('dhdb')\n",
    "        print(dhdb)\n",
    "        print(dhdb.shape)\n",
    "\n",
    "\n",
    "        dLdh = CrossEntropyLoss(prediction, y, grad=True)\n",
    "        print('dLdh:')\n",
    "        print(dLdh.shape)\n",
    "        print(dLdh)\n",
    "        print('dhdw')\n",
    "        print(dhdw)\n",
    "        print(dhdw.shape)\n",
    "        dLdw = dLdh * dhdw\n",
    "        print(dLdw)\n",
    "        dLdb = dLdh * dhdb\n",
    "        new_w = H.w - learning_rate * dLdw\n",
    "        new_b = H.b - learning_rate * dLdb\n",
    "        H.update_params(new_w, new_b)\n",
    "    plot_hypothesis(X, H, ax=ax1)\n",
    "    fig.canvas.draw()\n",
    "    losses.append(np.mean(epoch_losses))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot class probability landscape for each class\n",
    "Evaluate mesh and plot probability of being a member of each class vertically "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- multiclass classification requires a different loss function \n",
    "- softmax is a differentiable function that turns a vector of real numbers into a probability distribution\n",
    "\n",
    "## Next steps\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit4e36a22c5ab145cdaac0d5c6b1a34fd0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}