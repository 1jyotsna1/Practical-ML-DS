{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "## Learning objectives\n",
    "- Understand how classification can be implemented when there are more than 2 classes\n",
    "- Implement a multiclass classifier from scratch\n",
    "\n",
    "## Intro - Binary classification vs multiclass classification\n",
    "\n",
    "In binary classification the output must be either true or false. Either the example falls into this class, or it doesn't. We have seen that we can represent this by our model having a single output node whose value is forced between 0 and 1, and as such represents a confidence in the fact that the example belongs to the positive class. Alternatively, still for binary classification, we could have two output nodes, where the value of the first represents the confidence that the input belongs to the positive class (true/class 1) and the value of the second represents the confidence that the input belongs to the negative class (false/class 2). In this case, the values of each output node must be positive and they must sum to 1, because this output layer represents a probability distribution over the output classes. \n",
    "\n",
    "# \n",
    "\n",
    "![](./images/binary-class.jpg)\n",
    "\n",
    "In the case where we have two nodes to represent true and false, we can think about it as having trained two models.\n",
    "\n",
    "Treating true and false as separate classes with separate output nodes shows us how we can extend this idea to do multiclass classification; we simply add more nodes and ensure that their values are positive and sum to one.\n",
    "\n",
    "![](./images/multiclass.jpg)\n",
    "\n",
    "### What function can we use to convert the output layer into a distribution over classes?\n",
    "\n",
    "The **softmax function** exponentiates each value in a vector to make it positive and then divides each of them by their sum to normalise them (make them sum to 1). This ensures that the vector then can be interpreted as a probability distribution.\n",
    "\n",
    "![](./images/softmax.jpg)\n",
    "\n",
    "### Properties of softmax\n",
    "- increasing the value of any entry decreases the value of all of the others, because the whole vector must always sum to one. \n",
    "This means that when calculating the loss for a prediction we don't need to consider all of the classes, pushing the correct label larger and pushing the incorrect labels lower. \n",
    "Instead we can just push the correct label up, and because of the softmax formula, the others will be pushed down.\n",
    "\n",
    "Let's implement our own softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.51303363 0.75243903 0.38204912 0.31023859 0.47814088 0.72571698]\n[0.16215675 0.2060191  0.14224899 0.13239216 0.15659623 0.20058675]\n1.0\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z, label=None, grad=False):\n",
    "    if grad:\n",
    "        if not label:\n",
    "            raise ValueError\n",
    "        else:\n",
    "            g = np.empty_like(z)\n",
    "            for j in range(len(z)):\n",
    "                if j == label:\n",
    "                    g[j] = softmax(z)[label] * (1 - softmax(z)[label])\n",
    "                else:\n",
    "                    g[j] = - softmax(z)[label] * softmax(z)[j]\n",
    "            return g\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "x = np.random.rand(6)\n",
    "print(x)\n",
    "print(softmax(x))\n",
    "print(sum(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- multiclass classification requires a different loss function \n",
    "- softmax is a differentiable function that turns a vector of real numbers into a probability distribution\n",
    "\n",
    "## Next steps\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit4e36a22c5ab145cdaac0d5c6b1a34fd0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}