{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "- Understand\n",
    "    - gradient boosting algorithms\n",
    "- implement gradient boosting\n",
    "- implement XGBoost\n",
    "\n",
    "## Intro - Feedback loops\n",
    "In the real world, lots of classical control systems depend on feedback loops. \n",
    "Feedback loops work by repeatedly taking a measurement, and then taking an action to push the value measured towards the desired value. \n",
    "Boosting kind of implements this for machine learning models by having a sequence of models whose outputs are added together, where the next model is trained to correct the error of the one before.\n",
    "\n",
    "## Gradient boosting\n",
    "\n",
    "We will start off by looking at gradient boosting for regression.\n",
    "For this, we will need a dataset with continuous features and a differentiable loss function.\n",
    "\n",
    "Gradient boosting produces a sequence of models, each of which predicts the difference between the previous model's prediction and the ideal output. The models' predictions are weighted by a learning rate and then summed together to produce a final output.\n",
    "\n",
    "Gradient boosting starts by outputting a prediction for all values using a single leaf, that is, by predicting the same constant value, $c$, for all of them.\n",
    "\n",
    "## $H_1(x) = c$\n",
    "\n",
    "But what is the value of $c$? \n",
    "Let's minimise our mean squared error objective to solve this.\n",
    "\n",
    "## $c = argmin_c \\ MSE(H_1(X), Y) = argmin_w \\frac{1}{m} \\Sigma^m_{i=1} (H_1(x_i) - y_i)^2$\n",
    "\n",
    "This can be found by calculating the gradient of the MSE loss above and setting to zero.\n",
    "## $\\frac{\\partial L}{\\partial c} = \\frac{2}{m} \\Sigma^m_{i=1} (H_1(x_i) - y_i)$\n",
    "## $0 = \\frac{2}{m} \\Sigma^m_{i=1} (H_1(x_i) - y_i)$\n",
    "## $c = \\frac{1}{m} \\Sigma^m_{i=1} y_i$\n",
    "The solution to the minimisation above is the mean of all of the labels.\n",
    "## $H_1(x_i) = \\frac{1}{m} \\Sigma^m_i y_i$\n",
    "\n",
    "Gradient boosting then sequentially uses other trees to make a prediction of the amount and direction by which the prediction made by the previous tree needs to be shifted to reduce the loss.\n",
    "The adjustment needed to be made for each example depends on the error resulting from the latest prediction. \n",
    "For the $l$th tree, this error is given by \n",
    "\n",
    "$H_l(x) - y$. \n",
    "\n",
    "If the error is positive, then our estimate is too high, and we need to push it in the negative direction.\n",
    "And vice versa.\n",
    "This sounds a bit like gradient descent right?\n",
    "In fact, it is implemented in almost the same way.\n",
    "\n",
    "This is where the name gradient boosting comes from: each sequential model boosts the predictions of the previous one by adjusting it's prediction in a direction that reduces the error, by following the gradient of the loss with respect to the most recent prediction downhill.\n",
    "\n",
    "The difference between gradient descent for optimisation and gradient boosting is that this shifting process does not update the model parameters, but updates the prediction, and it is not done whilst training, but is done whilst making predictions.\n",
    "\n",
    "We calculate the update that needs to be made to the prediction \n",
    "\n",
    "Another way to look at the error is that it is the negative of what extra needs to be added to the prediction.\n",
    "This difference is known as the residual.\n",
    "\n",
    "When we use the half MSE loss function, it's derivative gives us the residual exactly.\n",
    "When using other loss functions though, such as the MSE loss for simplicity, or the cross entropy loss for classification, the derivative which gives us the update to make to the predictions is not equal to the residual.\n",
    "In this case, we call the update quantities **pseudo residuals**.\n",
    "\n",
    "\n",
    "Then we build a tree based on the predictions from the first leaf.\n",
    "This is similar to AdaBoost in the sense that we are going to combine the predictions of successive models.\n",
    "What is different however, is that this second tree predicts the **residual**, rather than also predicting the same the label.\n",
    "The residual is the difference between the label and the prediction.\n",
    "## Residual, $r_{il} = H_l(x_i) - y_i$\n",
    "We then take a weighted contribution of the trees' outputs.\n",
    "\n",
    "## How is each tree's contribution weighted?\n",
    "\n",
    "Gradient boost scales the contribution of each successive tree by a learning rate \n",
    "# to prevent overfitting?\n",
    "As we've seen before, the learning rate $alpha$ is a value between zero and 1.\n",
    "\n",
    "## learning_rate, $\\alpha \\in [0, 1]$\n",
    "\n",
    "When we did gradient based optimisation earlier, \n",
    "\n",
    "You can imagine gradient boosting like another ensemble method, where each tree nudges the predictions towards the label by adding the models' outputs rather than avaeraging them.\n",
    "\n",
    "Each successive model predicts the residuals that remain from the addition of all of the previous models' predictions.\n",
    "\n",
    "By adding more trees, \n",
    "This reduces the bias in the model.\n",
    "However, it also increases the variance as there becomes more model parameters overall, and hence more possible suitable parameterisations.\n",
    "\n",
    "\n",
    "\n",
    "We can restrict the size of these trees by either specifying the maximum depth of the tree of the maximum number of leaves.\n",
    "\n",
    "## Mathematical approach\n",
    "\n",
    "##\n",
    "## $r_{il} = - [\\frac{\\partial L(H(x_i), y_i)}{\\partial H(x_i)}]_{H(x)=H_{l-1}(x)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}