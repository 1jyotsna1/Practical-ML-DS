{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles 1: Random forests\n",
    "\n",
    "## Learning objectives\n",
    "- understand \n",
    "    - bootstrapping datasets\n",
    "    - building ensembles by aggregating predictions\n",
    "    - bagging\n",
    "    - random forests\n",
    "- implement\n",
    "    - your first esemble\n",
    "    - a random forests\n",
    "\n",
    "## Intro - ensembles\n",
    "### \"The collective is smarter than the individual\"\n",
    "\n",
    "\n",
    "If we attempted to build an ensemble with multiple instances of the same model on the same dataset, this wouldn't help. \n",
    "Why?\n",
    "Because they would all be identical (other than the differences induced by any stochastisity in the optimisation process).\n",
    "This means that they would all make the same mistakes, and combining their predictions would not give any improvement.\n",
    "\n",
    "### Why do ensembles work?\n",
    "Mathematically, ensembles work because the mistakes made between models are not correlated.\n",
    "This is because all of the errors are correlated.\n",
    "\n",
    "To make sure the model errors are not correlated, we can't just train the same model on the same dataset, otherwise all of the errors would be the same for each model.\n",
    "We can make the predictions differ (and uncorrelate the errors) in different ways, some of which we discuss below. \n",
    "\n",
    "### Bootstrapping datasets and bagging\n",
    "The first way that we can make the models differ is by training them on different data.\n",
    "We can \"bootstrap\" new datasets by sampling with replacement from the existing datasets.\n",
    "Bootstrapping datasets to train different models on and aggregating their results is known as bootstrap aggregating or **bagging**.\n",
    "\n",
    "Let's get our data and make some bootstrapped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from get_colors import colors\n",
    "\n",
    "def get_data(sd=6, m=10, n_features=2, n_clusters=2):\n",
    "    X, Y = sklearn.datasets.make_blobs(n_samples=m, n_features=n_features, centers=n_clusters, cluster_std=sd)\n",
    "    return X, Y\n",
    "\n",
    "def create_bootstrapped_dataset(existing_X, existing_Y, sample_weights=None):\n",
    "    \"\"\"Create a single bootstrapped dataset\"\"\"\n",
    "    idxs = np.random.choice(np.arange(len(existing_X)), replace=True)\n",
    "    return existing_X[idxs], existing_Y[idxs]\n",
    "\n",
    "def create_bootstrapped_datasets(X, Y, n):\n",
    "    \"\"\"Create n bootstrapped datasets\"\"\"\n",
    "    datasets = []\n",
    "    for d in range(n):\n",
    "        x, y = create_bootstrapped_dataset(X, Y)\n",
    "        datasets.append((x, y))\n",
    "    return datasets\n",
    "\n",
    "X, Y = get_data()\n",
    "n_trees = 10\n",
    "bootstrapped_datasets = create_bootstrapped_datasets(X, Y, n_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Random forests\n",
    "Random forests are ensembles of decision trees (many trees make a forest).\n",
    "\n",
    "### Learning on feature subsets\n",
    "Another way that we can make the predictions differ, and further uncorrelate errors, is by only allowing the model to make predictions based on a limited number of the features. \n",
    "That is, we train each model on the features of examples from the dataset that have been projected into a subspace of the feature space.\n",
    "\n",
    "To recap, the randomness in random forests is induced in two ways:\n",
    "1. By training each tree on a random dataset, bootstrapped from the original\n",
    "2. By having each tree project the data into a subspace of feature space before fitting it\n",
    "\n",
    "## Implementation of random forest from scratch\n",
    "\n",
    "Let's use the sklearn `DecisionTreeClassifier` as our model, and train an ensemble of them on different random subspaces of features to create a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[-7.95532263]\n [13.00869119]\n [ 5.21951674]\n [-1.65583386]\n [-4.43587736]\n [-5.07070575]\n [-6.05277874]\n [ 1.36745376]\n [-1.22256703]\n [11.27625644]]\n[[ -7.95532263  -6.5324316 ]\n [ 13.00869119   6.24425574]\n [  5.21951674   6.51036873]\n [ -1.65583386  -1.3158393 ]\n [ -4.43587736   0.32247454]\n [ -5.07070575   2.00740652]\n [ -6.05277874  -1.4526738 ]\n [  1.36745376  -3.05795278]\n [ -1.22256703   1.28295427]\n [ 11.27625644 -14.02775716]]\n"
    }
   ],
   "source": [
    "def project_into_subspace(X, feature_idxs):\n",
    "    return X[:, feature_idxs]\n",
    "\n",
    "projected_X = project_into_subspace(X, [0])\n",
    "print(projected_X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "forest: [\n    {\n        \"depth\": 1,\n        \"features\": [\n            0\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            0\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            0\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            0\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            0\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            0\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            0\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            1\n        ]\n    },\n    {\n        \"depth\": 1,\n        \"features\": [\n            0\n        ]\n    }\n]\n"
    }
   ],
   "source": [
    "import sklearn.tree\n",
    "import json\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=4):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        for tree_idx in range(self.n_trees):\n",
    "            n_features = np.random.choice(range(1, X.shape[1]))\n",
    "            subspace_feature_indices = np.random.choice(range(X.shape[1]), size=n_features)\n",
    "            projected_X = project_into_subspace(X, subspace_feature_indices)\n",
    "            tree = sklearn.tree.DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree.fit(projected_X, Y)\n",
    "            tree.feature_indices = subspace_feature_indices\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((len(X), self.n_trees)) # empty array of prediction with shape n_examples x n_trees\n",
    "        for tree_idx, tree in enumerate(self.trees):\n",
    "            x = project_into_subspace(X, tree.feature_indices)\n",
    "            predictions[:, tree_idx] = tree.predict(x)\n",
    "        prediction = np.mean(predictions, axis=1)\n",
    "        prediction = prediction.astype(int)\n",
    "        return prediction\n",
    "\n",
    "    def __repr__(self):\n",
    "        forest = []\n",
    "        for idx, tree in enumerate(self.trees):\n",
    "            forest.append({\n",
    "                'depth': tree.max_depth,\n",
    "                'features': tree.feature_indices.tolist()\n",
    "            })\n",
    "        return json.dumps(forest, indent=4)\n",
    "\n",
    "randomForest = RandomForest(n_trees=20, max_depth=1)\n",
    "randomForest.fit(X, Y)\n",
    "randomForest.predict(X)\n",
    "print('forest:', randomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/home/ice/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "randomForest = sklearn.ensemble.RandomForestClassifier()\n",
    "randomForest.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of random forests\n",
    "-\n",
    "\n",
    "## Next steps\n",
    "- Boosting\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit4e36a22c5ab145cdaac0d5c6b1a34fd0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}