{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation graph\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Up to this point we used ready made algorithms from `sklearn` except for linear regression with analytical solution. When we called `fit` the algorithm was trained on our data.\n",
    "\n",
    "All machine learning algorithms are essentially functions taking some input and producing some output and could be written like this:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = f(X)\n",
    "$$\n",
    "\n",
    "where `X` is data and $\\hat{Y}$ is prediction. Those functions depend on `parameters` or `weights` (denoted by $\\theta$) so our mathematical description changes to:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = f_{\\theta}(X)\n",
    "$$\n",
    "\n",
    "Also there is `loss` which measures the error (always positive scalar value) compared to true values (`Y`). The less, the better as we saw previously. In maths:\n",
    "\n",
    "$$\n",
    "L(f_{\\theta}(X), Y)\n",
    "$$\n",
    "\n",
    "Some of those functions (including neural networks) are differentiable, hence we can __calculate partial derivatives__.\n",
    "\n",
    "Automatic differentiation graph allows us to compute `gradient` of `loss` ($\\nabla L$) with respect to (abbreviated often as w.r.t.) parameters $\\theta$ using chain rule, which one can write like this:\n",
    "\n",
    "$$\n",
    "\\nabla L_{\\theta}\n",
    "$$\n",
    "\n",
    "In this section we will only focus on `backpropagation` itself, loss and how it fits within Machine Learning domain will be shown in the next lesson.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Intuitively, we can think of parameters gradient as:\n",
    "\n",
    "> how and by how much we should change their value in order to improve loss (decrease it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is graph?\n",
    "\n",
    "> Graph is a data structure containing __nodes__ and __edges__ connecting one node to another\n",
    "\n",
    "Example image of graph will tell you everything you need to know:\n",
    "\n",
    "![](images/graph_data_structure.png)\n",
    "\n",
    "We can see you can travel across the nodes by moving along the edges, here are some questions:\n",
    "- What is the path to get to get from A to E?\n",
    "- Which path has the __largest__ and __smallest__ cost for A to E traversal?\n",
    "- What are the __cycles__ inside this graph?\n",
    "- What are the nodes which one __cannot reach__ from A and which from E?\n",
    "\n",
    "## Undirected vs directed graph\n",
    "\n",
    "Graphs can either __directed__ or __undirected__:\n",
    "\n",
    "### Undirected graph\n",
    "\n",
    "> Undirected graph can be traversed along edges __in any direction__\n",
    "\n",
    "![](images/undirected_graph.png)\n",
    "\n",
    "### Directed graph\n",
    "\n",
    "> Directed graph can be traversed along edges in a __specified direction__\n",
    "\n",
    "We have seen an example at the very top\n",
    "\n",
    "### Directed acyclic graph\n",
    "\n",
    "> Directed acyclic graph (DAG) can be traversed in a __specified direction__ and __has no cycles__\n",
    "\n",
    "![](images/dag.png)\n",
    "\n",
    "__This data structure is especially important for us!__\n",
    "\n",
    "It will be used as a structure which will keep our automatic differentiation graph and is used across machine learning all the time (especially in deep learning and neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph and chain rule\n",
    "\n",
    "Let's write an example chain of functions:\n",
    "\n",
    "$$\n",
    "a = b(c(d(e(X))))\n",
    "$$\n",
    "\n",
    "Now, we would like to know how change in `e(X)` influences `a` value. To do that, we have to calculate gradient of `a` w.r.t. to `e`. Using chain rule this would be (each derivative is evaluated at certain point starting with `X`, left it out for brevity):\n",
    "\n",
    "$$\n",
    "\\frac{da}{de} = \\frac{da}{db} \\frac{db}{dc} \\frac{dc}{dd} \\frac{dd}{de}\n",
    "$$\n",
    "\n",
    "> backpropagation is an algorithm which given output (`a` in this case) runs operations (__their derivative formulas__) backward in order to calculate gradient of inputs\n",
    "\n",
    "`backprop` in computer programming can be described as graph, conceptually:\n",
    "- when the value is calculated (forward pass) graph records each __operation__ on data\n",
    "- when special function is run on the graph (we will name it `backward`), __derivatives__ of recorded operations are run in opposite order\n",
    "- when we get to the __parameter__ we update it's gradient (in example above `e` would be a parameter)\n",
    "\n",
    "__Let's see in visual form how the graph looks like:__\n",
    "\n",
    "![](images/comp_graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic vs static graph\n",
    "\n",
    "There are out-of-the box frameworks which provide highly optimized graph implementations.\n",
    "Two major approaches exist:\n",
    "\n",
    "- static graph\n",
    "- dynamic graph\n",
    "\n",
    "They are mostly used in __neural networks__ (which we are gonna see in a separate module)\n",
    "\n",
    "### Static graph\n",
    "\n",
    "> graph is defined only once and cannot be updated afterwards\n",
    "\n",
    "Best known representant of this approach in industry is [Tensorflow](https://www.tensorflow.org/), especially with it's `1.x` version. Currently running out of favours among developers (and researchers especially) and is a good fit for specific usage (very large neural networks).\n",
    "\n",
    "#### Upsides\n",
    "\n",
    "- Easier to distribute graph among multiple machines. Structure is known and can be shared more easily\n",
    "- Space for optimization (though it's impact is highly debatable)\n",
    "- Hence __might__ run faster\n",
    "\n",
    "#### Downsides\n",
    "\n",
    "- Does not feel Pythonic and is hard to use\n",
    "- Cannot be changed based on any condition, only based on part of the framework\n",
    "- Cumbersome syntax and usage; `if`, `for` and other constructs from Python do not exist\n",
    "- Hard to debug\n",
    "\n",
    "### Dynamic graph\n",
    "\n",
    "> graph is defined \"on the fly\" and nodes are added as they are run\n",
    "\n",
    "Best known representant of this approach is [PyTorch](https://pytorch.org/), while [Tensorflow](https://www.tensorflow.org/) tries to go the same path since `2.x` version.\n",
    "Largely favored among researchers with weaker industry adoption (though that changes rapidly).\n",
    "\n",
    "#### Upsides\n",
    "\n",
    "- Easy to use. If implemented well can feel just like writing in Python and `numpy`\n",
    "- Easier to debug\n",
    "- Can be changed however we wish based on any condition\n",
    "\n",
    "#### Downsides\n",
    "\n",
    "- Graph is harder to distribute as it might change any time. Sharing it might be costly\n",
    "- Might be harder to optimize (though optimized versions exist and usually isn't a problem)\n",
    "- __Might__ run slower\n",
    "\n",
    "As the last one is getting more and more traction we will make our own simplistic version of dynamic graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph implementation\n",
    "\n",
    "Let's start with `graph` and how it works. Conceptually, it will need the following:\n",
    "- Keeping `parameters` and `operations` references inside\n",
    "- Functions to register `parameters` and `operations` inside graph\n",
    "- `backward` function to populate parameters with gradient\n",
    "\n",
    "Below is a `class` which does it. Also as it is a dynamic graph `operations` have to be cleared after each `backward` call so they can be redefined \"on-the-fly\".\n",
    "\n",
    "__Don't worry, we will walk you through it step by step!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    \"\"\"Graph class used for backward automatic differentiation (backpropagation).\n",
    "\n",
    "\n",
    "    Can only differentiate w.r.t. scalar values. `backpropagate` function\n",
    "    should be called on final parameter (after all operations\n",
    "    were performed).\n",
    "\n",
    "    Attributes:\n",
    "        operations (Dict[int, (Operation, Dict[int, (int, bool)])]):\n",
    "            List of operations which, when backpropagated produce gradients\n",
    "            for Parameters. Each item is a Tuple containing:\n",
    "            - Instance of operation\n",
    "            - Dictionary containing:\n",
    "                - index of input parameter (so usually it is [0, 1, 2, 3...])\n",
    "                - Tuple containing:\n",
    "                    - index of operation which created this input parameter\n",
    "                    - True/False value whether this node is a leaf\n",
    "\n",
    "            If node is a leaf it has to be parameter and backpropagation stops\n",
    "            at this call to `backward` (see `Parameter` class)\n",
    "\n",
    "        parameters (List[Parameter])\n",
    "            List of parameters added to this graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.operations = {}\n",
    "        self.parameters = []\n",
    "\n",
    "    def _register_parameter(self, parameter: \"Parameter\"):\n",
    "        \"\"\"Registers parameter inside the graph\n",
    "\n",
    "        Arguments:\n",
    "            Instance of Parameter to be registered\n",
    "\n",
    "        Returns:\n",
    "            Index of parameter inside the graph which is saved in parameter's instance.\n",
    "\n",
    "        \"\"\"\n",
    "        self.parameters.append(parameter)\n",
    "        return len(self.parameters) - 1\n",
    "\n",
    "    def _register_operation(self, operation: \"Operation\", inputs):\n",
    "        \"\"\"Registers operation inside the graph\n",
    "\n",
    "        Returns:\n",
    "            Index of operation inside the graph which is saved in parameter's instance.\n",
    "\n",
    "        \"\"\"\n",
    "        if has_grad():\n",
    "            last_index = len(list(self.operations.keys()))\n",
    "            self.operations[last_index] = (operation, inputs)\n",
    "            return last_index\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_gradient(upstream_gradient, output_index):\n",
    "        \"\"\"If gradient is a Tuple return element otherwise return upstream_gradient\n",
    "        as is.\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(upstream_gradient, (tuple, list)):\n",
    "            return upstream_gradient[output_index]\n",
    "        return upstream_gradient\n",
    "\n",
    "    def _backpropagate_node(\n",
    "        self, upstream_gradient, output_index, operation_index, is_leaf\n",
    "    ) -> None:\n",
    "        \"\"\"Backpropagate through single node (Operation or Parameter).\n",
    "\n",
    "        If Parameter (is_leaf=True) is reached backpropagation will end with\n",
    "        populating it's gradient.\n",
    "\n",
    "        If Operation (is_leaf=False) is reached the node will be run through\n",
    "        graph backpropagation again.\n",
    "\n",
    "        \"\"\"\n",
    "        gradient = Graph._get_gradient(upstream_gradient, output_index)\n",
    "        if is_leaf:\n",
    "            self.parameters[operation_index].backward(gradient)\n",
    "        else:\n",
    "            # If we went through this operation we should raise an error\n",
    "            new_operation = self.operations.pop(operation_index, None)\n",
    "            if new_operation is None:\n",
    "                raise ValueError(\n",
    "                    \"Trying to backpropagate through non-existent node. Are your paths disjoint?\"\n",
    "                )\n",
    "            self._backpropagate_graph(new_operation, gradient)\n",
    "\n",
    "    def _backpropagate_graph(self, operation_and_mapping, upstream_gradient):\n",
    "        \"\"\"Backpropagate through graph of operations.\n",
    "\n",
    "        For any incoming operation it will go over their inputs\n",
    "        (defined by mapping which points to input nodes) and propagate\n",
    "        current upstream gradient to them.\n",
    "\n",
    "        After calculation of gradient it's internal cached is clear by the graph\n",
    "\n",
    "        \"\"\"\n",
    "        operation, mapping = operation_and_mapping\n",
    "        upstream_gradient = operation.backward(upstream_gradient)\n",
    "        # Clean cache\n",
    "        operation.cache = None\n",
    "        # Multiple outputs\n",
    "        for output_index, (operation_index, is_leaf) in mapping.items():\n",
    "            self._backpropagate_node(\n",
    "                upstream_gradient, output_index, operation_index, is_leaf\n",
    "            )\n",
    "\n",
    "    def backward(self, upstream_gradient=1) -> None:\n",
    "        \"\"\"Entrypoint for backpropagation through registered nodes.\n",
    "\n",
    "        `backward` will run through all the nodes contained inside graph in succession,\n",
    "        starting with the one added as the last one.\n",
    "\n",
    "        If there are multiple __separable__ paths those will be backpropagated\n",
    "        as well. If they aren't separable an error will be raised.\n",
    "\n",
    "        When graph's `backward` is called it will be cleaned from\n",
    "        all operations (parameters stay inside graph until the graph instance\n",
    "        is available, usually throughout the whole program).\n",
    "\n",
    "        \"\"\"\n",
    "        if not has_grad():\n",
    "            raise ValueError(\"Cannot perform backward as tape recording is off.\")\n",
    "\n",
    "        while self.operations:\n",
    "            last_index = list(self.operations.keys())[-1]\n",
    "            self._backpropagate_graph(\n",
    "                self.operations.pop(last_index), upstream_gradient\n",
    "            )\n",
    "        for parameter in self.parameters:\n",
    "            parameter.last_operation_index = None\n",
    "\n",
    "\n",
    "class _GlobalGraph:\n",
    "    \"Class used to hide global state from the main namespace.\"\n",
    "    graph = Graph()\n",
    "    on: bool = True\n",
    "\n",
    "\n",
    "def get():\n",
    "    \"\"\"Return global graph\"\"\"\n",
    "    return _GlobalGraph.graph\n",
    "\n",
    "\n",
    "def has_grad():\n",
    "    return _GlobalGraph.on\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def no_grad():\n",
    "    _GlobalGraph.on = False\n",
    "    yield\n",
    "    _GlobalGraph.on = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that __graph is global__. It is simpler that way as `parameters` are registered only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "__Why initial gradient is 1?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation\n",
    "\n",
    "Next let's create `Operation`. Let's recap what is needed:\n",
    "- take data value and calculate mathematical operation on it\n",
    "- register itself in graph so it can be run during `backward`\n",
    "- implement `backward`\n",
    "\n",
    "This is exactly what `base classes` are for. Later we can implement specific math operation by deriving from this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Operation(abc.ABC):\n",
    "    \"\"\"Base class of mathematical operation to be run on Parameter/np.array\n",
    "\n",
    "    Attributes:\n",
    "        cache (Optional[np.array])\n",
    "            Cache attribute one can use to save anything during forward pass\n",
    "            to reuse in backward\n",
    "        index_in_graph (int):\n",
    "            Index of operation in graph's operation dictionary\n",
    "        is_leaf (bool):\n",
    "            Always `False`, used by graph to easily discern between parameters\n",
    "            and operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "        self.index_in_graph = None\n",
    "        self.is_leaf = False\n",
    "\n",
    "    def __call__(self, *arguments):\n",
    "        \"\"\"Run forward and register operation in graph.\n",
    "\n",
    "        Additionally operation's inputs will be registered using mapping and\n",
    "        whether those are leafs (parameters) or operations to be further\n",
    "        backpropagated.\n",
    "\n",
    "        \"\"\"\n",
    "        if has_grad():\n",
    "            mapping = {}\n",
    "            add_to_graph = False\n",
    "            for input_index, argument in enumerate(arguments):\n",
    "                if isinstance(argument, Parameter):\n",
    "                    add_to_graph = True\n",
    "                    is_first_operation = argument.last_operation_index is None\n",
    "                    if is_first_operation:\n",
    "                        mapping[input_index] = (argument.index_in_graph, True)\n",
    "                    else:\n",
    "                        mapping[input_index] = (argument.last_operation_index, False)\n",
    "\n",
    "            if add_to_graph:\n",
    "                self.index_in_graph = get()._register_operation(self, mapping)\n",
    "                for argument in arguments:\n",
    "                    if isinstance(argument, Parameter):\n",
    "                        argument.last_operation_index = self.index_in_graph\n",
    "\n",
    "        # Pack return value in tuple always\n",
    "        return self.forward(*arguments)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, *_):\n",
    "        \"\"\"Define your forward pass here.\n",
    "\n",
    "        Use self.cache to cache anything needed during backpropagation.\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def backward(self, upstream_gradient):\n",
    "        \"\"\"Define your backward pass here.\n",
    "\n",
    "        Use self.cache in order to calculate gradient. There has to be as\n",
    "        many outputs as there was inputs to forward.\n",
    "\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter\n",
    "\n",
    "Last class we need is `Parameter`. As we know `np.array` and how it works we are going to base our implementation by inheriting from this class.\n",
    "\n",
    "`numpy.ndarray` requires special approach when inheriting from it. `__new__` instead of `__init__` and `__array_finalize__` special method. For those curious [you can read more about it here](https://numpy.org/doc/stable/user/basics.subclassing.html).\n",
    "\n",
    "What we need for our `Parameter`?\n",
    "- `gradient` attribute which keeps the gradient (or `None` if `backward` wasn't called)\n",
    "- when `backward` is called, `self.gradient` should be populated\n",
    "- `gradient` should be cleared when it isn't needed anymore. `clear` method will simply assign `None` to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# https://numpy.org/doc/stable/user/basics.subclassing.html\n",
    "class Parameter(np.ndarray):\n",
    "    \"\"\"Parameter class to be populated with gradient.\n",
    "\n",
    "    Attributes:\n",
    "        gradient (Optional[np.array]):\n",
    "            Array with gradients with which parameter can be optimized via\n",
    "            optimizer\n",
    "        index_in_graph (int):\n",
    "            Index of parameter in graph's list\n",
    "        is_leaf (bool):\n",
    "            Always True, used by graph to easily discern between parameters\n",
    "            and operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, input_array):\n",
    "        # Input array is an already formed ndarray instance\n",
    "        # We first cast to be our class type\n",
    "        obj = np.asarray(input_array).view(cls)\n",
    "        # Gradient is None until populated\n",
    "        obj.gradient = None\n",
    "        obj.index_in_graph = get()._register_parameter(obj)\n",
    "        obj.is_leaf = True\n",
    "        obj.last_operation_index = None\n",
    "        # Return newly created object\n",
    "        return obj\n",
    "\n",
    "    # Don't sweat over it, just assigning the same attributes as in new\n",
    "    def __array_finalize__(self, obj):\n",
    "        \"\"\"Re-assign data contained in parameter.\n",
    "\n",
    "        Workaround for `numpy` subclassing.\n",
    "\n",
    "        \"\"\"\n",
    "        if obj is None:\n",
    "            return\n",
    "        self.gradient = getattr(obj, \"gradient\", None)\n",
    "        self.index_in_graph = getattr(obj, \"index_in_graph\", None)\n",
    "        self.is_leaf = getattr(obj, \"is_leaf\", True)\n",
    "        self.last_operation_index = getattr(obj, \"last_operation_index\", None)\n",
    "\n",
    "    def broadcast_fix(self, gradient):\n",
    "        \"\"\"Try to fix numpy's broadcasting with gradient.\n",
    "\n",
    "        `1` dimensions may be broadcasted to other automatically. There is no\n",
    "        clear way to know about that which could be easily implemented.\n",
    "\n",
    "        Broadcasting is equal to summing all the values, hence __any__ dimension\n",
    "        which might be off in gradient is summed by the function below.\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(gradient, np.ndarray):\n",
    "            return gradient\n",
    "\n",
    "        if gradient.flatten().shape == self.flatten().shape:\n",
    "            return gradient\n",
    "        flattened_gradient = gradient.flatten()\n",
    "        flattened_data = self.flatten()\n",
    "        if len(flattened_gradient.shape) < len(flattened_data.shape):\n",
    "            raise ValueError(\n",
    "                \"Data has more dimension than gradient, something went very wrong.\"\n",
    "            )\n",
    "\n",
    "        to_sum = []\n",
    "        # Gradient cannot be None as the condition is checked above\n",
    "        for index, (data_shape, gradient_shape) in enumerate(\n",
    "            itertools.zip_longest(flattened_data.shape, flattened_gradient.shape)\n",
    "        ):\n",
    "            if data_shape is None or gradient_shape > data_shape:\n",
    "                to_sum.append(index)\n",
    "            if data_shape > gradient_shape:\n",
    "                raise ValueError(\"Data has more elements than it's gradient\")\n",
    "        return np.sum(flattened_gradient, axis=tuple(to_sum)).flatten()\n",
    "\n",
    "    def backward(self, upstream_gradient) -> None:\n",
    "        \"\"\"Take upstream gradient and update param's gradient with it.\"\"\"\n",
    "        self.gradient = self.broadcast_fix(upstream_gradient)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear gradient to save RAM memory.\"\"\"\n",
    "        self.gradient = None"
   ]
  },
  {
   "source": [
    "# Graph high-level\n",
    "​\n",
    "Graph optimization contains of three main things:\n",
    "- Parameters\n",
    "- Operations\n",
    "- Graph\n",
    "​\n",
    "## General\n",
    "​\n",
    "__All objects have `backward` method__:\n",
    "- For `graph` `backward` runs backpropagation on operations until all parameters\n",
    "are met\n",
    "- For `operations` `backward` has implementation of gradient of this operation\n",
    "which is passed along the graph during backward call\n",
    "- For `parameters` `backward` takes the `upstream_gradient` and saves it inside parameter\n",
    "​\n",
    "## Parameters\n",
    "​\n",
    "Parameters are simply `np.array` instances with additional attribute called\n",
    "`gradient`.\n",
    "​\n",
    "They can be created by wrapping `np.array` instances like this:\n",
    "​\n",
    "```\n",
    "my_array = np.array([1., 2., 3.])\n",
    "my_parameter = g.Parameter(my_array)\n",
    "```\n",
    "​\n",
    "`.gradient` parameter is, by default, equal to `None` (as there was no gradient calculated\n",
    "w.r.t. to this parameter).\n",
    "​\n",
    "Parameters have `backward(upstream_gradient)` method. All it does is it sets\n",
    "`.gradient = upstream_gradient` (with some possible summation, but that is out of scope).\n",
    "​\n",
    "## Operations\n",
    "​\n",
    "Operations consist of two methods:\n",
    "- `forward` - calculate forward pass value like a*b, a+b or others. Simple `numpy`\n",
    "- `backward` - calculates backward pass value, e.g. gradient of the `forward` operation\n",
    "w.r.t. `forward` arguments\n",
    "​\n",
    "> As many arguments to `forward` method, as many return values in `backward`!\n",
    "​\n",
    "Operation can take either `Parameter` instance or pure `np.array`. If it is the latter,\n",
    "__it is not registered as input__ and hence one could return anything in the `backward` call.\n",
    "​\n",
    "### Operation's `forward`\n",
    "​\n",
    "- We implement `forward` method, but use `__call__`\n",
    "- `__call__` __registers operation and it's input nodes into graph__\n",
    "​\n",
    "Regarding last one, `graph` keeps `id` of our current operation and also\n",
    "keeps the `id`s of it's inputs.\n",
    "​\n",
    "### Operation's `backward`\n",
    "​\n",
    "- Is run by graph __automatically__ when we call `backward` __on the whole graph__\n",
    "- Gets `upstream_gradient` (gradient coming from nodes which __are calculated after this one__, but __previous in terms of running backward__)\n",
    "- Returns gradients of this operation w.r.t to `forward` inputs __multiplied by `upstream_gradient`__\n",
    "​\n",
    "## Graph\n",
    "​\n",
    "- Keeps all the `parameters` and `operations`\n",
    "- Is global\n",
    "- __One can get it via `g.get()`__\n",
    "- Runs the backpropagation algorithm along the `operations` and `parameters`\n",
    "using `backward()` function (with default initial gradient of `1`)\n",
    "- Is called by `Parameter` and `Operation` (during `__init__` and `__call__` respectively)\n",
    "in order to register these as nodes\n",
    "​\n",
    "## Example\n",
    "​\n",
    "For `a=b+c` case, let's assume that:\n",
    "- `b` is an output from operation `d + e`\n",
    "- `d` is a Parameter instance, `e` is `np.array` (__not a Parameter!__)\n",
    "- `c` is a Parameter instance\n",
    "​\n",
    "Our code will look like that:\n",
    "​\n",
    "```python\n",
    "# Forward pass\n",
    "d = g.Parameter(np.array([1]))\n",
    "e = np.array([2])\n",
    "​\n",
    "b = g.add(d, e)\n",
    "c = g.Parameter(np.array([3]))\n",
    "​\n",
    "a = g.add(b, c)\n",
    "​\n",
    "# backward pass\n",
    "​\n",
    "g.get().backward()\n",
    "​\n",
    "# results\n",
    "d.gradient, c.gradient\n",
    "```\n",
    "​\n",
    "### `forward` pass\n",
    "​\n",
    "- `d` is created as Parameter and assigned `id=0` __during creation__\n",
    "​\n",
    "- `d + e` operation is run (it is `add(d, e)` precisely).\n",
    "This creates `_Add` node and runs `__call__` method. `__call__` method iterates\n",
    "over `forward` arguments (`d` and `e`). `d` is a `Parameter`, hence it is added to dictionary,\n",
    "namely `{0: 0}`, `e` is not (it's just `np.array`!), hence it is not registered\n",
    "  - dictionary meaning - it is simply saying that `0`th input to `forward` has\n",
    "  `id=0` inside graph. This also means that `0`th return value from operation's\n",
    "  `backward` should be passed to `0`th node in graph (to it's `backward` method precisely)\n",
    "- Next id is assigned, this time to `_Add` operation. It is kept inside operation (`self.index_in_graph`)\n",
    "and is registered inside `graph` as well. It will be the next `id`, hence `1`.\n",
    "- In total, to `self.parameters` (via Graph's `_register_operation`) the following is added:\n",
    "`{1: {0: 0}}`, which means:\n",
    "  - Node `1` in graph has `0`th input argument which is registered as `0`th node inside graph.\n",
    "​\n",
    "- Value of `forward` is returned and used in `a=b+c`\n",
    "- This time the idea is similar, but `c` is a Parameter, hence it will be given `id=2`\n",
    "- New addition operation will get and `id=3` and have the following dictionary registered\n",
    "inside the graph: `{3: {0: 1, 1: 2}}`, which means:\n",
    "  - This operation has `id=3`, it has two inputs along which we have to backpropagate.\n",
    "  `0`th input has `id=1`, `1`th input has `id=2` (and is a Parameter)\n",
    "​\n",
    "We get the result of `a = (d + e) + c`.\n",
    "​\n",
    "### `backward` pass\n",
    "​\n",
    "Having forward value calculated, we can now run `g.get().backward()` which runs the\n",
    "backpropagation:\n",
    "​\n",
    "- Last registered node is taken (`id=3`) and initial `upstream_gradient` (equal to `1`)\n",
    "- We run this node's `backward` function which gives us a `tuple` with two outputs.\n",
    "- Inputs dictionary of this node is taken (`{0: 1, 1: 2}`). This tells us that now,\n",
    "we have to get `0`th element of tuple and pass it as `upstream_gradient` to node\n",
    "with `id=1`. Same, we have to take `1`st element of tuple and pass it to `backward`\n",
    "method of node with `id=2`.\n",
    "- `1`st node has it's input nodes and the procedure is repeated, __but__ this time\n",
    "`upstream_gradient` is only passed to node with `id=0`. It occurs this node is a parameter,\n",
    "hence the algorithm __along this route__ stops by calling it's backward and populating\n",
    "`.gradient` attribute of parameter `d` (nothing is propagated to `e` as it's `np.array`)\n",
    "- `2`nd node is a parameter, hence it's `backward` is called and `c.gradient` is populated\n",
    "with this value\n",
    "​\n",
    "### Results\n",
    "​\n",
    "Given the code mentioned at the top, we obtain `gradient` for parameters\n",
    "​\n",
    "## Optimizer\n",
    "​\n",
    "- Gets parameters and applies optimization formula which consists of changing parameter(s)\n",
    "values using their gradient (usually by subtracting gradient from current value)\n",
    "- It is all it does really, simply access the parameter and it's `gradient` attribute\n",
    "and modify parameter using this value"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement operations\n",
    "\n",
    "Now that we have defined our classes we can implement concrete `operations`. Let's start easy with `addition` to see what this is all about.\n",
    "\n",
    "## Addition\n",
    "\n",
    "So all we have to implement, as seen previously, is `forward` and `backward`. We will also wrap the object in `add` function to use it easier afterwards.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Formula for addition is known to everyone:\n",
    "\n",
    "$$\n",
    "c = a + b\n",
    "$$\n",
    "\n",
    "Okay, so now, questions:\n",
    "- what is the derivative of `c` w.r.t. `a`?\n",
    "- what is the derivative of `c` w.r.t. `b`?\n",
    "\n",
    "So now we have all we need, let's put this in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete implementations\n",
    "class _Add(Operation):\n",
    "    # Simply add two values\n",
    "    def forward(self, a, b):\n",
    "        return a+b\n",
    "\n",
    "    # Return gradient for a and b as `tuple`\n",
    "    def backward(self, upstream_gradient):\n",
    "        return upstream_gradient, upstream_gradient\n",
    "\n",
    "# Create object and run it with (a, b)\n",
    "def add(a, b):\n",
    "    return _Add()(a, b)\n",
    "\n",
    "add(3,4)\n",
    "get().backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean\n",
    "\n",
    "We will also implement second operation (for now, in the whole chapter we will implement a few more).\n",
    "\n",
    "## Exercise\n",
    "\n",
    "### Part 1\n",
    "\n",
    "__We will talk about solutions after this part, but if you finish fast you can go to the second part__\n",
    "\n",
    "Formula for mean (denoted `m` here) of `N` variables is:\n",
    "\n",
    "$$\n",
    "m = \\frac{1}{N}\\sum_{i=1}^{N}x_i\n",
    "$$\n",
    "\n",
    "As we have `N` variables we will have to return `N` values. Fortunately, during implementation, as we take `mean` of `np.array` we will return `np.array` of the same shape.\n",
    "\n",
    "Derivative this time will be a little harder, so let's so let's start small and calculate derivative of `1` variable:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial m}{\\partial x_1}(\\frac{1}{1} * x_1) = 1\n",
    "$$\n",
    "\n",
    "Try the same with two variables, maybe three. What would the result be for `N` variables?\n",
    "\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Implement `_Mean` operation and `mean` function in analogous way we did with `_Add` and `add`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Mean(Operation):\n",
    "    def __init__(self, axis: int = None):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        mean = np.mean(inputs, axis=self.axis)\n",
    "        self.cache = np.ones_like(inputs) / inputs.size\n",
    "        return mean\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        return upstream_gradient * self.cache\n",
    "\n",
    "def mean(inputs, axis: int = None):\n",
    "    return _Mean(axis)(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running graph\n",
    "\n",
    "As we have everything in place, let's use it. __Remember to use functions NOT CLASSES__, it should be pretty natural.\n",
    "\n",
    "Your task is to create two `Parameters` from `np.random.randn` arrays of shape `10, 5`, __add__ them together, take the __mean__, backpropagate and check gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799]\n [-0.97727788  0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n [ 0.14404357  1.45427351  0.76103773  0.12167502  0.44386323]\n [ 0.33367433  1.49407907 -0.20515826  0.3130677  -0.85409574]\n [-2.55298982  0.6536186   0.8644362  -0.74216502  2.26975462]\n [-1.45436567  0.04575852 -0.18718385  1.53277921  1.46935877]\n [ 0.15494743  0.37816252 -0.88778575 -1.98079647 -0.34791215]\n [ 0.15634897  1.23029068  1.20237985 -0.38732682 -0.30230275]\n [-1.04855297 -1.42001794 -1.70627019  1.9507754  -0.50965218]\n [-0.4380743  -1.25279536  0.77749036 -1.61389785 -0.21274028]]\n[[-0.89546656  0.3869025  -0.51080514 -1.18063218 -0.02818223]\n [ 0.42833187  0.06651722  0.3024719  -0.63432209 -0.36274117]\n [-0.67246045 -0.35955316 -0.81314628 -1.7262826   0.17742614]\n [-0.40178094 -1.63019835  0.46278226 -0.90729836  0.0519454 ]\n [ 0.72909056  0.12898291  1.13940068 -1.23482582  0.40234164]\n [-0.68481009 -0.87079715 -0.57884966 -0.31155253  0.05616534]\n [-1.16514984  0.90082649  0.46566244 -1.53624369  1.48825219]\n [ 1.89588918  1.17877957 -0.17992484 -1.07075262  1.05445173]\n [-0.40317695  1.22244507  0.20827498  0.97663904  0.3563664 ]\n [ 0.70657317  0.01050002  1.78587049  0.12691209  0.40198936]]\n\n0.11961603106896997\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Parameter([[0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02]]),\n",
       " Parameter([[0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02],\n",
       "            [0.02, 0.02, 0.02, 0.02, 0.02]]))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x1 = Parameter(np.random.randn(10, 5))\n",
    "x2 = Parameter(np.random.randn(10, 5))\n",
    "print(x1)\n",
    "print(x2)\n",
    "print()\n",
    "\n",
    "forward_result = mean(add(x1, x2))\n",
    "print(forward_result)\n",
    "get().backward()\n",
    "\n",
    "x1.gradient, x2.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.mean(np.array([[[1,1,1],[1,1,1],[1,1,1]]]))\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- Learn more about graphs. You can start from [wikipedia](https://en.wikipedia.org/wiki/Graph_(abstract_data_type)) and move on to [graph traversal](https://en.wikipedia.org/wiki/Graph_traversal)\n",
    "- Code your own simple graph and implement [BFS](https://en.wikipedia.org/wiki/Breadth-first_search) and [DFS](https://en.wikipedia.org/wiki/Depth-first_search) on it. What are the advantages/disadvantages of one method over the other?\n",
    "- Go through this lesson multiple times. This one is hard so make sure you understand what is going on, ask questions if needed\n",
    "- Read about [reverse (backward) vs forward automatic differentiation](https://math.stackexchange.com/questions/2195377/reverse-mode-differentiation-vs-forward-mode-differentiation-where-are-the-be). Which frameworks implement both (tip: check out [JAX](https://github.com/google/jax))?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}