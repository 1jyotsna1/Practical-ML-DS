{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Intro\n",
    "\n",
    "As you should know by now, most tasks can be either defined as regression (predicting a continuous value) or classification (predicting a discrete value) problems.\n",
    "\n",
    "The simplest form of a classification problem is __binary classification__:\n",
    "\n",
    "> Special case of classification, where our targets (values to predict) can either take value `0` or `1`.\n",
    "\n",
    "`cat` vs `dog` is an example, where `cat=0` and `dog=1`. That is, every example has a label which is either `True` or `False`.\n",
    "\n",
    "See below for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 385.78125 262.19625\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 385.78125 262.19625 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\nL 378.58125 7.2 \r\nL 43.78125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"PathCollection_1\">\r\n    <defs>\r\n     <path d=\"M -3 3 \r\nL 3 -3 \r\nM -3 -3 \r\nL 3 3 \r\n\" id=\"m7247681bc7\" style=\"stroke:#ff0000;stroke-width:1.5;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p344525b7da)\">\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"58.999432\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"76.325156\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"89.642986\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"99.912003\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"106.197432\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"109.350507\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"129.187175\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"133.348321\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"137.626195\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"139.738718\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"141.707072\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"151.065161\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"153.940464\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"154.410499\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"156.097833\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"160.570193\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"161.552262\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"162.361241\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"164.401732\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"166.842055\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"169.287851\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"170.770833\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"173.107219\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"177.808615\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"183.095759\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"183.560213\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"185.412715\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"188.411719\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"195.509772\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"201.397224\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"204.121387\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"207.334375\" xlink:href=\"#m7247681bc7\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"225.90792\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"226.478662\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"226.762692\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"228.587235\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"233.590928\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"239.478765\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"242.361606\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"252.380783\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"257.145001\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"260.344109\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"262.429855\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"263.410026\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"264.449504\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"267.162449\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"276.213022\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"276.788989\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"334.185988\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#ff0000;stroke-width:1.5;\" x=\"363.363068\" xlink:href=\"#m7247681bc7\" y=\"17.083636\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 87.816469 224.64 \r\nL 87.816469 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_2\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m4f291947d9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"87.816469\" xlink:href=\"#m4f291947d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- âˆ’2 -->\r\n      <defs>\r\n       <path d=\"M 10.59375 35.5 \r\nL 73.1875 35.5 \r\nL 73.1875 27.203125 \r\nL 10.59375 27.203125 \r\nz\r\n\" id=\"DejaVuSans-8722\"/>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(80.445375 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 145.019645 224.64 \r\nL 145.019645 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"145.019645\" xlink:href=\"#m4f291947d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- âˆ’1 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(137.648551 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 202.222822 224.64 \r\nL 202.222822 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.222822\" xlink:href=\"#m4f291947d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(199.041572 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 259.425998 224.64 \r\nL 259.425998 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"259.425998\" xlink:href=\"#m4f291947d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(256.244748 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_9\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 316.629175 224.64 \r\nL 316.629175 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"316.629175\" xlink:href=\"#m4f291947d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(313.447925 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_11\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 373.832351 224.64 \r\nL 373.832351 7.2 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"373.832351\" xlink:href=\"#m4f291947d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(370.651101 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- X -->\r\n     <defs>\r\n      <path d=\"M 6.296875 72.90625 \r\nL 16.890625 72.90625 \r\nL 35.015625 45.796875 \r\nL 53.21875 72.90625 \r\nL 63.8125 72.90625 \r\nL 40.375 37.890625 \r\nL 65.375 0 \r\nL 54.78125 0 \r\nL 34.28125 31 \r\nL 13.625 0 \r\nL 2.984375 0 \r\nL 29 38.921875 \r\nz\r\n\" id=\"DejaVuSans-88\"/>\r\n     </defs>\r\n     <g transform=\"translate(207.75625 252.916562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-88\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_13\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 43.78125 214.756364 \r\nL 378.58125 214.756364 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_14\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma57484ab12\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma57484ab12\" y=\"214.756364\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 218.555582)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_15\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 43.78125 175.221818 \r\nL 378.58125 175.221818 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma57484ab12\" y=\"175.221818\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 179.021037)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_17\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 43.78125 135.687273 \r\nL 378.58125 135.687273 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma57484ab12\" y=\"135.687273\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 139.486491)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_19\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 43.78125 96.152727 \r\nL 378.58125 96.152727 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma57484ab12\" y=\"96.152727\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 99.951946)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_21\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 43.78125 56.618182 \r\nL 378.58125 56.618182 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma57484ab12\" y=\"56.618182\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 60.417401)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_23\">\r\n      <path clip-path=\"url(#p344525b7da)\" d=\"M 43.78125 17.083636 \r\nL 378.58125 17.083636 \r\n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma57484ab12\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Y -->\r\n     <defs>\r\n      <path d=\"M -0.203125 72.90625 \r\nL 10.40625 72.90625 \r\nL 30.609375 42.921875 \r\nL 50.6875 72.90625 \r\nL 61.28125 72.90625 \r\nL 35.5 34.71875 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 34.71875 \r\nz\r\n\" id=\"DejaVuSans-89\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 118.973906)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-89\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 43.78125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 224.64 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 7.2 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p344525b7da\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_binary_data(m=50): \n",
    "    X = np.random.randn(m) #sample from a normal distribution\n",
    "    X = np.array(sorted(X))\n",
    "    Y = X > 0.2    # return binary vector with true where X above some threshold and false if below\n",
    "    return X, Y #returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', marker='x')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "X, Y = make_binary_data()\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, the output of our model could be any real number, negative or positive, unbounded in magnitude. Here the thing is a little more involved.\n",
    "\n",
    "> In classification, we can interpret the model output as a confidence that the example belongs to a particular class.\n",
    "\n",
    "Which bring us to...\n",
    "\n",
    "## Logits\n",
    "\n",
    "> vector (in binary case a scalar) of unnormalized probabilities in the range $(-\\infty, \\infty)$\n",
    "\n",
    "Maybe you've noticed it is __exactly what we've been outputting with regression__!\n",
    "\n",
    "You know what probability is, so logits are pretty similar, but:\n",
    "- __50% chance (probability=$0.5$) is equal to logit=$0$__\n",
    "- Anything below zero is less probable, anything above more\n",
    "\n",
    "Logits are used for classification in order to:\n",
    "- not making unnecessary operation transforming them into probabilities if we are after label (we will see the transformation shortly)\n",
    "- for binary classification every output from our classifier which is greater than `1` is considered `True`, anything below is `False`\n",
    "\n",
    "__Note:__ mathematically, the term logit refers to the log of the odds of a probability and __transforms probability (p below) back into $(-\\infty, \\infty)$ range__.\n",
    "It can be written by the following formula:\n",
    "\n",
    "$$\n",
    "    L = \\ln \\frac{p}{1 - p}\n",
    "$$\n",
    "\n",
    "However, we will use the term logit later to simply refer to the values outputted by the model.\n",
    "\n",
    "## From logits to probabilities\n",
    "\n",
    "We can do this by applying a **sigmoid** function to our output:\n",
    "\n",
    "![](images/sigmoid.jpg)\n",
    "\n",
    "> sigmoid squashes logits from $(-\\infty, \\infty)$ to $(0, 1)$ range which we can easily interpret\n",
    "\n",
    "We can also write a function to compute the derivative of the sigmoid function.\n",
    "We will need this to differentiate our loss with respect to the model parameters, as they only affect the loss through the sigmoid.\n",
    "\n",
    "__Note:__ Sigmoid is the inverse function of logit\n",
    "\n",
    "![](images/sigmoid_deriv.jpg)\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's implement our sigmoid `class` and `sigmoid` function in code. We will code it as and `g.Operation`  as it was done up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import aicore.ml.graph as g\n",
    "import numpy as np\n",
    "\n",
    "class Sigmoid(g.Operation):\n",
    "    def forward(self, inputs):\n",
    "        self.cache=1/(np.exp(-inputs)+1)\n",
    "        return self.cache\n",
    "\n",
    "    def backward(self,upstream_gradient):\n",
    "        return upstream_gradient*self.cache*(1-self.cache)\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(inputs):\n",
    "    return Sigmoid()(inputs)\n",
    "\n",
    "print(sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we input some values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.37754067 0.62245933 0.5       ]\n[0.04742587 0.73105858 0.73105858]\n[3.72007598e-44 1.00000000e+00]\n[1.  0.5]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sigmoid(np.array([-0.5, 0.5, 0])),\n",
    "    sigmoid(np.array([-3, 1, 1])),\n",
    "    sigmoid(np.array([-100, 100])),\n",
    "    sigmoid(np.array([1000000, 0])),\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is one input which destroys our function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-2-49e7f18eb864>:6: RuntimeWarning: overflow encountered in exp\n  self.cache=1/(np.exp(-inputs)+1)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "sigmoid(np.array([-10000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "What happened, why did we get an error? Value of `np.exp` grows exponentially and `double` (`float64`) type cannot keep such large numbers in memory.\n",
    "\n",
    "Let's check what is the maximum `value` one can pass into `exp(value)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "maximum value: 709.782712893384\n",
      "exp(maximum value): 1.7976931348622732e+308\n",
      "\n",
      "OUTPUT BELOW AS A NON-ML ADDITIONAL CHALLENGE TO SOLVE\n",
      " See Challenges at the end\n",
      "\n",
      "double epsilon: 2.220446049250313e-16\n",
      "exp(maximum value + 255 * epsilon) == exp(maximum value): True\n",
      "exp(maximum value + 255*epsilon): 1.7976931348622732e+308\n",
      "exp(maximum value + 256*epsilon): inf\n",
      "<ipython-input-6-d164304c9247>:21: RuntimeWarning: overflow encountered in exp\n",
      "  print(f\"exp(maximum value + 256*epsilon): {np.exp(max_value_to_exp + 256*epsilon)}\")\n"
     ]
    }
   ],
   "source": [
    "# Get maximum double value in numpy\n",
    "max_double = np.finfo(\"d\").max\n",
    "max_value_to_exp = np.log(max_double)\n",
    "\n",
    "print(f\"maximum value: {max_value_to_exp}\")\n",
    "print(f\"exp(maximum value): {np.exp(max_value_to_exp)}\")\n",
    "\n",
    "# Get epsilon value for double\n",
    "print(\n",
    "    \"\\nOUTPUT BELOW AS A NON-ML ADDITIONAL CHALLENGE TO SOLVE\\n\",\n",
    "    \"See Challenges at the end\\n\",\n",
    ")\n",
    "epsilon = np.finfo(\"d\").eps\n",
    "print(f\"double epsilon: {epsilon}\")\n",
    "print(\n",
    "    \"exp(maximum value + 255 * epsilon) == exp(maximum value): {}\".format(\n",
    "        np.exp(max_value_to_exp + 255 * epsilon) == np.exp(max_value_to_exp)\n",
    "    )\n",
    ")\n",
    "print(f\"exp(maximum value + 255*epsilon): {np.exp(max_value_to_exp + 255*epsilon)}\")\n",
    "print(f\"exp(maximum value + 256*epsilon): {np.exp(max_value_to_exp + 256*epsilon)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural logarithm $\\ln$ (`np.log` in `numpy`) is inverse of `exp`, hence we can get the maximum this way which is around `709` for double (default) type.\n",
    "\n",
    "__Note:__ `epsilon` is the smallest change which is registered by type. See challenges at the end for more info, this one will be additional (https://en.wikipedia.org/wiki/Octuple-precision_floating-point_format).\n",
    "\n",
    "So, how can we solve the overflow issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable sigmoid\n",
    "\n",
    "`sigmoid` formula can be written as:\n",
    "\n",
    "$$\n",
    "    S(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^{x}}{1 + e^{x}}\n",
    "$$\n",
    "\n",
    "Our previous implementation explodes __only when the value is large (`709`) and negative__. That is because the negative sign is inverted with `-z` and we get positive `709`. Given this:\n",
    "\n",
    "- $e^x$ will overflow when $x$ is positive\n",
    "- $e^{-x}$ will overflow when $x$ is negative\n",
    "\n",
    "This leads us to another exercise...\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Implement stable version of sigmoid. We will use two `helper` functions:\n",
    "- `_negative_sigmoid` - used when values passed as `inputs` are `negative`\n",
    "- `_positive_sigmoid` - used when values passed as `inputs` are `positive`\n",
    "\n",
    "Formulas are provided above, that's the only two functions you have to implement.\n",
    "\n",
    "__Read `sigmoid` code and make sure you understand what is going on and why!__ (you can read [this stackoverflow answer](https://stackoverflow.com/a/64717799/10886420) for in-depth explanation, but that's optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_sigmoid(inputs):\n",
    "    # Second formula for negative values\n",
    "    # Cache exp so you won't have to calculate it twice\n",
    "    return np.exp(inputs)/(np.exp(inputs)+1)\n",
    "\n",
    "\n",
    "def _positive_sigmoid(inputs):\n",
    "    # First formula for positive values\n",
    "    return 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "\n",
    "def sigmoid(inputs):\n",
    "    positive = inputs >= 0\n",
    "    # Boolean array inversion is faster than another comparison\n",
    "    negative = ~positive\n",
    "\n",
    "    # empty contains junk hence will be faster to allocate than zeros\n",
    "    result = np.empty_like(inputs)\n",
    "    result[positive] = _positive_sigmoid(inputs[positive])\n",
    "    result[negative] = _negative_sigmoid(inputs[negative])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version should work correctly for all of the cases below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.37754067 0.62245933 0.5       ]\n[0.04742587 0.73105858 0.73105858]\n[3.72007598e-44 1.00000000e+00]\n[1 0]\n[0]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sigmoid(np.array([-0.5, 0.5, 0])),\n",
    "    sigmoid(np.array([-3, 1, 1.])),\n",
    "    sigmoid(np.array([-100., 100.])),\n",
    "    sigmoid(np.array([1000, 0])),\n",
    "    sigmoid(np.array([-1000])),\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss function for binary classification-  Binary cross entropy (BCE)\n",
    "\n",
    "Basic formula for BCE is:\n",
    "\n",
    "$$\n",
    "    BCE = - (y\\ln\\hat{y} + (1-y)\\ln(1-\\hat{y}))\n",
    "$$\n",
    "\n",
    "We can break it down into parts exclusively:\n",
    "- if the label is `1` $(1-y)\\ln(1-\\hat{y})$ is `0`\n",
    "- if the label is `0` $y\\ln\\hat{y}$ is `0`\n",
    "\n",
    "![](./images/bce.jpg)\n",
    "\n",
    "### Why not calculate only one part per label?\n",
    "\n",
    "Can't we calculate one part for `1` label and the second for `0` label with some kind of `if label == 0` switch? Yes, we could, but sometimes in machine learning & deep learning we use technique called `label smoothing`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label smoothing\n",
    "\n",
    "> Label smoothing changes `{0, 1}` labels into \"soft targets\", for example `{0.1, 0.9}`\n",
    "\n",
    "### Why would we do that?\n",
    "\n",
    "- It is impossible to reach `1` value for sigmoid in practice. Hence, even if our predicted probability is `0.99999` we will still have some loss left\n",
    "- Same thing happens for `0` as it is really hard for the classifier to reach exactly this value\n",
    "\n",
    "### How to do that?\n",
    "\n",
    "- there are many implementations. Usually we subtract from largest labels (`1`) some constant and add it to `0` labels (for binary case)\n",
    "- $\\alpha$ is our __smoothing hyperparameter__, usually around `0.1`\n",
    "\n",
    "### How does it help?\n",
    "\n",
    "- Our classifier is __less confident__ about it's predictions. Due to inherit noise in data it is often desirable (especially for the more complex and powerful models, yes, neural networks are prime example)\n",
    "- Predictions are more smooth and gradual. Instead of having rough jumps from `0.999` probability to `0.0001` probability in another case, our algorithms try to distribute the probability more evenly\n",
    "- Classifier __does not try to \"be sure\" about hard examples so much__. As it has to reach `0.9` (let's say) instead of `1`, `0.8` will be also fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy implementation\n",
    "\n",
    "Let's start with naive implementation, simply follow the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(prediction, label):\n",
    "    return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.47407698 0.47407698 0.69314718]\n"
     ]
    }
   ],
   "source": [
    "prediction = sigmoid(np.array([-0.5, 0.5, 0])) \n",
    "labels = np.array([0, 1, 1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as usual, __numerical instability creeps in__. As a rule of thumb, always look suspicious at `exp` and it's inverse function `ln`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[inf]\n",
      "<ipython-input-9-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([1])\n",
    "labels = np.array([0])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[inf]\n",
      "<ipython-input-9-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([0])\n",
    "labels = np.array([1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nan]\n",
      "<ipython-input-9-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n",
      "<ipython-input-9-f3031382e7cc>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([0])\n",
    "labels = np.array([0])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nan]\n",
      "<ipython-input-9-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n",
      "<ipython-input-9-f3031382e7cc>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([1])\n",
    "labels = np.array([1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- if our model __is very condfident and IS WRONG__ (first two cases) we are left with  `np.log(0)` (which goes towards $-\\infty$, when we take `-` it is simply $\\infty$ we observe). \n",
    "- if our model __is very condfident and IS RIGHT__ (last two cases) we will be left with $ 0 * \\ln{0} $, hence with $ 0 * \\infty $ which is undefined `NaN` (not a number)\n",
    "\n",
    "Can we improve it somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable BCE\n",
    "\n",
    "To stabilize binary cross entropy loss we can mix activation we saw earlier (`sigmoid`) and `binary cross entropy` itself.\n",
    "\n",
    "### Formulation\n",
    "\n",
    "One can derive numerically stable version. It takes some math so a few steps are skipped. You can see the formula that we will use [here](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    BCE & = - (y\\ln\\hat{y} + (1-y)\\ln(1-\\hat{y})) \\\\\n",
    "        & = - (y\\ln(\\frac{1}{1+e^{-x}}) + (1-y)\\ln(1-\\frac{1}{1+e^{-x}}) \\\\\n",
    "        & ... \\\\\n",
    "        & = x - xy + \\ln(1 + e^{-x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Unfortunately this leaves us with $e^{-x}$ once again. After a few math tricks we come to this formula (really, go through it yourself to see some beautiful math tricks):\n",
    "\n",
    "$$\n",
    "    \\max(0, x) - xy + \\ln(1 + e^{-|x|})\n",
    "$$\n",
    "\n",
    "where `x` are logits, `y` are labels\n",
    "\n",
    "You can see no matter the `x` value it will always be negative so `e` can only underflow which is fine for our case (and $\\ln(1 + 0) = 0$). There are also [other formulas](https://discuss.pytorch.org/t/numerical-stability-of-bcewithlogitsloss/8246) but we won't go into details.\n",
    "\n",
    "### Derivative\n",
    "\n",
    "We also have to calculate derivative of `BCEWithLogits` to backpropagate through our model.\n",
    "\n",
    "It is a little cumbersome, but remember you can always use [Wolfram Alpha](https://www.wolframalpha.com/) for such tasks (or it will get you some idea or direction at least).\n",
    "\n",
    "Given that, the derivative is presented as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial BCE}{\\partial x} &= \\sigma(x) - y \\\\\n",
    "    \\frac{\\partial BCE}{\\partial y} &= -x\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "__Implement stable binary cross entropy with logits__\n",
    "\n",
    "- Formula for `forward` and `backward` was already provided\n",
    "- Use `sigmoid` (the stable version) in your implementation of `backward`!\n",
    "- Name the function `bce_with_logits` (and name `class` appropriately according to [`PEP8`](https://www.python.org/dev/peps/pep-0008/) standard, __essentially CamelCase__ with underscore at the beginning)\n",
    "\n",
    "__This binary cross entropy will work directly on `logits`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _BCEWithLogits(g.Operation):\n",
    "    def forward(self, logits, targets):\n",
    "        self.cache = (logits, targets)\n",
    "        return (\n",
    "            np.maximum(np.zeros_like(logits), logits)\n",
    "            - logits * targets\n",
    "            + np.log(1 + np.exp(-np.abs(logits)))\n",
    "        )\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        return sigmoid(self.cache[0]) - self.cache[1], -self.cache[0]\n",
    "\n",
    "\n",
    "def bce_with_logits(logits, targets):\n",
    "    return _BCEWithLogits()(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.89962981 0.4295872  0.47407698]\n"
     ]
    }
   ],
   "source": [
    "prediction = sigmoid(np.array([-0.5, 0.5, 0])) \n",
    "labels = np.array([0, 1, 1])\n",
    "\n",
    "print(bce_with_logits(prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally let's build our binary classifier model class\n",
    "\n",
    "We'll use the chain rule to compute the gradient with our previously defined loss function:\n",
    "\n",
    "![](images/binary_classification.jpg)\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's create `BinaryLogisticRegression` class with `sklearn`-like API!\n",
    "\n",
    "For that, we will have to code quite a few methods:\n",
    "- `__init__` - taking `n_features` and `optimizer` (same idea as previously)\n",
    "- `parameters` - returning parameters\n",
    "- `predict_logits(self, X)` - it is essentially linear regression `predict` method\n",
    "- `predict_proba(self, X)` - use `with g.no_grad()` context manager, and return logits  predictions with `sigmoid` function applied on it\n",
    "- `predict(self, X)` - use `with g.no_grad()` context manager and return logits predictions thresholded at `0` value\n",
    "- `fit(self, X, y, epochs: int)` - like previously with linear regression, this time we will use `g.mean` on `bce_with_logits` function (__we are not doing batches here!__)\n",
    "\n",
    "We will make our `BinaryLogisticRegression` with `sklearn` like API and fully stable during `forward` and `backward` pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression():\n",
    "    def __init__(self,n_features,optimizer):\n",
    "        self.W= g.Parameter(np.random.randn(n_features))\n",
    "        self.b = g.Parameter(np.random.randn(1))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def parameters(self):\n",
    "        return(self.W,self.b)\n",
    "    \n",
    "    def predict_logits(self,X):\n",
    "        return(g.add(np.dot(X,self.W),self.b))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        with g.no_grad():\n",
    "            return sigmoid(self.predict_logits(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with g.no_grad():\n",
    "            return self.predict_logits(X) > 0\n",
    "  \n",
    "    def fit(self, X, y, epochs: int = 100):\n",
    "        for _ in range(epochs):\n",
    "                y_pred = self.predict_logits(X)\n",
    "                print(y_pred)\n",
    "                # loss is our final node\n",
    "                g.mean(bce_with_logits(y_pred, y))\n",
    "                g.get().backward()\n",
    "                self.optimizer(self.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our binary classifier\n",
    "\n",
    "We will load [breast cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) dataset and split it as per usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, model_selection\n",
    "\n",
    "from aicore.ml import data\n",
    "\n",
    "(X_train, y_train), (X_validation, y_validation), (X_test, y_test) = data.split(\n",
    "    datasets.load_breast_cancer(return_X_y=True)\n",
    ")\n",
    "\n",
    "X_train, X_validation, X_test = data.standardize_multiple(X_train, X_validation, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss before fit: 2.5815617375925086\nValidation loss before fit: 3.074668939997718\nTest loss before fit: 3.074668939997718\n"
     ]
    }
   ],
   "source": [
    "def calculate_loss(model, X, y_true):\n",
    "    with g.no_grad():\n",
    "        y_pred = model.predict_logits(X)\n",
    "        return g.mean(bce_with_logits(y_pred, y_true))\n",
    "\n",
    "\n",
    "optimizer = g.optimizers.SGD(lr=1e-3)\n",
    "model = BinaryLogisticRegression(n_features=X_train.shape[1], optimizer=optimizer)\n",
    "\n",
    "print(f\"Training loss before fit: {calculate_loss(model, X_train, y_train)}\")\n",
    "print(\n",
    "    f\"Validation loss before fit: {calculate_loss(model, X_validation, y_validation)}\"\n",
    ")\n",
    "print(f\"Test loss before fit: {calculate_loss(model, X_validation, y_validation)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-2.70322369e+00  8.15048231e-01 -1.17009028e-01  5.22700326e-01\n -5.74597682e-01 -1.25334473e-01  3.33620456e+00  3.11157308e+00\n  6.37868935e-01 -6.06839935e+00  2.97957874e+00  6.44377283e+00\n  1.44279593e+00  3.18456515e+00 -1.27347737e+00 -4.82375025e+00\n -8.21145516e-01  3.03155833e+00  2.84841085e+00  7.73637565e-01\n -5.11448073e+00  4.31536101e+00  1.33079037e+00  5.99271326e+00\n -5.12428425e+00  1.20393902e+00 -4.16211475e+00  1.56429848e+00\n -1.00985190e+00  1.39123523e+00 -4.77501603e-01  1.09955759e+00\n -1.36741607e+00  3.59253011e+00 -2.54996906e+00  6.18362740e+00\n -2.24380570e+00  3.97078933e-01  7.51678857e+00 -7.91425147e+00\n -1.08550017e+01  3.74492689e+00  6.40607699e-02  1.42245619e+00\n  5.97905712e+00 -8.08352060e+00 -4.01943894e-01  9.50619049e+00\n -8.28556629e-01 -1.75215161e+01 -5.94393825e-01 -3.79094523e+00\n -2.56100695e+00  4.40612768e+00  8.04299841e+00  5.91859881e-01\n -3.49514703e-01  7.03080514e+00 -5.61379676e+00  4.56911694e+00\n -3.77629539e-01 -4.54855945e+00 -1.14318239e+01 -8.06439926e+00\n  1.91661298e-01  4.53381445e+00  2.19411911e+00 -7.92924708e-01\n  3.49051539e+00  6.44901549e+00 -1.18849898e+00  5.50562021e-01\n -2.29472941e-01  9.59731725e-01  2.38788047e+00 -1.16688156e+01\n  6.66947090e-01  6.99253149e+00 -1.65560868e+00  3.48785567e+00\n  1.06798080e+00 -2.87679480e-02  4.84983838e+00 -6.97866604e-01\n -1.04931788e+01  2.04762607e+00  2.86973809e+00  1.10616707e+00\n  1.88360834e+00 -2.24491632e+00 -4.79484269e+00  7.78281160e+00\n  2.70256528e+00  1.94718956e+00  1.08459126e+00 -4.95916227e-01\n  2.51151305e+00 -1.21714885e+01  6.55526423e+00  1.18582150e+00\n  2.01814241e+00 -5.04631778e+00  3.98197818e+00  4.35793965e-01\n  3.93757230e+00 -2.36328951e+00  4.86889055e+00 -1.22535124e+00\n -5.83090979e+00 -3.95904447e+00  7.65738849e+00  9.95154615e-01\n -2.63924168e+01  6.11967252e+00  2.81475887e+00  2.06774743e+00\n  5.86905359e+00  7.17462819e+00  3.64251228e+00  1.08445427e+00\n  2.15887922e+00 -7.93589912e+00 -6.01663123e+00  7.14474069e+00\n  6.20971171e+00  2.68792918e+00  6.78751314e+00 -7.24013727e-01\n  4.01695010e+00  7.57353890e+00  4.36289705e+00 -4.44553596e+00\n  1.45224961e+00  2.30186147e-01  3.37498314e+00  5.33259117e-01\n  3.91176162e-01  1.23579535e+01 -5.96710315e+00  5.76330793e+00\n  5.10995956e+00 -1.12750085e+01 -2.16482540e+00 -3.19418611e+00\n  2.27062798e+00  2.46079162e+00 -4.62899251e+00  4.31054773e+00\n -2.59394672e+00 -1.05655384e+00 -4.11691843e+00 -2.17072088e-01\n -3.90642826e+00 -3.80394168e+00 -5.14242674e+00  5.92673571e+00\n -2.83038302e+00  6.38822149e-01  4.58139587e+00 -1.23911106e+00\n  3.55119985e+00  5.16266749e+00  4.77108605e+00  4.86967438e+00\n  2.77367685e+00  4.25902459e-01 -2.32943020e+00 -4.08483688e+00\n -6.85291877e-01  8.36674677e+00  5.49814284e+00  2.16942074e+00\n  3.72156808e+00 -9.02949169e-01  8.18636163e+00  8.71619266e-01\n  4.89367151e+00 -1.38964516e+01  2.37296336e+00  4.30855020e+00\n  1.36272518e+00  3.53590682e+00  3.27903203e-01  4.90480996e+00\n  6.38836445e+00  2.86354453e+00  2.49519652e+00  8.74644870e+00\n  3.47391672e+00 -3.80027666e+00 -6.41240052e+00 -3.80938550e+00\n  5.27777497e+00  3.40944405e-02  3.07319652e+00  9.40189955e-01\n  9.35145612e+00  5.89918858e-01  1.62823350e+00 -5.25357693e+00\n -2.18673000e+00  1.62159293e+00  1.36899715e+00  9.14885025e+00\n -3.08741805e+00  1.67226394e+00 -3.33181000e+00 -2.52535806e+00\n  4.72056606e+00  2.29093867e-02 -8.52590728e-01 -1.77746976e+00\n -3.29069054e+00 -3.27699535e+00  4.66119401e+00  1.76142649e+00\n -4.31778785e+00  4.46824736e-01  1.83262248e+00  4.00854557e+00\n -3.08272593e+00  5.57989036e+00 -2.48326812e+00  2.96855392e+00\n  5.76923670e-02 -4.45163167e+00 -2.43492050e+00 -4.80441482e+00\n  2.17872757e+00 -9.54943947e-01  5.79319853e+00 -6.01919963e+00\n -4.08932856e+00 -1.47631586e+00 -8.39192883e+00  2.19724970e-01\n -1.78134604e+00 -1.57870075e+01 -1.00765552e+00 -1.25543938e+01\n  3.80445033e+00  2.86716414e+00  2.97248144e+00 -1.38384964e+01\n  7.84014419e+00  5.43755634e+00  2.93728087e+00  1.57516308e+00\n  3.01714610e+00  1.55335347e+00 -1.56915375e+00  1.11298185e+00\n  1.75321871e+00  1.31346972e+01  7.19987138e+00 -1.17072017e+01\n -2.05481106e+01 -4.13042508e-01 -2.24539194e+00  1.68445904e+00\n  4.02506562e+00  2.92633689e+00 -6.29878544e+00  9.21903679e+00\n  1.23941316e+00 -2.24904175e+00  1.71083948e+00  4.20758675e+00\n -1.66747566e+00 -1.62305089e+00  4.80308628e+00  2.09690768e+00\n -4.58258539e+00 -1.80283406e+00 -2.76687880e-01  6.63296888e-01\n -6.59119240e+00 -3.12617045e+00  2.36950866e+00  2.95099092e+00\n  8.94574732e-01 -5.02900140e+00  9.34325552e-01 -5.39771611e+00\n -1.05676552e+01  4.35052009e+00  6.58738719e+00  3.79709019e+00\n -6.56459580e+00 -1.09306549e-01 -1.60408165e+00  3.06078381e-01\n  3.10827351e+00  6.04990744e+00 -2.42216847e+00 -2.14042864e+00\n -4.89331318e+00 -9.29052418e-02 -9.07279918e+00 -1.09102993e+00\n -6.32986145e+00 -3.39381103e+00  7.84204214e-01 -3.26199029e+00\n -1.11867180e+00  8.83973967e-01  3.13412001e+00 -1.56922076e+00\n -1.66682708e+00  5.43078480e+00 -3.85067034e+00 -1.71068680e+00\n -3.42521877e+00  2.49589451e+00  1.27544170e+00  2.69809678e+00\n  2.66119471e+00  2.57384945e+00  3.60004328e+00  2.81440770e+00\n -2.09146466e-01  1.78356574e+00 -1.54168976e+01 -2.29835986e+00\n -2.61347543e+00 -3.54416814e+00  5.30869336e+00  1.18660032e-01\n -2.80080543e+00 -8.25135005e+00  6.84528050e+00 -2.34366672e+00\n -3.81017199e-01 -7.19583883e+00  1.01987120e+00 -1.75494060e+00\n  1.82886790e+00 -1.31730314e+00 -3.30906516e+00 -7.81646994e+00\n  5.04648136e-01]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-37d92373ad38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Training loss after fit: {calculate_loss(model, X_train, y_train)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m print(\n\u001b[0;32m      5\u001b[0m     \u001b[1;34mf\"Validation loss after fit: {calculate_loss(model, X_validation, y_validation)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-3f9be3c638e2>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, epochs)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbce_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aicore\\ml\\graph\\optimizers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, parameters)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparameter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mparameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\aicore\\ml\\graph\\optimizers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, parameter)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mparameter\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10000)\n",
    "\n",
    "print(f\"Training loss after fit: {calculate_loss(model, X_train, y_train)}\")\n",
    "print(\n",
    "    f\"Validation loss after fit: {calculate_loss(model, X_validation, y_validation)}\"\n",
    ")\n",
    "print(f\"Test loss after fit: {calculate_loss(model, X_validation, y_validation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- What is `log-sum-exp` trick? Check out [this wiki article](https://en.wikipedia.org/wiki/LogSumExp) for more info\n",
    "- Implement non-stable binary cross entropy on your own (which takes __probabilities after sigmoid__ instead of `logits`)\n",
    "- As always, use `sklearn` for this classification task as well\n",
    "- Try implementing function called `binary_smoothing(labels, alpha)` which, given labels, smooths the targets with `alpha` parameter\n",
    "- Use batches, check what is inside `aicore` library in `aicore.ml.data` module\n",
    "- Go around [PEP8](https://www.python.org/dev/peps/pep-0008/) style guide and try to follow conventions provided there from now on (for variable naming, function and class naming at least)\n",
    "- Try to explain the numerical phenomena with epsilon (this one is additional and out of the scope of this course, but you might have some fun)\n",
    "\n",
    "## Summary\n",
    "\n",
    "- the labels for classification problems should be an integer representing the index of the class which that example belongs to\n",
    "- `logits` are unnormalized probability $(-\\infty, \\infty)$\n",
    "- `sigmoid` transforms `logits` into probabilities\n",
    "- naive implementation of `sigmoid` is numerically unstable, more stable version exists\n",
    "- binary cross entropy is a new differentiable loss function that can be optimised to solve classification problems\n",
    "- binary classification can be implemented by having a single boolean integer label for each example, where 1 represents it being a member of that class and 0 represents it not being a member of that class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}