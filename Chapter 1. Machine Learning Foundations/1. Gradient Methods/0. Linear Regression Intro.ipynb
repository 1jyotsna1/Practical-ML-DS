{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python\n",
    "- Linear algebra\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "- Know the difference between monovariate and multivariate regressions\n",
    "- Implement your first machine learning algorithm from scratch, in Python\n",
    "- Use analytical solution to solve for it\n",
    "- See how to optimize linear regression using analytical solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "\n",
    "Once again we will use `Boston` dataset from `sklearn`, we saw it previously, easy stuff by now. Let's also split it into validation and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, model_selection\n",
    "\n",
    "# 15% for validation and test, 70% for train in total\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_validation, X_test, y_validation, y_test = model_selection.train_test_split(\n",
    "    X_test, y_test, test_size=0.5\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is linear regression?\n",
    "\n",
    "Classic starting point for machine learning adventures, something like `Hello World` but in ML world.\n",
    "\n",
    "__Linear regression predicts continuous outputs__ - hence the regression part of the name.\n",
    "Linear regression makes predictions that are simply a __`w`eight__ed combination (a linear combination) of the inputs (plus some offset called __`b`ias__). It is described by linear function:\n",
    "\n",
    "$$\n",
    "    y = wx + b\n",
    "$$\n",
    "\n",
    "\n",
    "![](images/linear_model.jpg)\n",
    "\n",
    "In future we will experience much more complex, nonlinear relationships between features and labels that we wish to model. \n",
    "\n",
    "But __do not underestimate linear regression__ as it is often used in statistics and to explain a lot of phenomenas, at the end of the chapter we will see when it should be used in real world.\n",
    "\n",
    "Functions that a model represent are often referred to as the **hypothesis**.\n",
    "\n",
    "![](images/linear_model_example.jpg)\n",
    "\n",
    "We will make our model able to make predictions for many examples at a time by expressing the hypothesis in vector form as shown below.\n",
    "\n",
    "![](images/linear_model_vector.jpg)\n",
    "\n",
    "Here's an example of what that computation might look like numerically.\n",
    "\n",
    "![](images/linear_model_vector_example.jpg)\n",
    "\n",
    "\n",
    "## Mathematical formula of model\n",
    "\n",
    "Formula below presents linear regression for single example __but multiple features__:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y = w_1x_1 + w_2x_2 + ... + w_Nx_N + b = \\sum_{i=1}^{N} w_ix_i + b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Essentially:\n",
    "- each feature in our sample is multiplied by bias\n",
    "Now let's implement our first machine learning model in code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple features\n",
    "\n",
    "We will go for multiple features, so here is how our weights will look like:\n",
    "\n",
    "![title](images/w_vector.jpg)\n",
    "\n",
    "The weights variable (w) becomes a row vector so we need to transpose it when we multiply it by the X matrix (or take a `dot` product of `data` and `weights`).\n",
    "\n",
    "![title](images/vector_linear_regression.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monovariate vs multivariate\n",
    "\n",
    "A dychotomy you might sometimes come across:\n",
    "\n",
    "> monovariate linear regression is linear regression done with __one or multiple features__ but __predicting single target__\n",
    "\n",
    "And __multivariate__ (as you may of guessed) would be\n",
    "\n",
    "> linear regression with __one or multiple variables (features)__ but __predicting multiple targets__ (which are correlated with each other)\n",
    "\n",
    "In this notebook we will be doing __monovariate__ only, but we will get to __multivariate__ when we do multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "`LinearRegression` implementation is our task!\n",
    "\n",
    "- Create a class `LinearRegression` which takes a single `n_features` argument during initialization.\n",
    "    - Create `W` and `b` variables inside initialization. One of shape `(n_features, 1)` and `bias` of shape `1` initialized with random normal distribution\n",
    "- Create `__call__` function (what does it do, what is a functor?) which takes `X` (`np.array`). It should return predictions our linear regression should do (see formulas above in the picture, it's two operations only)\n",
    "- Create `update_params` function which takes `W` and `b` and assigns them to appropriate variables in `self`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(n_features=13)  # instantiate our linear model\n",
    "y_pred = model(X_train)  # make prediction on data\n",
    "print(\"Predictions:\\n\", y_pred[:10]) # print first 10 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(y_pred, y_true):\n",
    "    samples = len(y_pred)\n",
    "    plt.figure()\n",
    "    plt.scatter(np.arange(samples), y_pred, c='r', label='predictions')\n",
    "    plt.scatter(np.arange(samples), y_true, c='b', label='true labels', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Sample numbers')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_pred[:10], y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "As you can see predictions of our model are __way off__. This happens because we initialized our model with random weights and bias.\n",
    "\n",
    "Now, we should learn how we can improve this model to learn from data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss - how do we know how good our model is?\n",
    "\n",
    "> Our **loss** should measure __how poor our model performs__. \n",
    "\n",
    "The larger the value, the worse so we will later try to __minimize it__ (bring as close to zero as possible). We will use it to give our model feedback about it's performance. \n",
    "\n",
    "> Loss values needs to return a **single number**, not a vector, not a matrix.\n",
    "\n",
    "__NOTE:__ minimising the objective is equivalent to maximising the negative of it. \n",
    "\n",
    "Commonly, loss value is also called __cost function__ though it is not exact. Let's go over the difference now.\n",
    "\n",
    "### Squared Error loss\n",
    "\n",
    "> loss is a function which takes prediction and true label and returns __a positive scalar__\n",
    "\n",
    "- The higher the loss value, the worse our model performs\n",
    "- __Loss is defined on a single data point__\n",
    "\n",
    "Squared error is one of the loss functions __used for regression tasks__ and is simply defined as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    (\\hat{y} - y)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This does exactly what you think: it calculates the error (difference between our model's prediction $\\hat{y}$ and the true value $y$):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\hat{y} - y\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and then squares it to make the value positive. As long as the error is not zero it will increase the value of loss regardles of whether our prediction is below (negative error) or above (positive error) the value of the label.\n",
    "\n",
    "### Mean Squared Error (MSE) cost function\n",
    "\n",
    "> cost function is a generalization of loss functions for many data samples\n",
    "\n",
    "So, __loss__ operates on single sample, while __cost__ operates on multiple of them.\n",
    "In case of __Mean Squared Error__ we calculate squared error for each sample and take the mean of that value:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    L_{mse} = \\frac{1}{N}\\sum_{i}^{N}(\\hat{y_i} - y_i)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "There are many other criterions that are useful for different tasks (e.g. the binary cross entropy (BCE) loss for classification, which we will cover later).\n",
    "\n",
    "Let's write a function to calculate the cost using the mean squared error loss function. It should take in an array of predictions for different example inputs as well as an array of corresponding example labels. It should return a single number (scalar) that represents the MSE loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement `mean_squared_error` function taking `y_pred` and `y_true`. Every formula is above (focusing on the last one is enough ;) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cost = mean_squared_error(y_pred, y_train)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analytical solution to minimising mean square error\n",
    "\n",
    "Now that we have our __loss__ equation we can calculate it's derivative w.r.t. weights. When we set it to zero we can calculate __weights values (`W`)__ which minimize it.\n",
    "\n",
    "![](images/analytical_linear_reg.jpg)\n",
    "\n",
    "Now let's implement this analytical solution for least squares regression in code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now that we have mathematical formula we can jump in straight to the implementation.\n",
    "\n",
    "- For matrix inverse, you can use [`np.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) function\n",
    "- Remember to return `weights` part of `matrix` first and `bias` after that (`bias` is the `0` element of the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minimize_loss(X_train, y_train):\n",
    "    X_with_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    ...\n",
    "    return ..., ... # Return weights and bias here\n",
    "\n",
    "\n",
    "weights, bias = minimize_loss(X_train, y_train)\n",
    "print(weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you didn't notice, this analytical solution has no mention of the model bias. \n",
    "In fact, we incorporate the model bias into our features matrix by adding an extra column filled with `1`.\n",
    "\n",
    "![](images/bias_in_weight_matrix.jpg)\n",
    "\n",
    "Doing this makes the analytical solution much clearer and means we have to solve it only for one value $W$, rather than also for $b$.\n",
    "\n",
    "In practice (iterative optimization), we treat them as separate variables (we will later see more about that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameters\n",
    "\n",
    "Now that we have found `optimal_w` we should update our model and see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_params(weights, bias)\n",
    "y_pred = model(X_train)\n",
    "cost = mean_squared_error(y_pred, y_train)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success, __BUT__\n",
    "\n",
    "__Congratulations, we have trained our first model from scratch!__\n",
    "\n",
    "We should talk about scale though... Let's plot our labels with respect to certain features to see how that looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_label(X_train, y_true, feature, n_samples: int = 20):\n",
    "    features = X_train[:n_samples, feature]\n",
    "    labels = y_true[:n_samples]\n",
    "    plt.figure()\n",
    "    plt.scatter(features, labels, c='r', label='targets')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Feature values')\n",
    "    plt.ylabel('Target values')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(13):\n",
    "    plot_feature_label(X_train, y, i, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization\n",
    "\n",
    "As you could notice in our plot above different features has different value ranges.\n",
    "- some features were binary or between `[0, 1]`\n",
    "- others has values ranging in hundreds or even thousands\n",
    "\n",
    "This is problematic to most of machine learning models.\n",
    "\n",
    "### Why is it a problem?\n",
    "\n",
    "> Small change in weight connected to feature with large values changes output significantly\n",
    "\n",
    "Let's take two weights `a` and `b` and single example `x` with two features:\n",
    "- first has value `0.1`\n",
    "- second has value `1000`\n",
    "\n",
    "Now, formula for linear regression would be the following:\n",
    "\n",
    "$$\n",
    "    \\hat{y} = 0.1a + 1000b\n",
    "$$\n",
    "\n",
    "Now, let's see impact of `a` and `b` on $\\hat{y}$:\n",
    "- $a = 10, b = 0.001$ - `a` and `b` have the same impact on $\\hat{y}$\n",
    "- $a = 1, b = 1$ - `b` has `10000` times (!!!) more impact on $\\hat{y}$\n",
    "\n",
    "It is unlikely `a` has `10000` times less impact on the value we want to predict (and is unlikely in real world).\n",
    "\n",
    "> We should assume all variables are __equally important unless we verify them__ via statistical testing or other measures\n",
    "\n",
    "The range of values __is not an important factor__, relative differences between values are.\n",
    "\n",
    "### Possible solution\n",
    "\n",
    "We can normalize our data, which means:\n",
    "\n",
    "> Normalization is a process of bringing features to the same value range\n",
    "\n",
    "This ensures that relative difference between values for each feature are important, not the scale.\n",
    "\n",
    "> We should always normalize our features (unless they are not continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization & standardization\n",
    "\n",
    "There are a lot of schemes to put values in the `[0, 1]` range. Here we will use `minmax` approach\n",
    "We can do this by subtracting the minimum then dividing by the range (feature normalisation).\n",
    "\n",
    "![title](images/normalisation.jpg)\n",
    "\n",
    "We can alternatively use a similar method called standardisation, where we subtract the mean then divide by the standard deviation.\n",
    "\n",
    "![](images/standardisation.jpg)\n",
    "\n",
    "Feature normalisation puts gradients of each different model parameter on the same order of magnitude. This converts loss surfaces that might look like *valleys* into loss surfaces that look like *bowls*. Feature normalisation means that we should be able to make progress with optimisation for all model parameters using the same learning rate.\n",
    "\n",
    "![](images/bowl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "We will implement standardization scheme.\n",
    "\n",
    "Formula was given above. In this case:\n",
    "- use `mean` and `std` if those are __both__ not `None`\n",
    "- otherwise calculate `mean` and `std` from dataset\n",
    "- standardize dataset with those values\n",
    "- Return `tuple` containing:\n",
    "    - standardized_dataset\n",
    "    - `tuple` with:\n",
    "        - mean\n",
    "        - std\n",
    "        \n",
    "Do you have any idea why we would like to do it this way? __Tip:__ We may want to normalize another dataset on which we should not calculate any values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(dataset, mean=None, std=None):\n",
    "    ...\n",
    "\n",
    "X_train, (mean, std) = standardize_data(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test again\n",
    "\n",
    "Now that we have our data standardized, let's see how our model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = minimize_loss(X_train, y_train)\n",
    "model.update_params(weights, bias)\n",
    "y_pred = model(X_train)\n",
    "cost = mean_squared_error(y_pred, y_train)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success?\n",
    "\n",
    "Our loss is exactly the same? Trust me we did everything correctly, so why this happened?\n",
    "See challenges at the end.\n",
    "\n",
    "> __Always normalize and sanitize your input data__ (though in this case it didn't change anything)\n",
    "\n",
    "In the next chapter we will see other tricks which will help you to improve the score even further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of computing the analytical solution\n",
    "\n",
    "This solution involves inverting a matrix of size $R^{n \\times n}$. \n",
    "Here $n$ is the number of features that each example has. \n",
    "\n",
    "With `560` features it is becoming more difficult. Furthermore, here, we only have ~500 samples, while in real life we can have millions or more.\n",
    "\n",
    "However, as we will see, most problems of practical interest contain examples with many more features. \n",
    "\n",
    "> For example, 1080p images have more than 1,000,000 features each. \n",
    "\n",
    "The time complexity of inverting a matrix of size $n \\times n$ is around $O(n^3)$. \n",
    "This means that computing the analytical solution for these kinds of real world problems is often computationally expensive or even impossible.\n",
    "\n",
    "Analytical solutions however, are not the only approach that we can take (and usually we __even cannot use them__ as the close form cannot be calculated).\n",
    "\n",
    "We will see how to update parameters iteratively soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- What other normalization schemes exist? Check out unit vector normalization\n",
    "- Try things presented in this notebook on different datasets. Maybe find one of your own and preprocess it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- linear regression is \"hello world\" basic machine learning model\n",
    "- linear regression updates it's weight vector and bias in order to improve on the task\n",
    "- this update can be carried out via analytically calculated formula\n",
    "- the MSE loss is appropriate for many regression problems and is the most common loss function for this task\n",
    "- normalization scheme almost always improves our scores __sometimes our solution will diverge without it!__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
