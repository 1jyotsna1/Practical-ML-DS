{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting and Adaboost\n",
    "\n",
    "## Learning objectives\n",
    "- implement\n",
    "    - your first boosted model - Adaboost\n",
    "- understand\n",
    "    - the ideas behind boosting\n",
    "    - how to apply boosting to decision trees and implement Adaboost\n",
    "\n",
    "## Intro - What is boosting?\n",
    "\n",
    "We begin by describing an algorithm called AdaBoost (Adaptive Boosting).\n",
    "AdaBoost is a classification algorithm that combines a sequence of weak classifiers to repeatedly modified versions of the data, which increasingly prioritise misclassified examples. The example labels are coded as $Y \\in {-1, 1}$.\n",
    "\n",
    "Weak classifiers are those who's predictions are only slightly better than random guessing.\n",
    "In this case, we use classification trees with a depth of 1.\n",
    "We call such limited trees \"stumps\".\n",
    "AdaBoost converts many \"weak learners\" into a single \"strong learner\" by combining these stumps.\n",
    "\n",
    "AdaBoost combines the predictions of all of the classifiers to make a prediction by evaluating:\n",
    "## $H(x) =  sign(\\Sigma^L_{l=1} \\alpha_l H_l(x)) $\n",
    "\n",
    "This is simply the sign of a weighted combination of predictions.\n",
    "\n",
    "### But where do the weights for each model come from?\n",
    "\n",
    "The models are applied sequentially, and because of their limited capacity, each of them is likely to make some mistake.\n",
    "For the case where each model is a stump, mistakes will be made on any dataset that is not linearly separable.\n",
    "\n",
    "The error of a model is calculated as:\n",
    "## $err_l = \\frac{\\Sigma_{i=1}^m w_i I(y_i \\neq H_l(x_i))}{\\Sigma_{i=1}^m w_i}$\n",
    "\n",
    "The weight of the prediction of each model is then computed based on the error rate given by:\n",
    "## $ \\alpha_l = \\frac{1}{2} log(\\frac{1-err_l}{err_l})$\n",
    "# show graph of this error curve\n",
    "\n",
    "Each model other than the first is trained on a dataset bootstrapped from the original.\n",
    "**But the sampling of examples will be weighted.**\n",
    "The weights of each example are increased if they were incorrectly classified by the previous model, and decreased if they were already classified correctly.\n",
    "\n",
    "For the first model in the sequence, the importance of classifying each example correctly is equal.\n",
    "That is, we weight the error contribution for each example in the dataset by the same equal amount, $w_i= \\frac{1}{m}$.\n",
    "For the next weighted sample from the dataset, \n",
    "To sample the bootstrapped dataset for the next model in the sequence to be trained on, we set the weight of each example to:\n",
    "## $w_i \\leftarrow w_i \\cdot e^{\\alpha_l \\cdot I(y_i \\neq H_l(x_i))}$\n",
    "Where $I(true) = +1$ and $I(false) = -1$.\n",
    "This shifts the weight of correctly classified examples down and that of incorrectly classified examples up.\n",
    "\n",
    "### What does this weight calculation do?\n",
    "Most examples may be correctly classified by our very simple weak classifier stumps.\n",
    "It is the edge cases that need extra attention.\n",
    "So sequentially, the importance of examples which are not able to be classified correctly by the previous model are increased and vice versa.\n",
    "Models later in the sequence hence focus on harder to classify examples.\n",
    "As depth increases, the importance of easy to classify examples dimishes and tends to zero.\n",
    "This effectively removes them from the dataset, leaving less examples for the later models to classify.\n",
    "Less examples are separable with a simpler decision boundary.\n",
    "\n",
    "The weighting of each model prediction serves to increase the influence of models that correctly classify examples from the bootstrapped dataset which they are trained on.\n",
    "\n",
    "# algorithm outline\n",
    "\n",
    "![](images/boosting.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree\n",
    "\n",
    "class BoostedModel:\n",
    "    def __init__(self, model_class, layers=10):\n",
    "        self.model_class = model_class\n",
    "        self.models = []\n",
    "\n",
    "    def calc_error(self, Y_hat, Y):\n",
    "        \"\"\"Compute classifier error rate\"\"\"\n",
    "        diff = Y_hat - Y\n",
    "        return np.sum(diff)\n",
    "\n",
    "    def calc_model_weight(self, error):\n",
    "        return 0.5 * np.log( (1 - error) / error)\n",
    "\n",
    "    def resample(self, X, error):\n",
    "        weights = calc_model_weight(error)\n",
    "        weights = weights / sum(weights) # normalise\n",
    "        return np.random.choice(X, len(X), weights)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        error = Y\n",
    "        for layer_idx in range(layers):\n",
    "            model = self.model_class()\n",
    "            model.fit(X, error)\n",
    "            error = model.predict(X) - Y\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            prediction += model.predict(X)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualise our predictions\n",
    "\n",
    "Firstly visualise the predictions for each classifier, then visualise their successive combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "- perform adaptive boosting with a model that is not a decision tree\n",
    "- adapt the above code to work for a regression model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit4e36a22c5ab145cdaac0d5c6b1a34fd0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}