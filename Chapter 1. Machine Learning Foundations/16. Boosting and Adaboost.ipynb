{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting and Adaboost\n",
    "\n",
    "## Learning objectives\n",
    "- implement\n",
    "    - your first boosted model - Adaboost\n",
    "- understand\n",
    "    - the ideas behind boosting\n",
    "    - how to apply boosting to decision trees and implement Adaboost\n",
    "\n",
    "## Intro - What is boosting?\n",
    "\n",
    "We begin by describing an algorithm called AdaBoost (Adaptive Boosting).\n",
    "AdaBoost is a classification algorithm that combines a sequence of weak classifiers to repeatedly modified versions of the data, which increasingly prioritise misclassified examples. The example labels are coded as $Y \\in {-1, 1}$.\n",
    "\n",
    "Weak classifiers are those who's predictions are only slightly better than random guessing.\n",
    "In this case, we use classification trees with a depth of 1.\n",
    "We call such limited trees \"stumps\".\n",
    "AdaBoost converts many \"weak learners\" into a single \"strong learner\" by combining these stumps.\n",
    "\n",
    "AdaBoost combines the predictions of all of the classifiers to make a prediction by evaluating:\n",
    "## $H(x) =  sign(\\Sigma^L_{l=1} \\alpha_l H_l(x)) $\n",
    "\n",
    "This is simply the sign of a weighted combination of predictions.\n",
    "\n",
    "### But where do the weights for each model come from?\n",
    "\n",
    "The models are applied sequentially, and because of their limited capacity, each of them is likely to make some mistake.\n",
    "For the case where each model is a stump, mistakes will be made on any dataset that is not linearly separable.\n",
    "\n",
    "The error of a model is calculated as:\n",
    "## err_l = proportion incorrect\n",
    "## $err_l = \\frac{\\Sigma_{i=1}^m w_i I(y_i \\neq H_l(x_i))}{\\Sigma_{i=1}^m w_i}$\n",
    "\n",
    "The weight of the prediction of each model is then computed based on the error rate given by:\n",
    "## $ \\alpha_l = \\frac{1}{2} log(\\frac{1-err_l}{err_l})$\n",
    "# show graph of this error curve\n",
    "\n",
    "Large negative model weight = your model sucks.\n",
    "Large positive model weight = your model rocks.\n",
    "Zero model weight = your model is as good as a random guessing.\n",
    "\n",
    "Each model other than the first is trained on a dataset bootstrapped from the original.\n",
    "**But the sampling of examples will be weighted.**\n",
    "The weights of each example are increased if they were incorrectly classified by the previous model, and decreased if they were already classified correctly.\n",
    "\n",
    "For the first model in the sequence, the importance of classifying each example correctly is equal.\n",
    "That is, we weight the error contribution for each example in the dataset by the same equal amount, $w_i= \\frac{1}{m}$.\n",
    "For the next weighted sample from the dataset, \n",
    "To sample the bootstrapped dataset for the next model in the sequence to be trained on, we set the weight of each example to:\n",
    "## $w_i \\leftarrow w_i \\cdot e^{- \\alpha_l \\cdot y_i H_l(x_i))}$\n",
    "Let's consider what this means for a variety of cases:\n",
    "- positive model weight and correct classification: weight of example pushed down\n",
    "- negative model weight and correct classification: weight of example pushed up\n",
    "- positive model weight and incorrect classification: weight of example pushed up\n",
    "- negative model weight and incorrect classification: weight of example pushed down\n",
    "\n",
    "\n",
    "### What does this weight calculation do?\n",
    "Most examples may be correctly classified by our very simple weak classifier stumps.\n",
    "It is the edge cases that need extra attention.\n",
    "So sequentially, the importance of examples which are not able to be classified correctly by the previous model are increased and vice versa.\n",
    "Models later in the sequence hence focus on harder to classify examples.\n",
    "As depth increases, the importance of easy to classify examples dimishes and tends to zero.\n",
    "This effectively removes them from the dataset, leaving less examples for the later models to classify.\n",
    "Less examples are separable with a simpler decision boundary.\n",
    "\n",
    "The weighting of each model prediction serves to increase the influence of models that correctly classify examples from the bootstrapped dataset which they are trained on.\n",
    "\n",
    "# algorithm outline\n",
    "\n",
    "![](images/boosting.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "error 0.9\nz 0.11989010989010986\n-1.0605898533359355\n[ 1  1  1 -1 -1  1  1 -1 -1  1]\n[ 1 -1  1 -1 -1  1  1 -1 -1  1]\n-1.0605898533359355\n[0.10965044 0.013146   0.10965044 0.10965044 0.10965044 0.10965044\n 0.10965044 0.10965044 0.10965044 0.10965044]\ntrained model 0\n\nerror 0.6\nz 0.6657377049180329\n-0.20342976123755235\n[ 1  1  1 -1 -1  1  1 -1 -1  1]\n[ 1 -1  1  1 -1 -1  1 -1  1  1]\n-0.20342976123755235\n[0.11543411 0.07684884 0.11543411 0.07684884 0.11543411 0.07684884\n 0.11543411 0.11543411 0.07684884 0.11543411]\ntrained model 1\n\nerror 0.6\nz 0.6657377049180329\n-0.20342976123755235\n[ 1  1  1  1 -1  1  1 -1  1  1]\n[ 1 -1  1  1  1  1  1  1 -1  1]\n-0.20342976123755235\n[0.11543411 0.07684884 0.11543411 0.11543411 0.07684884 0.11543411\n 0.11543411 0.07684884 0.07684884 0.11543411]\ntrained model 2\n\nerror 0.4\nz 1.4734146341463412\n0.19379129372317094\n[-1 -1  1 -1 -1  1  1 -1 -1  1]\n[-1  1 -1  1 -1 -1  1  1  1  1]\n0.19379129372317094\n[0.07787866 0.11474756 0.11474756 0.11474756 0.07787866 0.11474756\n 0.07787866 0.11474756 0.11474756 0.07787866]\ntrained model 3\n\nerror 0.8\nz 0.2569135802469135\n-0.679507757133448\n[-1 -1  1 -1 -1  1  1 -1 -1  1]\n[-1 -1  1  1 -1  1 -1 -1 -1  1]\n-0.679507757133448\n[0.11745599 0.11745599 0.11745599 0.03017604 0.11745599 0.11745599\n 0.03017604 0.11745599 0.11745599 0.11745599]\ntrained model 4\n\nerror 0.5\nz 0.9903921568627451\n-0.004827148122182299\n[ 1  1  1 -1 -1  1  1 -1 -1  1]\n[ 1 -1 -1  1  1  1 -1 -1 -1  1]\n-0.004827148122182299\n[0.10048271 0.09951729 0.09951729 0.09951729 0.09951729 0.10048271\n 0.09951729 0.10048271 0.10048271 0.10048271]\ntrained model 5\n\nerror 0.7\nz 0.43253521126760575\n-0.4190457712942921\n[-1 -1  1 -1 -1  1  1 -1 -1  1]\n[-1 -1  1  1  1  1  1 -1 -1 -1]\n-0.4190457712942921\n[0.12051669 0.12051669 0.12051669 0.05212771 0.05212771 0.12051669\n 0.12051669 0.12051669 0.12051669 0.05212771]\ntrained model 6\n\nerror 0.6\nz 0.6657377049180329\n-0.20342976123755235\n[-1 -1  1 -1 -1  1  1 -1 -1  1]\n[-1  1  1  1  1  1 -1 -1 -1  1]\n-0.20342976123755235\n[0.11543411 0.07684884 0.11543411 0.07684884 0.07684884 0.11543411\n 0.07684884 0.11543411 0.11543411 0.11543411]\ntrained model 7\n\nerror 0.4\nz 1.4734146341463412\n0.19379129372317094\n[ 1 -1  1  1  1  1  1  1 -1  1]\n[-1  1 -1  1  1  1 -1 -1  1  1]\n0.19379129372317094\n[0.11474756 0.11474756 0.11474756 0.07787866 0.07787866 0.07787866\n 0.11474756 0.11474756 0.11474756 0.07787866]\ntrained model 8\n\nerror 0.6\nz 0.6657377049180329\n-0.20342976123755235\n[-1 -1  1 -1 -1  1  1 -1 -1  1]\n[-1  1 -1 -1 -1 -1 -1 -1 -1  1]\n-0.20342976123755235\n[0.11543411 0.07684884 0.07684884 0.11543411 0.11543411 0.07684884\n 0.07684884 0.11543411 0.11543411 0.11543411]\ntrained model 9\n\n"
    }
   ],
   "source": [
    "import sklearn.tree\n",
    "from utils import get_classification_data\n",
    "import numpy as np\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_layers=10):\n",
    "        self.n_layers = n_layers\n",
    "        self.models = []\n",
    "\n",
    "    def calc_model_error(self, predictions, labels):\n",
    "        \"\"\"Compute classifier error rate\"\"\"\n",
    "        diff = predictions == labels\n",
    "        diff = diff.astype(int)\n",
    "        # weighted_diffs = weights * diff\n",
    "        return np.mean(diff)\n",
    "\n",
    "    def encode_labels(self, labels):\n",
    "        labels[labels == 0] = -1\n",
    "        labels[labels == 1] = +1\n",
    "        return labels\n",
    "\n",
    "    def calc_model_weight(self, error, delta=0.01):\n",
    "        print('error', error)\n",
    "        z = (1 - error) / (error + delta) + delta\n",
    "        print('z', z)\n",
    "        return 0.5 * np.log(z)\n",
    "\n",
    "    def sample(self, X, Y, weights):\n",
    "        idxs = np.random.choice(len(X), len(X), p=weights, replace=True)\n",
    "        return X[idxs], Y[idxs]\n",
    "\n",
    "    def update_weights(self, predictions, labels, model_weight):\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "        print(model_weight)\n",
    "        weights = np.exp(- model_weight * predictions * labels)\n",
    "        weights /= np.sum(weights)\n",
    "        return weights\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        features = X\n",
    "        labels = self.encode_labels(Y)\n",
    "        for layer_idx in range(self.n_layers):\n",
    "            model = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "            model.fit(features, labels)\n",
    "            self.models.append(model)\n",
    "            predictions = model.predict(X)\n",
    "            model_error = self.calc_model_error(predictions, labels)\n",
    "            model_weight = self.calc_model_weight(model_error)\n",
    "            print(model_weight)\n",
    "\n",
    "            example_weights = self.update_weights(predictions, labels, model_weight)\n",
    "            print(example_weights)\n",
    "            features, labels = self.sample(X, Y, example_weights)\n",
    "            print(f'trained model {layer_idx}')\n",
    "            print()\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            prediction += model.predict(X)\n",
    "        return prediction\n",
    "\n",
    "X, Y = get_classification_data()\n",
    "adaBoost = AdaBoost()\n",
    "adaBoost.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualise our predictions\n",
    "\n",
    "Firstly visualise the predictions for each classifier, then visualise their successive combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we boost other models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "- perform adaptive boosting with a model that is not a decision tree\n",
    "- adapt the above code to work for a regression model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit4e36a22c5ab145cdaac0d5c6b1a34fd0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}