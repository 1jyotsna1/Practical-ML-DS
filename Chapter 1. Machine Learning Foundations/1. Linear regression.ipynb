{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python\n",
    "- Linear algebra\n",
    "\n",
    "## Learning objectives\n",
    "- Implement your first machine learning algorithm from scratch, in Python\n",
    "- Understand the 4 parts of the general machine learning algorithm framework\n",
    "    - Data\n",
    "    - Models\n",
    "    - Criteria\n",
    "    - Optimisation\n",
    "- Implement a random search algorithm as a trivial optimisation technique\n",
    "\n",
    "## Intro\n",
    "\n",
    "This is the classic starting point for jumping into machine learning.\n",
    "\n",
    "Linear regression is a very simple method for making predictions of continuous values.\n",
    "\n",
    "## The problem\n",
    "\n",
    "I just bought a new red wine. But i'm not sure it will live up to my high standards. I dare not take a sip if it is below a 6/10 on my quality scale. How can I possibly hope to continue?\n",
    "\n",
    "I know! I will build a machine learning algorithm to predict the quality of my wine, given the information that I can find out about it. Obviously.\n",
    "\n",
    "## The general framework for machine learning algorithms\n",
    "\n",
    "Almost all machine learning algorithms consist of 4 components:\n",
    "1. the data\n",
    "2. the model\n",
    "3. the criterion\n",
    "4. the optimiser\n",
    "\n",
    "This notebook will introduce you to all of those. By the end, we will have used all of them to implement our first machine learning algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "The data represents the input-output relationship that our algorithm will learn.\n",
    "\n",
    "Our aim is to produce a mathematical function that takes in an example and makes a prediction. \n",
    "\n",
    "We will be working with a dataset that has many examples of different red wines. Yes, red wines.\n",
    "\n",
    "Each example contains 12 features of the wine. One of these is the wine quality, which we will use as a label. \n",
    "\n",
    "# csv scr\n",
    "![](images/wine.png)\n",
    "\n",
    "### How can we represent these data points numerically?\n",
    "\n",
    "Firstly, we need to separate the label (output) from the features (input). So for each example, we will have a single scalar label $y$.\n",
    "\n",
    "In our case, each example has several features. We can group these together mathematically as a vector, $x$. It will have as many rows as there are features in the example. Let's call this number of features per example $n$. \n",
    "\n",
    "![](images/single_data_point.jpg)\n",
    "\n",
    "We have $m$ examples, and indicate an arbitrary example with an $i$.\n",
    "\n",
    "We can stack these examples in columns to produce a design matrix $X$ which will then contain all of our data, as shown below.\n",
    "\n",
    "![](images/design_matrix.jpg)\n",
    "\n",
    "The scalar labels for each example can also be arranged into a single vector.\n",
    "\n",
    "![image](images/labels.jpg)\n",
    "\n",
    "Please note that this is just one specific example, and that other problems may have wildly different input output formats. The label could consist of many more values; it could even be something like an image. The same is true for the input. As long as it can be represented mathematically, it will be possible to create a model that processes examples of that type.\n",
    "\n",
    "For the rest of this notebook, we will just use a single feature of each example. This will make things easier to visualise and prevent some problems which we will address [later](Multivariate Regression & Feature Normalisation.ipynb)\n",
    "\n",
    "Now we know what our data should look like, let's get it into that format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(20, 1) (20, 1)\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_regression_data\n",
    " \n",
    "X, Y = get_regression_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data represented in the form we want. Can we get any closer to predicting whether or not I should sip this wine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Model\n",
    "\n",
    "## What does \"model\" mean?\n",
    "\n",
    "Imagine having a complete and perfect simulation of the world. This would allow you to take whatever action you wanted and see the outcome. This would mean that it would allow you to simulate different trajectories of events. That is, it would allow us to model it. Using your experience of what trajectories led to what outcomes, you'd be able to make more and more accurate predictions. \n",
    "On a smaller scale than the whole world/universe, we can build models of specific things we care about, so that we can again query them to make predictions from.\n",
    "\n",
    "## Our model\n",
    "\n",
    "In this dataset, we can see something like a straight-line relationship between the acidity and the quality.\n",
    "This indicates that the relationship may be well modelled by a linear equation, which is generally given in the form $y=wx + b$.\n",
    "\n",
    "Linear regression uses such straight line equations to model the input-output relationship of the data.\n",
    "\n",
    "In real problems of interest, this is rarely the case and we are likely to experience much more complex, nonlinear relationships between features and labels, as you can imagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHypothesis:\n",
    "    def __init__(self): # initalize parameters \n",
    "        self.w = np.random.randn() ## randomly initialise weight\n",
    "        self.b = np.random.randn() ## randomly initialise bias\n",
    "        \n",
    "    def __call__(self, X): # how do we calculate output from an input in our model?\n",
    "        ypred = self.w * X + self.b ## make a prediction using a linear hypothesis\n",
    "        return ypred # return prediction\n",
    "    \n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w ## set this instance's weights to the new weight value passed to the function\n",
    "        self.b = new_b ## do the same for the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input: [[63.    7.51  4.    7.73  5.87  6.57  3.73  1.98  4.76  8.44  5.62]\n [31.    7.95  7.2   3.85  1.57  2.6   8.79  0.61  1.5   4.64  5.18]\n [63.    3.98  8.16  0.47  2.29  0.68  0.41  8.72  3.19  9.02  9.66]\n [70.    6.18  2.26  1.58  2.25  6.2   9.94  1.26  7.35  3.7   0.98]\n [42.    6.49  5.46  1.5   1.94  1.91  6.68  1.02  9.25  0.1   6.35]\n [55.    5.26  8.41  1.28  9.68  0.95  9.46  3.29  8.29  8.88  5.48]\n [32.    6.09  2.01  6.26  2.08  7.49  4.09  7.93  0.29  2.09  4.64]\n [20.    6.74  8.91  7.91  9.46  8.11  9.17  2.84  2.22  6.82  6.4 ]\n [42.    3.48  0.23  9.25  1.87  8.02  3.25  6.4   1.72  1.95  7.71]\n [54.    4.47  5.81  5.09  2.44  9.37  9.98  1.88  1.67  0.9   1.7 ]\n [73.    5.28  3.64  6.2   3.93  0.03  9.51  2.6   6.01  9.38  8.51]\n [46.    1.6   6.26  8.08  4.75  5.84  0.81  8.39  1.84  9.68  2.13]\n [57.    6.43  8.27  6.8   4.73  0.28  1.66  8.58  8.33  5.23  6.56]\n [59.    1.13  6.97  0.69  2.49  1.92  4.81  6.34  7.95  5.33  8.67]\n [64.    6.11  9.12  6.79  0.67  7.24  3.07  6.21  4.26  1.26  5.05]\n [58.    2.12  1.42  7.31  5.6   2.18  2.88  0.63  9.58  5.33  9.54]\n [53.    6.34  7.2   9.21  7.78  8.55  7.41  2.89  0.05  4.59  2.12]\n [68.    5.35  5.48  8.76  0.83  4.29  9.29  1.05  8.82  5.27  4.9 ]\n [69.    8.67  8.01  6.86  5.56  4.15  4.49  4.79  5.65  4.52  0.82]\n [54.    7.36  3.88  3.86  3.41  1.05  4.22  6.36  1.37  9.95  5.21]\n [62.    4.11  9.28  5.11  5.34  4.5   2.03  2.81  2.8   3.52  1.65]\n [21.    4.22  8.    1.23  0.48  7.16  6.51  7.3   6.76  8.39  7.21]\n [26.    2.33  4.83  5.83  3.08  9.9   1.26  0.21  0.71  8.64  0.08]\n [39.    7.25  6.67  0.55  7.72  7.08  1.61  9.62  7.61  2.94  0.91]\n [39.    4.99  0.17  4.91  0.16  2.    8.79  7.13  0.06  5.82  7.96]\n [70.    0.03  5.1   1.78  5.96  6.89  5.86  2.04  6.46  8.1   3.56]\n [34.    6.1   6.69  1.06  5.24  1.08  7.29  5.9   5.7   5.7   8.92]\n [44.    4.87  4.39  7.95  3.98  3.1   4.33  3.98  2.49  8.08  0.4 ]] \n\nW: -0.8872014897057007 B: 0.6624357632499479 \n\nPrediction: [[-55.23125809  -6.00044742  -2.8863702   -6.19563175  -4.54543698\n   -5.16647802  -2.64682579  -1.09422319  -3.56064333  -6.82554481\n   -4.32363661]\n [-26.84081042  -6.39081608  -5.72541496  -2.75328997  -0.73047058\n   -1.64428811  -7.13606533   0.12124285  -0.66836647  -3.45417915\n   -3.93326795]\n [-55.23125809  -2.86862617  -6.57712839   0.24545106  -1.36925565\n    0.05913875   0.29868315  -7.07396123  -2.16773699  -7.34012167\n   -7.90793063]\n [-61.44166852  -4.82046944  -1.3426396   -0.73934259  -1.33376759\n   -4.83821347  -8.15634704  -0.45543811  -5.85849519  -2.62020975\n   -0.2070217 ]\n [-36.6000268   -5.0955019   -4.18168437  -0.66836647  -1.05873513\n   -1.03211908  -5.26407019  -0.24250976  -7.54417802   0.57371561\n   -4.9712937 ]\n [-48.13364617  -4.00424407  -6.79892877  -0.47318214  -7.92567466\n   -0.18040565  -7.73049033  -2.25645714  -6.69246459  -7.21591347\n   -4.1994284 ]\n [-27.72801191  -4.74062131  -1.12083923  -4.89144556  -1.18294334\n   -5.98270339  -2.96621833  -6.37307205   0.40514733  -1.19181535\n   -3.45417915]\n [-17.08159403  -5.31730228  -7.24252951  -6.35532802  -7.73049033\n   -6.53276832  -7.4732019   -1.85721647  -1.30715154  -5.3882784\n   -5.01565377]\n [-36.6000268   -2.42502542   0.45837942  -7.54417802  -0.99663102\n   -6.45292018  -2.22096908  -5.01565377  -0.8635508   -1.06760714\n   -6.17788772]\n [-47.24644468  -3.3033549   -4.49220489  -3.85341982  -1.50233587\n   -7.6506422   -8.1918351   -1.00550304  -0.81919072  -0.13604558\n   -0.84580677]\n [-64.10327299  -4.0219881   -2.56697766  -4.83821347  -2.82426609\n    0.63581972  -7.7748504   -1.64428811  -4.66964519  -7.65951421\n   -6.88764891]\n [-40.14883276  -0.75708662  -4.89144556  -6.50615227  -3.55177131\n   -4.51882094  -0.05619744  -6.78118474  -0.97001498  -7.92567466\n   -1.22730341]\n [-49.90804915  -5.04226982  -6.67472056  -5.37053437  -3.53402728\n    0.41401935  -0.81031871  -6.94975302  -6.72795265  -3.97762803\n   -5.15760601]\n [-51.68245213  -0.34010192  -5.52135862   0.05026674  -1.54669595\n   -1.0409911   -3.6050034   -4.96242168  -6.39081608  -4.06634818\n   -7.02960115]\n [-56.11845958  -4.75836534  -7.42884182  -5.36166235   0.06801077\n   -5.76090302  -2.06127281  -4.84708549  -3.11704258  -0.45543811\n   -3.81793176]\n [-50.79525064  -1.21843139  -0.59739035  -5.82300713  -4.30589258\n   -1.27166348  -1.89270453   0.10349882  -7.83695451  -4.06634818\n   -7.80146645]\n [-46.35924319  -4.96242168  -5.72541496  -7.50868996  -6.23999183\n   -6.92313697  -5.91172728  -1.90157654   0.61807569  -3.40981907\n   -1.21843139]\n [-59.66726554  -4.08409221  -4.1994284   -7.10944929  -0.07394147\n   -3.14365863  -7.57966608  -0.2691258   -7.16268138  -4.01311609\n   -3.68485154]\n [-60.55446703  -7.02960115  -6.44404817  -5.42376646  -4.27040452\n   -3.01945042  -3.32109893  -3.58725937  -4.35025265  -3.34771497\n   -0.06506946]\n [-47.24644468  -5.8673672   -2.77990602  -2.76216199  -2.36292132\n   -0.2691258   -3.08155452  -4.98016571  -0.55303028  -8.16521906\n   -3.959884  ]\n [-54.3440566   -2.98396236  -7.57079406  -3.87116385  -4.07522019\n   -3.32997094  -1.13858326  -1.83060042  -1.82172841  -2.46051348\n   -0.80144669]\n [-17.96879552  -3.08155452  -6.43517615  -0.42882207   0.23657905\n   -5.6899269   -5.11324593  -5.81413511  -5.33504631  -6.78118474\n   -5.73428698]\n [-22.40480297  -1.40474371  -3.62274743  -4.50994892  -2.07014483\n   -8.12085898  -0.45543811   0.47612345   0.03252271  -7.00298511\n    0.59145964]\n [-33.93842234  -5.76977504  -5.25519817   0.17447494  -6.18675974\n   -5.61895078  -0.76595864  -7.87244257  -6.08916757  -1.94593662\n   -0.14491759]\n [-33.93842234  -3.76469967   0.51161151  -3.69372355   0.52048352\n   -1.11196722  -7.13606533  -5.66331086   0.60920367  -4.50107691\n   -6.39968809]\n [-61.44166852   0.63581972  -3.86229183  -0.91678289  -4.62528512\n   -5.4503825   -4.53656497  -1.14745528  -5.06888586  -6.5238963\n   -2.49600154]\n [-29.50241489  -4.74949332  -5.2729422   -0.27799782  -3.98650004\n   -0.29574185  -5.8052631   -4.57205303  -4.39461273  -4.39461273\n   -7.25140152]\n [-38.37442978  -3.65823549  -3.23237878  -6.39081608  -2.86862617\n   -2.08788885  -3.17914669  -2.86862617  -1.54669595  -6.50615227\n    0.30755517]] \n\n"
    }
   ],
   "source": [
    "H = LinearHypothesis() # instantiate our linear model\n",
    "y_hat = H(X) # make prediction\n",
    "print('Input:',X, '\\n')\n",
    "print('W:', H.w, 'B:', H.b, '\\n')\n",
    "print('Prediction:', y_hat, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_h_vs_y(X, y_hat, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', label='Label')\n",
    "    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8cbd863ce398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_h_vs_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-ee42948da5eb>\u001b[0m in \u001b[0;36mplot_h_vs_y\u001b[0;34m(X, y_hat, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_h_vs_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Hypothesis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m         plotnonfinite=plotnonfinite, **({\"data\": data} if data is not\n\u001b[0;32m-> 2847\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2848\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4442\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4444\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
     ]
    }
   ],
   "source": [
    "plot_h_vs_y(X, y_hat, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The criterion - How do we know how good our model is?\n",
    "\n",
    "Our criterion should be a measure of how bad our model is. We will use it to compare different models. As the model gets worse the loss function should return larger values.\n",
    "\n",
    "Criterions need to return a single number, not a vector or anything else. This criterion is the thing which we will try to minimise\n",
    "\n",
    "<strong>Common synonyms</strong>\n",
    "- Loss funtion = cost function = criterion = error function\n",
    "\n",
    "### Mean squared error (MSE) loss\n",
    "\n",
    "**One way** to evaluate the performance of a model that predicts continuous (not discrete or bounded) outputs is to use the mean squared error loss. This does exactly what you think: it calculates the error (difference between our model's prediction and the true label) and then squares it and takes the mean of those square errors for each example. Squaring any value makes it positive, so as long as the error is not zero it will increase the value of the loss - regardless of whether our prediction is below (negative error) or above (positive error) the value of the label, the values of that **squared** difference will increase the returned loss.\n",
    "\n",
    "There are many other criterions that are useful for different tasks (e.g. the binary cross entropy (BCE) loss for classification).\n",
    "\n",
    "Let's write a function to calculate the cost using the mean squared error loss function. It should take in an array of predictions for different example inputs as well as an array of corresponding example labels. It should return a single number (scalar) that represents the MSE loss. \n",
    "\n",
    "![title](images/NN1_cost_function.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(y_hat, labels): # define our criterion (loss function)\n",
    "    errors = y_hat - labels ## calculate errors\n",
    "    squared_errors = errors ** 2 ## square errors\n",
    "    mean_squared_error = sum(squared_errors) / len(squared_errors) ## calculate mean \n",
    "    return mean_squared_error # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (28,11) (28,) ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ed5b4fe703a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-33cbe7d9eae9>\u001b[0m in \u001b[0;36mL\u001b[0;34m(y_hat, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# define our criterion (loss function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;31m## calculate errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msquared_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m## square errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmean_squared_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquared_errors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquared_errors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## calculate mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean_squared_error\u001b[0m \u001b[0;31m# return loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (28,11) (28,) "
     ]
    }
   ],
   "source": [
    "cost = L(y_hat, Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analytical solution to minimising mean square error\n",
    "\n",
    "Given the data, model and criterion that we have used above, we can now express the objective that we wish to minimise mathematically. \n",
    "\n",
    "# draw mathematical expression\n",
    "\n",
    "Note: this equation does not explicitly mention the bias. How can it be applied to our problem in the same form? What would we need to change about our input example vectors and weight matrix? \n",
    "\n",
    "We can now simply find the minimum point on this curve by finding an expression for it's derivative and setting this to zero.\n",
    "\n",
    "# draw MSE analytical solution\n",
    "\n",
    "Now let's implement this analytical solution for least squares regression in code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(28, 11)\n(28, 12)\n[[ 1.   63.    7.51  4.    7.73  5.87  6.57  3.73  1.98  4.76  8.44  5.62]\n [ 1.   31.    7.95  7.2   3.85  1.57  2.6   8.79  0.61  1.5   4.64  5.18]\n [ 1.   63.    3.98  8.16  0.47  2.29  0.68  0.41  8.72  3.19  9.02  9.66]\n [ 1.   70.    6.18  2.26  1.58  2.25  6.2   9.94  1.26  7.35  3.7   0.98]\n [ 1.   42.    6.49  5.46  1.5   1.94  1.91  6.68  1.02  9.25  0.1   6.35]\n [ 1.   55.    5.26  8.41  1.28  9.68  0.95  9.46  3.29  8.29  8.88  5.48]\n [ 1.   32.    6.09  2.01  6.26  2.08  7.49  4.09  7.93  0.29  2.09  4.64]\n [ 1.   20.    6.74  8.91  7.91  9.46  8.11  9.17  2.84  2.22  6.82  6.4 ]\n [ 1.   42.    3.48  0.23  9.25  1.87  8.02  3.25  6.4   1.72  1.95  7.71]\n [ 1.   54.    4.47  5.81  5.09  2.44  9.37  9.98  1.88  1.67  0.9   1.7 ]\n [ 1.   73.    5.28  3.64  6.2   3.93  0.03  9.51  2.6   6.01  9.38  8.51]\n [ 1.   46.    1.6   6.26  8.08  4.75  5.84  0.81  8.39  1.84  9.68  2.13]\n [ 1.   57.    6.43  8.27  6.8   4.73  0.28  1.66  8.58  8.33  5.23  6.56]\n [ 1.   59.    1.13  6.97  0.69  2.49  1.92  4.81  6.34  7.95  5.33  8.67]\n [ 1.   64.    6.11  9.12  6.79  0.67  7.24  3.07  6.21  4.26  1.26  5.05]\n [ 1.   58.    2.12  1.42  7.31  5.6   2.18  2.88  0.63  9.58  5.33  9.54]\n [ 1.   53.    6.34  7.2   9.21  7.78  8.55  7.41  2.89  0.05  4.59  2.12]\n [ 1.   68.    5.35  5.48  8.76  0.83  4.29  9.29  1.05  8.82  5.27  4.9 ]\n [ 1.   69.    8.67  8.01  6.86  5.56  4.15  4.49  4.79  5.65  4.52  0.82]\n [ 1.   54.    7.36  3.88  3.86  3.41  1.05  4.22  6.36  1.37  9.95  5.21]\n [ 1.   62.    4.11  9.28  5.11  5.34  4.5   2.03  2.81  2.8   3.52  1.65]\n [ 1.   21.    4.22  8.    1.23  0.48  7.16  6.51  7.3   6.76  8.39  7.21]\n [ 1.   26.    2.33  4.83  5.83  3.08  9.9   1.26  0.21  0.71  8.64  0.08]\n [ 1.   39.    7.25  6.67  0.55  7.72  7.08  1.61  9.62  7.61  2.94  0.91]\n [ 1.   39.    4.99  0.17  4.91  0.16  2.    8.79  7.13  0.06  5.82  7.96]\n [ 1.   70.    0.03  5.1   1.78  5.96  6.89  5.86  2.04  6.46  8.1   3.56]\n [ 1.   34.    6.1   6.69  1.06  5.24  1.08  7.29  5.9   5.7   5.7   8.92]\n [ 1.   44.    4.87  4.39  7.95  3.98  3.1   4.33  3.98  2.49  8.08  0.4 ]]\n[ 2.73157286  0.60929372  0.06224007 -0.08041394  0.0224751   0.06705545\n  0.16532118 -0.0382203  -0.03917421  0.04741092 -0.07833171  0.22743189]\n(12,)\n"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X_ = np.hstack((\n",
    "    np.ones((X.shape[0], 1)),\n",
    "    X\n",
    "))\n",
    "print(X_.shape)\n",
    "print(X_.round(2))\n",
    "\n",
    "optimal_w = np.matmul(\n",
    "    np.linalg.inv(\n",
    "        np.matmul(\n",
    "            X_.T, \n",
    "            X_\n",
    "        )\n",
    "    ), \n",
    "    np.matmul(\n",
    "        X_.T, \n",
    "        Y\n",
    "    )\n",
    ")\n",
    "\n",
    "print(optimal_w)\n",
    "print(optimal_w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Drawbacks of computing the analytical solution\n",
    "\n",
    "This solution involves inverting a matrix of size $R^{nxn}$. \n",
    "Here $n$ is the number of features that each example has. In our example $n=1$, so computing the analytical solution is feasible. \n",
    "However, as we will soon see, most problems of practical interest contain examples with many more features. \n",
    "For example, 1080p images have more than 1,000,000 features each. \n",
    "The time complexity of inverting a matrix of size $nxn$ is around $O(n^3)$. \n",
    "This means that computing the analytical solution for these kinds of real world problems is often computationally expensive, to the extent that it can become computationally infeasible.\n",
    "\n",
    "Analytical solutions however, are not the only approach that we can take. \n",
    "We can alternatively use one of many numerical optimisation techniques.\n",
    "These numerical optimisation techniques can be applied where analytical techniques are not feasible, and as such will be used throughout the course from here onward.\n",
    "\n",
    "## 4. The optimiser\n",
    "\n",
    "The optimiser adjusts the model such that it's performance improves with respect to the criterion. Most machine learning models are **parametric**, which means that the function which they represent depends on their parameters (in our case the weight (slope) and bias (intercept)). Different optimisers improve our models using different algorithms.\n",
    "\n",
    "In this notebook we will implement a trivial optimisation technique called **random search**.\n",
    "\n",
    "### Random Search\n",
    "Random seach is the process of randomly choosing values within a specified range and testing them to evaluate how good they are. E.g. test random values between 0 and 10.\n",
    "\n",
    "![](images/NN1_randomsearch.JPG)\n",
    "\n",
    "Let's implement a function that tries a bunch of possible values for the weight and bias of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(n_samples, limit=20):\n",
    "    best_weights = np.random.uniform(-limit, limit) # no best weight found yet\n",
    "    best_bias = np.random.uniform(-limit, limit) # no best bias found yet\n",
    "    lowest_cost = float('inf') # initialize it very high\n",
    "    for i in range(0, n_samples): # try this many different parameterisations\n",
    "        w = np.random.uniform(-limit, limit) # randomly sample a weight within the limits of the search\n",
    "        b = np.random.uniform(-limit, limit) # randomly sample a bias within the limits of the search\n",
    "        H.update_params(w, b) # update our model with random parameters\n",
    "        y_hat = H(X) # make prediction\n",
    "        cost = L(y_hat, Y) # calculate loss\n",
    "        if cost < lowest_cost: # if this is the best parameterisation so far\n",
    "            lowest_cost = cost # update the lowest running cost to the cost for this parameterisation\n",
    "            best_weights = w # get best weights so far from the model\n",
    "            best_bias = b # get best bias so far from the model\n",
    "    print('Lowest cost of', lowest_cost, 'achieved with weight of', best_weights, 'and bias of', best_bias)\n",
    "    return best_weights, best_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights, best_bias = random_search(100) # do 100 samples in a random search \n",
    "H.update_params(best_weights, best_bias) # make sure to set our model's weights to the best values we found\n",
    "plot_h_vs_y(X, H(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "Our random search optimisation was able to fit the input-output relationship of our data! Or at least it got close. \n",
    "\n",
    "#### What would happen if our true parameter values were outside of the range [-20, 20]? \n",
    "\n",
    "The model performance wouldn't improve much!\n",
    "\n",
    "This is because of the limits of the values of the parameters that we perform the grid search over. In this case, by default we are only trying parameters in the range from -20 to 20. But if the true bias were to be, say 30, which is outside of this range - then the model would never sample a value close to this. \n",
    "\n",
    "So here we've assumed the range of values that our optimal parameterisation might be included in. Feel free to change this limit in the function definition to see the how the model performance changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will random search and grid search get us all the way?\n",
    "\n",
    "Aside from the issues showcased above, the major limitation of these search methods is how they scale with the number of parameters in our model. \n",
    "\n",
    "To model more complex functions we'll need more complex models - models with more parameters. \n",
    "\n",
    "But the time taken for these search methods scales **exponentially** with the number of parameters. This is because these methods have to search the whole space, and they keep searching even if they find the optimal value (they can't be sure it's the best parameterisation in the domain that they're checking until they've compared it to everywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('DATA/winequality-red.csv', delimiter=';') ## Import income data and save to variable.\n",
    "\n",
    "data = data.round(2)\n",
    "print('RAW DATA')\n",
    "print(data)\n",
    "data = data[1:] # remove NaNs\n",
    "print()\n",
    "\n",
    "X = data[:, :-1] # get all of the rows and all but the last column (the last column is the labels)\n",
    "print('FEATURES (design matrix), X:')\n",
    "print(X)\n",
    "print('Design matrix shape:', X.shape)\n",
    "print()\n",
    "\n",
    "print('CHOSEN FEATURE:', )\n",
    "\n",
    "Y = data[:, -1] # get the last column as the labels\n",
    "print('LABELS, Y:')\n",
    "print(Y)\n",
    "print('Labels shape:', Y.shape)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure() # create a figure\n",
    "    plt.scatter(X, Y, c='r') # plot the data in color=red\n",
    "    plt.xlabel('Acidity')\n",
    "    plt.ylabel('Quality')\n",
    "    plt.show()\n",
    "\n",
    "plot_data(X[:, 0], Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- the very basic recipe for making machine learning algorithms consists of:\n",
    "    1. The data - our examples of inputs and outputs (in the supervised case) which determine the function that our model will learn to represent and hence the problem that we are solving\n",
    "    2. The model - our mathematical function that we pass our data forward through to make a prediction for the output\n",
    "    3. The criterion - how we measure how bad our model is.\n",
    "    4. The optimiser - our method for updating the parameters of our models.\n",
    "- random search is a trivial optimisation strategy\n",
    "- the MSE loss is appropriate for this regression problem\n",
    "\n",
    "## Next steps\n",
    "- [Gradient based optimisation]() - in this notebook we will look at optimisation techniques that do scale to \n",
    "more complex models and problems. Can we use gradient based optimisation to find the best parameters for our model and establish whether this wine is drinkable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "1. Implement grid Search\n",
    "\n",
    "Grid search is the process of trying out values at common intervals within a specified range for each parameter, and testing them to evaluate how good they are. E.g. test the values [0, 1, 2, 3, 4, 5]. It is the same as random search, other than the fact that the test coordinates are chosen systematically rather than randomly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit4e36a22c5ab145cdaac0d5c6b1a34fd0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}