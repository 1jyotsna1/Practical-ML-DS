{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "## Prerequisites\n",
    "- Regression trees\n",
    "- Gradient boosting\n",
    "\n",
    "## Intro - XGBoost summary\n",
    "XGBoost is a shorthand for eXtreme Gradient Boost. It is much like gradient boosting with decision trees. Similarly, it starts by making a single constant estimate for all examples and then uses a decision tree which predicts not the label, but the size and magnitude of the update that needs to be made to the previous estimate. The difference lies in the type of tree that it uses to make predictions, how they are grown and cut down, and a bunch of optimisations that are made to allow it to work well for large datasets. Below, we firstly discuss the XGBoost trees, then making them into an ensemble, then the optimisations.\n",
    "\n",
    "Here's a summary of the XGBoost algorithm\n",
    "- make an initial prediction\n",
    "- build an XGBoost tree the predicts the residual of the initial prediction\n",
    "- prune the XGBoost tree\n",
    "- build another tree that \n",
    "- stop when \n",
    "\n",
    "\n",
    "## XGBoost trees\n",
    "\n",
    "Each XGBoost tree starts with a single leaf which contains all of the residuals between the final predictions and their labels. We then calculate a **similarity score** for this leaf. This similarity score is like the mean squared error except that the quantity is squared once the individual errors have been added rather than before.\n",
    "This means that negative and positive errors can cancel out to reduce the overall similarity.\n",
    "The prediction made by the leaf is always the mean of the examples; so there will be feature values above and below the prediction (if more than a single example).\n",
    "The equation also includes a regularisation parameter $\\lambda$ added to the denominator. \n",
    "You can see this from the equation used to compute it below.\n",
    "\n",
    "## Similarity $= \\frac{(\\Sigma^k x - y)^2}{k+\\lambda}$\n",
    "Where k is the number of examples which this tree makes predictions for.\n",
    "\n",
    "**Note: we do not square the residuals before adding them up.** This means that residuals that are on opposite sides of the prediction cancel each other out, reducing the similarity score. \n",
    "\n",
    "The question is: would splitting the data classified by this tree produce a total similarity that is less than the current similarity.\n",
    "To do this, we consider every possible split of this subset of datapoints (every midpoint between successive datapoints ordered along each axis). So we test each of these points by splitting the data at that feature's value to produce two leaves, then we again evaluate the similarity score for each of those leaves and add them together. The difference between the original similarity score and the sum of similarity scores over each leaf of one of these XGBoost trees is called the **gain**.\n",
    "\n",
    "$Gain \\ = \\ similarity_{left} + similarity_{right} - similarity_{root}$\n",
    "\n",
    "This gain is a heuristic indicating the best way to extend the tree.\n",
    "Unlike other trees, it is not weighted by the number of examples that end up in each region.\n",
    "It is also dependent on the similarity score of the parent leaf. \n",
    "Relative rather than absolute?\n",
    "\n",
    "We evaluate the gain resulting from each of the potential splits. \n",
    "Then we extend the tree by adding the best split to the \n",
    "\n",
    "## How is this type of tree different to regular boosting trees?\n",
    "Instead of continuing to grow the tree by whichever split produces the least mean squared error (for regression), these XGBoost trees grow the tree by making branches (splits) that produce leaves with the greatest gain in similarity score.\n",
    "\n",
    "## Why is it better?\n",
    "\n",
    "## What is the meaning of the similarity score?\n",
    "\n",
    "## Pruning trees\n",
    "Once we have built a tree, we prune it based on it's similarity scores.\n",
    "To do this, we choose a number $\\gamma$. \n",
    "We then start at the branches closest to the leaves of the tree and evaluate whether the similarity score for each of them is greater than $\\gamma$ by computing\n",
    "\n",
    "$ similarity - \\gamma$\n",
    "\n",
    "If this quantity is less than zero, that is that the similarity is less than \\gamma, then we remove that branch.\n",
    "\n",
    "$\\gamma$ is known as the tree complexity parameter.\n",
    "This is because the larger it is, the greater the gain needed for a split to prevent it's fork being pruned, and more pruning reduces the capacity (complexity of representable functions) of the tree.\n",
    "\n",
    "**Note that even if we set \\gamma to zero, we do not turn off pruning because the gain may be negative, in which case the above equation will still be less than zero and cause the branch to be pruned.\n",
    "\n",
    "When a leaf gets pruned, the predictions for examples that originally ended up at that leaf will now be made by the parent.\n",
    "So the parent will become a predictor that makes predictions based on more examples.\n",
    "The denominator in the similarity score computation then becomes less sensitive to lambda.\n",
    "\n",
    "## The regularisation parameter $\\lambda$\n",
    "Recall that our equation for computing the similarity included a term in the denominator which was $+\\lambda$.\n",
    "$\\lambda$'s effect on the similarity is to reduce the metrics' sensitivity to individual observations. \n",
    "It does this by increasing the susceptibility of branches to be pruned as it is increased.\n",
    "When this happens, the examples that would have then been classified by each leaf stemming from that branch are now classified by a leaf which replaces it.\n",
    "That is, a single prediction has to be made for a bunch of datapoints that were previously split into different leaves.\n",
    "This prediction will have worse training error, but should help the model to generalise by reducing the sensitivity to more specific splits of data.\n",
    "This is how $lambda$ regluarises the model and counteracts overfitting. \n",
    "\n",
    "## Making predictions with individual XGBoost trees\n",
    "Once we've made these trees we need to make predictions for examples that end up on each leaf.\n",
    "The predicted value for an example that arrives at a given leaf of an XGBoost regression tree is given by\n",
    "\n",
    "## $output = \\frac{sum\\ of \\ residuals}{number\\ of\\ residuals + \\lambda}$\n",
    "\n",
    "This is much like the calculation for the similarity score, except for the fact that we don't square the sum of the residuals. It is like taking the average of the labels of the examples from the training set that arrived at this leaf, except from the regularisation term which reduces the contribution that this tree has to the output as it increases. As the number of training examples at a leaf increases far beyond the value of lambda (or lambda = 0), the prediction tends to the average of the label.  \n",
    "\n",
    "This prediction is added to the sequence of previous predictions when the trees are combined with the initial average prediction.\n",
    "So increasing $\\lambda$ reduces the influence of each tree on the output prediction.\n",
    "\n",
    "## Let's implement an XGBoost regression tree from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(30, 1) (30, 1)\n"
    }
   ],
   "source": [
    "from utils import get_regression_data, visualise_regression_predictions\n",
    "X, Y = get_regression_data(m=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4822690998982e-32\ngain: -2.704005641919679e-31\nsimilarity: 3.0814879110195774e-31\nsimilarity: 0.22939975478120422\nsimilarity: 0.22939975478120456\ngain: 0.4587995095624088\nsimilarity: 3.0814879110195774e-31\nsimilarity: 0.28753480876806653\nsimilarity: 0.28753480876806664\ngain: 0.5750696175361332\nsimilarity: 3.0814879110195774e-31\nsimilarity: 1.8278644626605107\nsimilarity: 1.8278644626605107\ngain: 3.6557289253210214\nbest gain on this feature: 3.6557289253210214\nsplit val: -0.5317495264712843, gain: 3.6557289253210214, split feature:0\nsimilarity: 3.0814879110195774e-31\nsimilarity: 0.45997583828075883\nsimilarity: 0.4599758382807576\ngain: 0.9199516765615164\nsimilarity: 3.0814879110195774e-31\nsimilarity: 1.372929307509451\nsimilarity: 1.3729293075094495\ngain: 2.745858615018901\nsimilarity: 3.0814879110195774e-31\nsimilarity: 0.45997583828075883\nsimilarity: 0.4599758382807576\ngain: 0.9199516765615164\nsimilarity: 3.0814879110195774e-31\nsimilarity: 7.804743615931096\nsimilarity: 7.804743615931093\ngain: 15.60948723186219\nsimilarity: 3.0814879110195774e-31\nsimilarity: 1.7243417359763455\nsimilarity: 1.7243417359763433\ngain: 3.448683471952689\nsimilarity: 3.0814879110195774e-31\nsimilarity: 5.992497713981962\nsimilarity: 5.99249771398196\ngain: 11.984995427963922\nsimilarity: 3.0814879110195774e-31\nsimilarity: 0.0006511809608303937\nsimilarity: 0.0006511809608303596\ngain: 0.0013023619216607533\nsimilarity: 3.0814879110195774e-31\nsimilarity: 1.0340725107204558\nsimilarity: 1.0340725107204545\ngain: 2.0681450214409103\nsimilarity: 3.0814879110195774e-31\nsimilarity: 0.4208999023928529\nsimilarity: 0.42089990239285163\ngain: 0.8417998047857045\nsimilarity: 3.0814879110195774e-31\nsimilarity: 0.45997583828075883\nsimilarity: 0.4599758382807576\ngain: 0.9199516765615164\nsimilarity: 3.0814879110195774e-31\nsimilarity: 1.7243417359763455\nsimilarity: 1.7243417359763433\ngain: 3.448683471952689\nsimilarity: 3.0814879110195774e-31\nsimilarity: 0.0006511809608303937\nsimilarity: 0.0006511809608303596\ngain: 0.0013023619216607533\nsimilarity: 3.0814879110195774e-31\nsimilarity: 1.372929307509451\nsimilarity: 1.3729293075094495\ngain: 2.745858615018901\nsimilarity: 3.0814879110195774e-31\nsimilarity: 1.0340725107204558\nsimilarity: 1.0340725107204545\ngain: 2.0681450214409103\nbest gain on this feature: 15.60948723186219\nsplit val: 2.061863991445776, gain: 15.60948723186219, split feature:0\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.13329863966426625\nsimilarity: 0.13329863966426636\ngain: 0.2665972793285326\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.16300975214826158\nsimilarity: 0.16300975214826158\ngain: 0.32601950429652315\nsimilarity: 1.7333369499485123e-31\nsimilarity: 5.195385722298602\nsimilarity: 5.195385722298604\ngain: 10.390771444597206\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.7473885917179215\nsimilarity: 0.747388591717922\ngain: 1.4947771834358434\nsimilarity: 1.7333369499485123e-31\nsimilarity: 7.313950736689661\nsimilarity: 7.313950736689661\ngain: 14.627901473379321\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.0026522432235023294\nsimilarity: 0.002652243223502341\ngain: 0.00530448644700467\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.0026522432235023294\nsimilarity: 0.002652243223502341\ngain: 0.00530448644700467\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.1678146118644627\nsimilarity: 0.1678146118644629\ngain: 0.33562922372892556\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.004796873038591726\nsimilarity: 0.004796873038591723\ngain: 0.009593746077183449\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.13329863966426625\nsimilarity: 0.13329863966426636\ngain: 0.2665972793285326\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.13329863966426625\nsimilarity: 0.13329863966426636\ngain: 0.2665972793285326\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.0026522432235023294\nsimilarity: 0.002652243223502341\ngain: 0.00530448644700467\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.13329863966426625\nsimilarity: 0.13329863966426636\ngain: 0.2665972793285326\nsimilarity: 1.7333369499485123e-31\nsimilarity: 0.13329863966426625\nsimilarity: 0.13329863966426636\ngain: 0.2665972793285326\nbest gain on this feature: 14.627901473379321\nsplit val: -2.412030969462103, gain: 14.627901473379321, split feature:0\n\ntree 19\nsimilarity: 1.9721522630525295e-31\nsimilarity: 1.0340725107204554\nsimilarity: 1.0340725107204558\ngain: 2.068145021440911\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.4208999023928529\nsimilarity: 0.42089990239285235\ngain: 0.8417998047857053\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.16300975214826158\nsimilarity: 0.16300975214826158\ngain: 0.32601950429652315\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.008061561518354483\nsimilarity: 0.008061561518354503\ngain: 0.016123123036708986\nsimilarity: 1.9721522630525295e-31\nsimilarity: 1.3729293075094502\nsimilarity: 1.372929307509451\ngain: 2.7458586150189013\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.45997583828075883\nsimilarity: 0.45997583828075866\ngain: 0.9199516765615174\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.6129262642214914\nsimilarity: 0.612926264221491\ngain: 1.2258525284429824\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.6129262642214914\nsimilarity: 0.612926264221491\ngain: 1.2258525284429824\nsimilarity: 1.9721522630525295e-31\nsimilarity: 1.7243417359763455\nsimilarity: 1.7243417359763455\ngain: 3.448683471952691\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.1332986396642663\nsimilarity: 0.13329863966426642\ngain: 0.2665972793285327\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.6129262642214914\nsimilarity: 0.612926264221491\ngain: 1.2258525284429824\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.4208999023928529\nsimilarity: 0.42089990239285235\ngain: 0.8417998047857053\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.002652243223502341\nsimilarity: 0.002652243223502341\ngain: 0.005304486447004682\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.002652243223502341\nsimilarity: 0.002652243223502341\ngain: 0.005304486447004682\nsimilarity: 1.9721522630525295e-31\nsimilarity: 3.0814879110195774e-31\nsimilarity: 2.1613748800885754e-31\ngain: 3.2707105280556233e-31\nsimilarity: 1.9721522630525295e-31\nsimilarity: 1.0340725107204554\nsimilarity: 1.0340725107204558\ngain: 2.068145021440911\nsimilarity: 1.9721522630525295e-31\nsimilarity: 1.6554354459167189\nsimilarity: 1.6554354459167178\ngain: 3.310870891833437\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.004796873038591723\nsimilarity: 0.004796873038591723\ngain: 0.009593746077183446\nsimilarity: 1.9721522630525295e-31\nsimilarity: 3.0814879110195774e-31\nsimilarity: 2.1613748800885754e-31\ngain: 3.2707105280556233e-31\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.03621228318204354\nsimilarity: 0.03621228318204369\ngain: 0.07242456636408723\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.1332986396642663\nsimilarity: 0.13329863966426642\ngain: 0.2665972793285327\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.002652243223502341\nsimilarity: 0.002652243223502341\ngain: 0.005304486447004682\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.33474955121083416\nsimilarity: 0.3347495512108339\ngain: 0.6694991024216681\nsimilarity: 1.9721522630525295e-31\nsimilarity: 1.7243417359763455\nsimilarity: 1.7243417359763455\ngain: 3.448683471952691\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.0006511809608303937\nsimilarity: 0.0006511809608303851\ngain: 0.0013023619216607787\nsimilarity: 1.9721522630525295e-31\nsimilarity: 1.3729293075094502\nsimilarity: 1.372929307509451\ngain: 2.7458586150189013\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.6129262642214914\nsimilarity: 0.612926264221491\ngain: 1.2258525284429824\nsimilarity: 1.9721522630525295e-31\nsimilarity: 0.1332986396642663\nsimilarity: 0.13329863966426642\ngain: 0.2665972793285327\nsimilarity: 1.9721522630525295e-31\nsimilarity: 3.0814879110195774e-31\nsimilarity: 2.1613748800885754e-31\ngain: 3.2707105280556233e-31\nbest gain on this feature: 3.448683471952691\nsplit val: 1.715231339947687, gain: 3.448683471952691, split feature:0\nsimilarity: 1.1093356479670479e-31\nsimilarity: 1.9528506267297752\nsimilarity: 1.9528506267297763\ngain: 3.9057012534595517\nsimilarity: 1.1093356479670479e-31\nsimilarity: 4.0232816325504634\nsimilarity: 4.023281632550465\ngain: 8.046563265100929\nsimilarity: 1.1093356479670479e-31\nsimilarity: 3.931642727393133\nsimilarity: 3.9316427273931347\ngain: 7.863285454786268\nsimilarity: 1.1093356479670479e-31\nsimilarity: 1.7850033072941143\nsimilarity: 1.7850033072941156\ngain: 3.57000661458823\nbest gain on this feature: 8.046563265100929\nsplit val: 2.061863991445776, gain: 8.046563265100929, split feature:0\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.025649915779860723\nsimilarity: 0.025649915779860786\ngain: 0.05129983155972151\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.019913803206781867\nsimilarity: 0.01991380320678196\ngain: 0.039827606413563824\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.10917686068244026\nsimilarity: 0.10917686068244\ngain: 0.21835372136488024\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.07772060042854974\nsimilarity: 0.07772060042854986\ngain: 0.15544120085709962\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.0999696703590593\nsimilarity: 0.09996967035905888\ngain: 0.1999393407181182\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.025649915779860723\nsimilarity: 0.025649915779860786\ngain: 0.05129983155972151\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.025649915779860723\nsimilarity: 0.025649915779860786\ngain: 0.05129983155972151\nsimilarity: 1.1093356479670479e-31\nsimilarity: 6.221677037209417\nsimilarity: 6.221677037209417\ngain: 12.443354074418833\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.0999696703590593\nsimilarity: 0.09996967035905888\ngain: 0.1999393407181182\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.0999696703590593\nsimilarity: 0.09996967035905888\ngain: 0.1999393407181182\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.6207630249514838\nsimilarity: 0.6207630249514833\ngain: 1.241526049902967\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.005102440261679969\nsimilarity: 0.005102440261679981\ngain: 0.01020488052335995\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.43076793339623415\nsimilarity: 0.4307679333962346\ngain: 0.8615358667924687\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.060464155377901896\nsimilarity: 0.06046415537790162\ngain: 0.12092831075580351\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.6207630249514838\nsimilarity: 0.6207630249514833\ngain: 1.241526049902967\nsimilarity: 1.1093356479670479e-31\nsimilarity: 1.672714067585991\nsimilarity: 1.6727140675859922\ngain: 3.345428135171983\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.025649915779860723\nsimilarity: 0.025649915779860786\ngain: 0.05129983155972151\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.0999696703590593\nsimilarity: 0.09996967035905888\ngain: 0.1999393407181182\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.06855776997903644\nsimilarity: 0.06855776997903638\ngain: 0.13711553995807282\nsimilarity: 1.1093356479670479e-31\nsimilarity: 1.672714067585991\nsimilarity: 1.6727140675859922\ngain: 3.345428135171983\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.07772060042854974\nsimilarity: 0.07772060042854986\ngain: 0.15544120085709962\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.002259727631251213\nsimilarity: 0.002259727631251232\ngain: 0.0045194552625024455\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.025649915779860723\nsimilarity: 0.025649915779860786\ngain: 0.05129983155972151\nsimilarity: 1.1093356479670479e-31\nsimilarity: 0.6207630249514838\nsimilarity: 0.6207630249514833\ngain: 1.241526049902967\nbest gain on this feature: 12.443354074418833\nsplit val: -2.412030969462103, gain: 12.443354074418833, split feature:0\n\n[-12.74901224  -5.96808111 -12.74901224 -12.74901224  -5.96808111\n  -5.96808111  -5.96808111 -12.74901224  -5.96808111  -5.96808111\n -12.74901224  -5.96808111 -12.74901224 -12.74901224 -12.74901224\n  -5.96808111  -5.96808111 -12.74901224 -12.74901224  -5.96808111\n -12.74901224 -12.74901224 -12.74901224  -5.96808111  -5.96808111\n  -5.96808111  -5.96808111 -12.74901224 -12.74901224  -5.96808111]\n[-13.45464891  -3.14553442 -13.45464891 -13.45464891  -6.67371778\n  -6.67371778  -3.14553442 -13.45464891  -3.14553442  -3.14553442\n -13.45464891  -3.14553442 -13.45464891 -13.45464891 -13.45464891\n  -6.67371778  -6.67371778 -13.45464891 -13.45464891  -6.67371778\n -13.45464891 -13.45464891 -13.45464891  -6.67371778  -3.14553442\n  -6.67371778  -6.67371778 -13.45464891 -13.45464891  -6.67371778]\n[-13.02631909  -2.71720459 -13.02631909 -15.59629803  -6.24538796\n  -6.24538796  -2.71720459 -15.59629803  -2.71720459  -2.71720459\n -15.59629803  -2.71720459 -13.02631909 -15.59629803 -13.02631909\n  -6.24538796  -6.24538796 -13.02631909 -15.59629803  -6.24538796\n -13.02631909 -13.02631909 -13.02631909  -6.24538796  -2.71720459\n  -6.24538796  -6.24538796 -13.02631909 -13.02631909  -6.24538796]\n[-11.89235259  -3.85117109 -11.89235259 -14.46233153  -7.37935446\n  -7.37935446  -3.85117109 -14.46233153  -3.85117109  -3.85117109\n -14.46233153  -3.85117109 -11.89235259 -14.46233153 -11.89235259\n  -7.37935446  -7.37935446 -11.89235259 -14.46233153  -7.37935446\n -11.89235259 -11.89235259 -11.89235259  -7.37935446  -3.85117109\n  -7.37935446  -7.37935446 -11.89235259 -11.89235259  -7.37935446]\n[-11.62616239  -3.58498089 -11.62616239 -15.52709234  -7.11316426\n  -7.11316426  -3.58498089 -15.52709234  -3.58498089  -3.58498089\n -15.52709234  -3.58498089 -11.62616239 -15.52709234 -11.62616239\n  -7.11316426  -7.11316426 -11.62616239 -15.52709234  -7.11316426\n -11.62616239 -12.9571134  -11.62616239  -7.11316426  -3.58498089\n  -7.11316426  -7.11316426 -11.62616239 -11.62616239  -7.11316426]\n[-11.813904    -3.06869146 -11.813904   -15.71483395  -7.30090587\n  -7.30090587  -3.06869146 -15.71483395  -3.06869146  -3.06869146\n -15.71483395  -3.06869146 -11.813904   -15.71483395 -11.813904\n  -7.30090587  -7.30090587 -11.813904   -15.71483395  -6.59687483\n -11.813904   -13.14485501 -11.813904    -6.59687483  -3.06869146\n  -7.30090587  -7.30090587 -11.813904   -11.813904    -7.30090587]\n[-11.35997219  -3.52262327 -11.35997219 -15.26090214  -7.75483768\n  -7.75483768  -3.52262327 -15.26090214  -3.52262327  -3.52262327\n -15.26090214  -3.52262327 -11.35997219 -15.26090214 -11.35997219\n  -7.75483768  -7.75483768 -11.35997219 -15.26090214  -7.05080664\n -11.35997219 -12.6909232  -11.35997219  -7.05080664  -3.52262327\n  -7.75483768  -7.75483768 -11.35997219 -11.35997219  -7.75483768]\n[-11.72183892  -3.28137879 -11.1187277  -15.62276887  -7.51359319\n  -7.51359319  -3.28137879 -15.62276887  -3.28137879  -3.28137879\n -15.62276887  -3.28137879 -11.72183892 -15.62276887 -11.72183892\n  -7.51359319  -7.51359319 -11.72183892 -15.62276887  -6.80956215\n -11.1187277  -13.05278993 -11.72183892  -6.80956215  -3.28137879\n  -7.51359319  -7.51359319 -11.1187277  -11.72183892  -7.51359319]\n[-11.48059443  -3.52262327 -10.87748322 -15.38152438  -7.75483768\n  -7.75483768  -3.52262327 -15.38152438  -3.52262327  -3.52262327\n -15.38152438  -3.52262327 -11.48059443 -15.38152438 -11.48059443\n  -7.75483768  -7.75483768 -11.48059443 -15.38152438  -7.05080664\n -10.87748322 -12.81154544 -11.48059443  -7.05080664  -3.52262327\n  -7.75483768  -7.75483768 -10.87748322 -11.48059443  -7.75483768]\n[-11.64566055  -3.06869146 -11.04254933 -15.54659049  -7.91990379\n  -7.91990379  -3.06869146 -15.54659049  -3.06869146  -3.06869146\n -15.54659049  -3.06869146 -11.64566055 -15.54659049 -11.64566055\n  -7.91990379  -7.91990379 -11.64566055 -15.54659049  -6.59687483\n -11.04254933 -12.97661156 -11.64566055  -6.59687483  -3.06869146\n  -7.91990379  -7.91990379 -11.04254933 -11.64566055  -7.91990379]\n[-11.48059443  -3.23375757 -10.87748322 -15.38152438  -8.08496991\n  -8.08496991  -3.23375757 -15.38152438  -3.23375757  -3.23375757\n -15.38152438  -3.23375757 -11.48059443 -15.38152438 -11.48059443\n  -8.08496991  -8.08496991 -11.48059443 -15.38152438  -6.76194094\n -10.87748322 -12.81154544 -11.48059443  -6.76194094  -3.23375757\n  -8.08496991  -8.08496991 -10.87748322 -11.48059443  -8.08496991]\n[-11.72183892  -3.07292792 -10.71665356 -15.62276887  -7.92414025\n  -7.92414025  -3.07292792 -15.62276887  -3.07292792  -3.07292792\n -15.62276887  -3.07292792 -11.72183892 -15.62276887 -11.72183892\n  -7.92414025  -7.92414025 -11.72183892 -15.62276887  -6.60111128\n -10.71665356 -13.05278993 -11.72183892  -6.60111128  -3.07292792\n  -7.92414025  -7.92414025 -10.71665356 -11.72183892  -7.92414025]\n[-11.56100926  -3.23375757 -10.5558239  -15.46193921  -8.08496991\n  -8.08496991  -3.23375757 -15.46193921  -3.23375757  -3.23375757\n -15.46193921  -3.23375757 -11.56100926 -15.46193921 -11.56100926\n  -8.08496991  -8.08496991 -11.56100926 -15.46193921  -6.76194094\n -10.5558239  -12.89196027 -11.56100926  -6.76194094  -3.23375757\n  -8.08496991  -8.08496991 -10.5558239  -11.56100926  -8.08496991]\n[-11.67348342  -3.06504633 -10.66829806 -15.57441337  -7.91625866\n  -8.19744407  -3.06504633 -15.57441337  -3.06504633  -3.06504633\n -15.57441337  -3.06504633 -11.67348342 -15.57441337 -11.67348342\n  -8.19744407  -7.91625866 -11.67348342 -15.57441337  -6.5932297\n -10.66829806 -13.00443443 -11.67348342  -6.5932297   -3.06504633\n  -7.91625866  -8.19744407 -10.66829806 -11.67348342  -7.91625866]\n[-11.56100926  -3.17752049 -10.5558239  -15.46193921  -8.02873282\n  -8.30991823  -3.17752049 -15.46193921  -3.17752049  -3.17752049\n -15.46193921  -3.17752049 -11.56100926 -15.46193921 -11.56100926\n  -8.30991823  -8.02873282 -11.56100926 -15.46193921  -6.70570386\n -10.5558239  -12.89196027 -11.56100926  -6.70570386  -3.17752049\n  -8.02873282  -8.30991823 -10.5558239  -11.56100926  -8.02873282]\n[-11.72183892  -3.07030072 -10.44860413 -15.62276887  -7.92151305\n  -8.20269846  -3.07030072 -15.62276887  -3.07030072  -3.07030072\n -15.62276887  -3.07030072 -11.72183892 -15.62276887 -11.72183892\n  -8.20269846  -7.92151305 -11.72183892 -15.62276887  -6.59848409\n -10.44860413 -13.05278993 -11.72183892  -6.59848409  -3.07030072\n  -7.92151305  -8.20269846 -10.44860413 -11.72183892  -7.92151305]\n[-11.61461915  -3.17752049 -10.34138436 -15.51554909  -8.02873282\n  -8.30991823  -3.17752049 -15.51554909  -3.17752049  -3.17752049\n -15.51554909  -3.17752049 -11.61461915 -15.51554909 -11.61461915\n  -8.30991823  -8.02873282 -11.61461915 -15.51554909  -6.70570386\n -10.34138436 -12.94557016 -11.61461915  -6.70570386  -3.17752049\n  -8.02873282  -8.30991823 -10.34138436 -11.61461915  -8.02873282]\n[-11.70475151  -3.0596551  -10.43151671 -15.60568145  -7.91086743\n  -8.40005059  -3.0596551  -15.60568145  -3.0596551   -3.0596551\n -15.60568145  -3.0596551  -11.70475151 -15.60568145 -11.70475151\n  -8.19205284  -7.91086743 -11.70475151 -15.60568145  -6.58783847\n -10.43151671 -13.03570251 -11.70475151  -6.58783847  -3.0596551\n  -7.91086743  -8.40005059 -10.43151671 -11.70475151  -7.91086743]\n[-11.61461915  -3.14978746 -10.34138436 -15.51554909  -8.00099979\n  -8.49018295  -3.14978746 -15.51554909  -3.14978746  -3.14978746\n -15.51554909  -3.14978746 -11.61461915 -15.51554909 -11.61461915\n  -8.2821852   -8.00099979 -11.61461915 -15.51554909  -6.67797083\n -10.34138436 -12.94557016 -11.61461915  -6.67797083  -3.14978746\n  -8.00099979  -8.49018295 -10.34138436 -11.61461915  -8.00099979]\n[-11.66714482  -2.88715907 -10.39391003 -15.56807477  -8.05352547\n  -8.54270863  -2.88715907 -15.56807477  -2.88715907  -3.20231314\n -15.56807477  -2.88715907 -11.66714482 -15.56807477 -11.66714482\n  -8.33471088  -8.05352547 -11.66714482 -15.56807477  -6.7304965\n -10.39391003 -12.99809583 -11.66714482  -6.7304965   -2.88715907\n  -8.05352547  -8.54270863 -10.39391003 -11.66714482  -8.05352547]\n\n[-11.66714482  -2.88715907 -10.39391003 -15.56807477  -8.05352547\n  -8.54270863  -2.88715907 -15.56807477  -2.88715907  -3.20231314\n -15.56807477  -2.88715907 -11.66714482 -15.56807477 -11.66714482\n  -8.33471088  -8.05352547 -11.66714482 -15.56807477  -6.7304965\n -10.39391003 -12.99809583 -11.66714482  -6.7304965   -2.88715907\n  -8.05352547  -8.54270863 -10.39391003 -11.66714482  -8.05352547]\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 390.982812 262.19625\" width=\"390.982812pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 390.982812 262.19625 \nL 390.982812 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 48.982813 224.64 \nL 383.782813 224.64 \nL 383.782813 7.2 \nL 48.982813 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mff38c2e264\" style=\"stroke:#ff0000;\"/>\n    </defs>\n    <g clip-path=\"url(#p033f25aa84)\">\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"160.869723\" xlink:href=\"#mff38c2e264\" y=\"146.868467\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"332.501608\" xlink:href=\"#mff38c2e264\" y=\"52.331771\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"208.584953\" xlink:href=\"#mff38c2e264\" y=\"121.954009\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"74.784181\" xlink:href=\"#mff38c2e264\" y=\"214.579347\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"250.041491\" xlink:href=\"#mff38c2e264\" y=\"101.606712\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"240.539604\" xlink:href=\"#mff38c2e264\" y=\"113.900309\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"347.289443\" xlink:href=\"#mff38c2e264\" y=\"34.107526\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"80.565495\" xlink:href=\"#mff38c2e264\" y=\"204.877349\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"346.701664\" xlink:href=\"#mff38c2e264\" y=\"42.20777\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"314.325062\" xlink:href=\"#mff38c2e264\" y=\"54.292323\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"67.151235\" xlink:href=\"#mff38c2e264\" y=\"207.518816\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"365.61439\" xlink:href=\"#mff38c2e264\" y=\"17.260653\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"201.377418\" xlink:href=\"#mff38c2e264\" y=\"144.217613\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"109.698191\" xlink:href=\"#mff38c2e264\" y=\"193.43049\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"201.126243\" xlink:href=\"#mff38c2e264\" y=\"160.360366\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"246.215092\" xlink:href=\"#mff38c2e264\" y=\"101.635034\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"251.496617\" xlink:href=\"#mff38c2e264\" y=\"105.245533\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"153.245963\" xlink:href=\"#mff38c2e264\" y=\"146.149389\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"140.960344\" xlink:href=\"#mff38c2e264\" y=\"166.908664\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"301.290137\" xlink:href=\"#mff38c2e264\" y=\"74.6649\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"219.214153\" xlink:href=\"#mff38c2e264\" y=\"119.996703\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"144.607244\" xlink:href=\"#mff38c2e264\" y=\"159.16209\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"176.897893\" xlink:href=\"#mff38c2e264\" y=\"150.481183\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"284.45831\" xlink:href=\"#mff38c2e264\" y=\"83.416937\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"360.579178\" xlink:href=\"#mff38c2e264\" y=\"26.103774\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"252.700256\" xlink:href=\"#mff38c2e264\" y=\"77.700797\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"229.148395\" xlink:href=\"#mff38c2e264\" y=\"113.713377\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"209.152843\" xlink:href=\"#mff38c2e264\" y=\"131.665709\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"177.356218\" xlink:href=\"#mff38c2e264\" y=\"145.80565\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"269.524548\" xlink:href=\"#mff38c2e264\" y=\"110.779172\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_2\">\n    <defs>\n     <path d=\"M -3 3 \nL 3 -3 \nM -3 -3 \nL 3 3 \n\" id=\"m672cd94da7\" style=\"stroke:#0000ff;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p033f25aa84)\">\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"160.869723\" xlink:href=\"#m672cd94da7\" y=\"147.051161\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"332.501608\" xlink:href=\"#m672cd94da7\" y=\"34.402299\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"208.584953\" xlink:href=\"#m672cd94da7\" y=\"130.71532\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"74.784181\" xlink:href=\"#m672cd94da7\" y=\"197.100823\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"250.041491\" xlink:href=\"#m672cd94da7\" y=\"100.687748\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"240.539604\" xlink:href=\"#m672cd94da7\" y=\"106.964059\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"347.289443\" xlink:href=\"#m672cd94da7\" y=\"34.402299\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"80.565495\" xlink:href=\"#m672cd94da7\" y=\"197.100823\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"346.701664\" xlink:href=\"#m672cd94da7\" y=\"34.402299\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"314.325062\" xlink:href=\"#m672cd94da7\" y=\"38.445785\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"67.151235\" xlink:href=\"#m672cd94da7\" y=\"197.100823\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"365.61439\" xlink:href=\"#m672cd94da7\" y=\"34.402299\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"201.377418\" xlink:href=\"#m672cd94da7\" y=\"147.051161\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"109.698191\" xlink:href=\"#m672cd94da7\" y=\"197.100823\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"201.126243\" xlink:href=\"#m672cd94da7\" y=\"147.051161\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"246.215092\" xlink:href=\"#m672cd94da7\" y=\"104.295409\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"251.496617\" xlink:href=\"#m672cd94da7\" y=\"100.687748\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"153.245963\" xlink:href=\"#m672cd94da7\" y=\"147.051161\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"140.960344\" xlink:href=\"#m672cd94da7\" y=\"197.100823\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"301.290137\" xlink:href=\"#m672cd94da7\" y=\"83.713038\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"219.214153\" xlink:href=\"#m672cd94da7\" y=\"130.71532\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"144.607244\" xlink:href=\"#m672cd94da7\" y=\"164.127512\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"176.897893\" xlink:href=\"#m672cd94da7\" y=\"147.051161\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"284.45831\" xlink:href=\"#m672cd94da7\" y=\"83.713038\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"360.579178\" xlink:href=\"#m672cd94da7\" y=\"34.402299\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"252.700256\" xlink:href=\"#m672cd94da7\" y=\"100.687748\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"229.148395\" xlink:href=\"#m672cd94da7\" y=\"106.964059\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"209.152843\" xlink:href=\"#m672cd94da7\" y=\"130.71532\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"177.356218\" xlink:href=\"#m672cd94da7\" y=\"147.051161\"/>\n     <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"269.524548\" xlink:href=\"#m672cd94da7\" y=\"100.687748\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mfad571194b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.757279\" xlink:href=\"#mfad571194b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(51.386185 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.306725\" xlink:href=\"#mfad571194b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(98.935631 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"153.856171\" xlink:href=\"#mfad571194b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(146.485077 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.405617\" xlink:href=\"#mfad571194b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- −1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(194.034523 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.955063\" xlink:href=\"#mfad571194b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(245.773813 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"296.504509\" xlink:href=\"#mfad571194b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1 -->\n      <g transform=\"translate(293.323259 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"344.053955\" xlink:href=\"#mfad571194b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2 -->\n      <g transform=\"translate(340.872705 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- X -->\n     <defs>\n      <path d=\"M 6.296875 72.90625 \nL 16.890625 72.90625 \nL 35.015625 45.796875 \nL 53.21875 72.90625 \nL 63.8125 72.90625 \nL 40.375 37.890625 \nL 65.375 0 \nL 54.78125 0 \nL 34.28125 31 \nL 13.625 0 \nL 2.984375 0 \nL 29 38.921875 \nz\n\" id=\"DejaVuSans-88\"/>\n     </defs>\n     <g transform=\"translate(212.957812 252.916562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-88\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m2489cbd0b0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.982813\" xlink:href=\"#m2489cbd0b0\" y=\"202.642505\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- −16 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(20.878125 206.441724)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.982813\" xlink:href=\"#m2489cbd0b0\" y=\"176.98213\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- −14 -->\n      <g transform=\"translate(20.878125 180.781349)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.982813\" xlink:href=\"#m2489cbd0b0\" y=\"151.321755\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- −12 -->\n      <g transform=\"translate(20.878125 155.120974)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.982813\" xlink:href=\"#m2489cbd0b0\" y=\"125.661381\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- −10 -->\n      <g transform=\"translate(20.878125 129.460599)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.982813\" xlink:href=\"#m2489cbd0b0\" y=\"100.001006\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- −8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(27.240625 103.800225)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.982813\" xlink:href=\"#m2489cbd0b0\" y=\"74.340631\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- −6 -->\n      <g transform=\"translate(27.240625 78.13985)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.982813\" xlink:href=\"#m2489cbd0b0\" y=\"48.680257\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- −4 -->\n      <g transform=\"translate(27.240625 52.479475)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"48.982813\" xlink:href=\"#m2489cbd0b0\" y=\"23.019882\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- −2 -->\n      <g transform=\"translate(27.240625 26.819101)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- Y -->\n     <defs>\n      <path d=\"M -0.203125 72.90625 \nL 10.40625 72.90625 \nL 30.609375 42.921875 \nL 50.6875 72.90625 \nL 61.28125 72.90625 \nL 35.5 34.71875 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 34.71875 \nz\n\" id=\"DejaVuSans-89\"/>\n     </defs>\n     <g transform=\"translate(14.798438 118.973906)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-89\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 48.982813 224.64 \nL 48.982813 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 383.782813 224.64 \nL 383.782813 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 48.982812 224.64 \nL 383.782812 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 48.982812 7.2 \nL 383.782812 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 55.982813 44.55625 \nL 143.496875 44.55625 \nQ 145.496875 44.55625 145.496875 42.55625 \nL 145.496875 14.2 \nQ 145.496875 12.2 143.496875 12.2 \nL 55.982813 12.2 \nQ 53.982813 12.2 53.982813 14.2 \nL 53.982813 42.55625 \nQ 53.982813 44.55625 55.982813 44.55625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"PathCollection_3\">\n     <g>\n      <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"67.982812\" xlink:href=\"#mff38c2e264\" y=\"21.173437\"/>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- Label -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     </defs>\n     <g transform=\"translate(85.982812 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"55.712891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"116.992188\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"180.46875\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"241.992188\" xlink:href=\"#DejaVuSans-108\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_4\">\n     <g>\n      <use style=\"fill:#0000ff;stroke:#0000ff;stroke-width:1.5;\" x=\"67.982812\" xlink:href=\"#m672cd94da7\" y=\"35.851562\"/>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- Hypothesis -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 43.015625 \nL 55.515625 43.015625 \nL 55.515625 72.90625 \nL 65.375 72.90625 \nL 65.375 0 \nL 55.515625 0 \nL 55.515625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-72\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     </defs>\n     <g transform=\"translate(85.982812 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"75.195312\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"134.375\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"197.851562\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"259.033203\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"298.242188\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"361.621094\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"423.144531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"475.244141\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"503.027344\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p033f25aa84\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"48.982813\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import sklearn.tree\n",
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter(indent=4).pprint\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# very similar to the regression tree that we implemented previously\n",
    "class XGBoostTree:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def get_leaf_similarity_score(self, examples_in_region, prediction):\n",
    "        \"\"\"Return the similarity score for a leaf\"\"\"\n",
    "        x, Y = examples_in_region\n",
    "        # prediction = np.mean(Y)\n",
    "        similarity = np.sum(Y - prediction)**2 # not mean squared error but squared mean error\n",
    "        print('similarity:', similarity)\n",
    "        return similarity\n",
    "\n",
    "    def get_split_gain(self, x, Y, split_pos):\n",
    "        \"\"\"Get the gain for a given split\"\"\"\n",
    "        parent_prediction = np.mean(Y)\n",
    "        current_similarity = self.get_leaf_similarity_score((x, Y), parent_prediction)\n",
    "        in_first_region = (x[x > split_pos], Y[x > split_pos])\n",
    "        not_in_first_region = (x[x <= split_pos], Y[x <= split_pos])\n",
    "        first_region_similarity = self.get_leaf_similarity_score(in_first_region, parent_prediction)\n",
    "        not_in_first_region_similarity = self.get_leaf_similarity_score(not_in_first_region, parent_prediction)\n",
    "        gain = first_region_similarity + not_in_first_region_similarity - current_similarity\n",
    "        print('gain:', gain)\n",
    "        return gain\n",
    "\n",
    "    def get_split_candidates(self, x):\n",
    "        \"\"\"Get the values of a given feature to be tested as a potential place to split\"\"\"\n",
    "        return (x[:-1] + x[1:]) / 2\n",
    "\n",
    "    def get_best_split(self, X, Y):\n",
    "        \"\"\"Return the best feature and the best value to split the provided datapoints on\"\"\"\n",
    "        best_feature_to_split_on = None\n",
    "        best_gain = 0\n",
    "        best_split_pos = None\n",
    "        n_features = 1\n",
    "        for feature_idx in range(n_features): # for each feature\n",
    "            x = X[:, feature_idx] # get this feature from each example\n",
    "            split_pos, split_gain = self.get_best_gain_on_this_feature(x, Y)\n",
    "            print('best gain on this feature:', split_gain)\n",
    "            if split_gain > best_gain:\n",
    "                best_split_pos = split_pos\n",
    "                best_gain = split_gain\n",
    "                best_feature_to_split_on = feature_idx\n",
    "        # print('best_split_gain:', best_gain)\n",
    "        return best_split_pos, best_gain, best_feature_to_split_on\n",
    "\n",
    "    def get_best_gain_on_this_feature(self, x, Y):\n",
    "        \"\"\"Get the best value to split the data on along this axis\"\"\"\n",
    "        split_candidates = self.get_split_candidates(x)\n",
    "        best_gain = 0\n",
    "        best_split_pos = None\n",
    "        for split_pos in split_candidates:\n",
    "            gain = self.get_split_gain(x, Y, split_pos)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_split_pos = split_pos\n",
    "        return best_split_pos, best_gain\n",
    "\n",
    "    def extend_tree(self, X, Y, depth=0):\n",
    "        \"\"\"Recursively make branches of the tree\"\"\"\n",
    "        split_val, split_gain, split_feature = self.get_best_split(X, Y)\n",
    "        print(f'split val: {split_val}, gain: {split_gain}, split feature:{ split_feature}')\n",
    "        if split_val == None: # split val will not exist (None) if there is only one datapoint in the split\n",
    "            new_branch = {\n",
    "                'split_gain': 0, # so the split is pure\n",
    "                'prediction': np.mean(Y)\n",
    "            }\n",
    "            return new_branch\n",
    "        \n",
    "        positive_idxs = X[:, split_feature] > split_val\n",
    "        positive_X = X[positive_idxs]\n",
    "        positive_Y = Y[positive_idxs]\n",
    "        negative_idxs = np.logical_not(positive_idxs)\n",
    "        negative_X = X[negative_idxs]\n",
    "        negative_Y = Y[negative_idxs]\n",
    "        new_branch = {\n",
    "            'split_gain': split_gain,\n",
    "        }\n",
    "\n",
    "        if depth < self.max_depth: # if not yet at max depth\n",
    "            new_branch.update({\n",
    "                'feature_to_split_on': split_feature,\n",
    "                'value_to_split_on': split_val,\n",
    "                'positive_branch': self.extend_tree(positive_X, positive_Y, depth=depth+1),\n",
    "                'negative_branch': self.extend_tree(negative_X, negative_Y, depth=depth+1)\n",
    "            })\n",
    "        else:\n",
    "            # print(len(Y))\n",
    "            new_branch.update({'prediction': np.mean(Y)})\n",
    "        return new_branch\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit the tree to the data\"\"\"\n",
    "        self.tree = self.extend_tree(X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return a prediction for a given input\"\"\"\n",
    "        classifications = np.zeros(len(X))\n",
    "        for idx, x in enumerate(X):\n",
    "            tree = self.tree\n",
    "            classified = False\n",
    "            while classified == False:\n",
    "                if 'prediction' in tree.keys(): # if the example can be classified from this branch (we have reached a leaf)\n",
    "                    classified = True\n",
    "                    classifications[idx] = tree['prediction']\n",
    "                else: # if we need to ask further questions about the example to make splits\n",
    "                    if x[tree['feature_to_split_on']] > tree['value_to_split_on']: # is the example1 in the positive branch\n",
    "                        tree = tree['positive_branch'] # \n",
    "                    else: \n",
    "                        tree = tree['negative_branch']\n",
    "        return classifications\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Return the pretty printed string\"\"\"\n",
    "        return json.dumps(self.tree, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling XGBoost regression trees to build an XGBoost regression algorithm\n",
    "\n",
    "Similarly to regular gradient boosting, XGBoost sequentially updates an initial prediction by adding predictions from a series of trees, weighted by a learning rate. \n",
    "\n",
    "The difference comes from\n",
    "- the trees that are used are different\n",
    "    - the best splits are evaluated using the gain in similarity score \n",
    "    - the predictions include a regularisation parameter lambda\n",
    "- the pruning applied to trees after they are fit, determined by the tree complexity parameter\n",
    "\n",
    "In XGBoost, the learning rate is known as $\\eta$ (eta). The usual default is $0.3$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost:\n",
    "    def __init__(self, n_trees=10, learning_rate=0.1, tree_depth=1):\n",
    "        self.n_trees = n_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tree_depth = tree_depth\n",
    "\n",
    "    def predict(self, X):\n",
    "        # prediction = np.zeros(len(X))\n",
    "        for tree_idx, tree in enumerate(self.trees):\n",
    "            if tree_idx == 0:\n",
    "                prediction = tree.predict(X)\n",
    "            else:\n",
    "                # print(tree.predict(X))\n",
    "                # scd\n",
    "                prediction += self.learning_rate * tree.predict(X)\n",
    "            print(prediction)\n",
    "        print()\n",
    "        return prediction\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.trees = []\n",
    "        target = Y\n",
    "        for tree_idx in range(self.n_trees):\n",
    "            print(f'tree {tree_idx}')\n",
    "            tree = XGBoostTree(max_depth=self.tree_depth)\n",
    "            # print('x:', X)\n",
    "            # print(X.shape)\n",
    "            tree.fit(X, target)\n",
    "            # print('target:', target)\n",
    "            # print(target.shape)\n",
    "            self.trees.append(tree)\n",
    "            prediction = tree.predict(X).reshape(-1, 1)\n",
    "            # print('prediction:', prediction)\n",
    "            # print(prediction.shape)\n",
    "            target = target - prediction\n",
    "            # print('residual:', target)\n",
    "            print()\n",
    "\n",
    "xgboost = XGBoost(n_trees=20, learning_rate=1, tree_depth=1)\n",
    "xgboost.fit(X, Y)\n",
    "predictions = xgboost.predict(X)\n",
    "print(predictions)\n",
    "visualise_regression_predictions(X, predictions, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost for classification\n",
    "\n",
    "## Cover\n",
    "The minimum number of residuals in each leaf of an XGBoost classification tree is determined by calculating the **cover** as such\n",
    " \n",
    "$cover = \\Sigma [previous\\ probability_i \\cdot (1 - previous\\ probability_i)]$\n",
    "\n",
    "The previous probability is the estimate that would be made if the parent node of this leaf were a leaf. \n",
    "\n",
    "This is the denominator of the similarity score minus $\\lambda$.\n",
    "It has a minimum of 1.\n",
    "So if the computed value for cover is less than this minimum, XGBoost would not allow the leaf to be created.\n",
    "\n",
    "## Pruning \n",
    "For classification we prune in the same way as for XGBoost regression.\n",
    "\n",
    "## Prediction with XGBoost classification trees\n",
    "\n",
    "The prediction for an XGBoost classification tree is given as the **log of the odds** of the example being a member of the positive class.\n",
    "\n",
    "## $prediction = log(odds) = log(\\frac{p}{1-p}) = \\frac{\\Sigma residual_i}{\\Sigma (previous\\ probability_i \\cdot (1 - previous\\ probability_i)  + \\lambda}$\n",
    "\n",
    "Other than lambda, this is the same formula used for regular gradient boosting.\n",
    "\n",
    "**Note: $previous\\ probability$ in the denominator is not the log odds predicted by any leaf of a tree, it is the actual probability. We would need to convert the log odds to a probability by applying the softmax function to it.\n",
    "\n",
    "## Prediction with the whole XGBoost algorithm\n",
    "\n",
    "We know that XGBoost is an ensemble method because it combines predictions made by multiple models.\n",
    "For regression this is easy, because each model is just predicting the residual.\n",
    "For classification it's a little different:\n",
    "\n",
    "Firstly, the initial prediction was made by taking an average of the binary class labels, which means that it represents a probability. The predictions from each tree however represent something different, that is the $log(odds)$. Hence it doesn't make sense to add these different quantitites together. Before we do that we need to transform the initial prediction to represent a $log(odds)$. We do that by applying the following equation:\n",
    "\n",
    "## $log(odds) = log(\\frac{p}{1-p})$\n",
    "\n",
    "**Note: This is the inverse of the logistic function.**\n",
    "\n",
    "Secondly, the final output prediction does not represent the usual confidence that the example is a member of the positive class because it is a sum of the $log(odds)$ rather than that probability. So we need to transform it back from a $log(odds)$ to represent the probability which is what we care about.\n",
    "\n",
    "This is because 1) they may sum up to more than 1 which doesn't make sense for a probability and 2) because the residual may be negative, and the equation for computing the output of a leaf is always positive.\n",
    "So to sum them, we sequentially add the prediction from each tree, weighted by the learning rate $\\eta$, to the initial prediction.\n",
    "\n",
    "We can convert this final prediction back into a probability by applying the logistic function to it:\n",
    "\n",
    "## $probability = \\sigma(log(odds)) = \\frac{1}{1 + e^{-log(odds)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's extreme about XGBoost (eXtreme Gradient Boost)? \n",
    "\n",
    "The above is really only part of XGBoost\n",
    "\n",
    "Several techniques are employed in the XGBoost algorithm to improve the time it takes to fit.\n",
    "They are:\n",
    "- the approximate greedy algorithm\n",
    "\n",
    "We'll talk through each of these below.\n",
    "\n",
    "### The approximate greedy algorithm\n",
    "Because the tree splits are made based on the immediate gain, without considering how the examples that end up on that leaf might be separated by further splits, the algorithm is **greedy**.\n",
    "If the algorithm did not choose to make splits greedily, it would have to consider all possible ways to grow the tree.\n",
    "Exponentially many more trees become possible as the number of branches increase, so because it acts greedily, XGBoost does not waste time rolling out all possible future splits and evaluating them from a given new leaf.\n",
    "\n",
    "As the number of examples increases, so does the number of possible splits (midpoints between any particular feature examples), and hence the compute time and memory.\n",
    "\n",
    "The number of features of each example also linearly increases the number of splits that need to be considered, and hence the compute time and memory.\n",
    "\n",
    "To counteract this, XGBoost does not consider every possible feature midpoint between sequential examples over all different feature dimensions. \n",
    "Instead it groups the examples into quantiles and considers splitting the data at only those positions.\n",
    "The number of quantiles is a hyperparameter.\n",
    "For example, if we chose to split a dataset of 30 examples into 3 quantiles, then for each feature, we would test splitting the data at the value of the 10th and 20th example's feature value.\n",
    "This would reduce the number of splits to test from 29 down to 2.\n",
    "This is called the *approximate* greedy algorithm.\n",
    "\n",
    "# diagram of data split by quantile\n",
    "\n",
    "### \n",
    "\n",
    "### Weighted quantile sketches\n",
    "\n",
    "#### Quantile sketches\n",
    "Firstly, what is a quantile sketch?\n",
    "\n",
    "In practice, when we have LOTS of data, even sorting the examples to find quantiles of features becomes non-trivial and time consuming.\n",
    "\n",
    "Instead, we \n",
    "\n",
    "#### Weighted quantile sketch\n",
    "\n",
    "### Sparsity-aware split finding\n",
    "\n",
    "\n",
    "### Cache-aware access\n",
    "Put the data on the cache\n",
    "\n",
    "### Blocks for out of core computation\n",
    "Compress the data using the CPU so that more of it can fit there."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594311023301",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}