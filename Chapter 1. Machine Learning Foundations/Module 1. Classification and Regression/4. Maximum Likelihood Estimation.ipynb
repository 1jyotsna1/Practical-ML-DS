{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## Learning objectives\n",
    "- Understand mathematically, what it means to find \"optimal\" parameters\n",
    "- Derive MLE from scratch\n",
    "- Quantify the confidence in a particular parameterisation\n",
    "- Understand why we use the mean squared error loss function for regression tasks rather than say the absolute or quartic squared error\n",
    "\n",
    "## Intro - what distribution best describes our data?\n",
    "\n",
    "![](images/mle_compare_dists.jpg)\n",
    "\n",
    "## How can we quantify this?\n",
    "\n",
    "The maximum likelihood parameters of a distribution are those which parameterise a given distribution such that the probability of sampling the observed data from it is maximised.\n",
    "\n",
    "![](images/mle_deriv.jpg)\n",
    "\n",
    "This objective, which results in a model that maximises the likelihood of observing our dataset, is a cross entropy loss.\n",
    "\n",
    "## What is cross entropy?\n",
    "\n",
    "Before we talk about more what it is, here's the mathematical defninition of the cross entropy between two distributions:\n",
    "\n",
    "![](images/cross_entropy_definition.jpg)\n",
    "\n",
    "Goodfellow et al clarify further that \"any loss term that contains a negative log-likelihood is a cross entropy between the empirical distribution defined by the training set and the probability distribution defined by the model\".\n",
    "What this means should become more obvious over the next few paragraphs.\n",
    "\n",
    "### The KL divergence\n",
    "\n",
    "The aim of machine learning is to be able to model things that we care about. \n",
    "We want to be able to know the distribution of features (unsupervised problems) and labels (supervised problems) that appear in the data.\n",
    "That is, we want to know the true data generating distribution.\n",
    "Maximum likelihood **implicitly** minimises the difference between the empirical distribution $\\hat{p}_{data}$ and the model distribution $p_{model}$.\n",
    "This happens because what we minimise in a cross entropy loss, is the same term as that which appears in the equation for the **KL-divergence**, which is an expression which explicitly measures the difference between two distributions, once the terms which do not depend on our model parameterisation are removed.\n",
    "\n",
    "![](images/kl_definition.jpg)\n",
    "\n",
    "There is much, much more to understand about the KL-divergence... look into it.\n",
    "\n",
    "### Minimising the cross entropy implicitly minimises the KL divergence\n",
    "\n",
    "![](images/kl_cross_entropy_argmin.jpg)\n",
    "\n",
    "Throughout the rest of the course, we will try to minimise the expression produced from the above derivation of the MLE parameters, which is a cross entropy between our empirical distribution and our model distribution.\n",
    "This should make sense: our data that we want to model has some underlying distribution, and we want to build a model of the same distribution - so we will minimise their difference.\n",
    "We'll do this by minimising the KL-divergence, which quantifies their difference.\n",
    "But we don't actually have an expression for the empirical distribution which appears in the KL-divergence equation.\n",
    "So instead of minimising the KL-divergence directly, we minimise the cross entropy, which has the same minimum point.\n",
    "\n",
    "The KL-divergence and the cross entropy are related by the [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "\n",
    "![](images/kl_cross_entropy_shannon_entropy_relationship.jpg)\n",
    "\n",
    "## Let's visualise some unsupervised data (no labels) so that we can try to fit a distribution to it using maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DON'T WORRY ABOUT THIS CELL, IT JUST SETS SOME STUFF UP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_regression_data, show_regression_data\n",
    "\n",
    "X, Y = get_regression_data()\n",
    "\n",
    "X = X[:, 0]\n",
    "# Y = np.zeros_like(Y)\n",
    "\n",
    "plt.scatter(X, np.zeros(X.shape[0]), c='r')\n",
    "plt.ylim(0, 0.03)\n",
    "plt.show()\n",
    "\n",
    "mu = np.mean(X)\n",
    "sigma = np.std(X)\n",
    "print('mean:', mu)\n",
    "print('standard deviation:', sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will model our data with a Gaussian (normal distribution). Let's code one up now.\n",
    "\n",
    "![](images/gaussian.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GaussianPDF:\n",
    "    def __init__(self, mu=0, sigma=1):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        p_x = np.exp(-(x - self.mu)**2 / (2*self.sigma**2)) / ( np.sqrt(2*np.pi) * self.sigma) # pdf equation for gaussian\n",
    "        return p_x\n",
    "\n",
    "p = GaussianPDF(mu, sigma)\n",
    "\n",
    "p(0)\n",
    "domain = np.linspace(min(X)-1, max(X)+1)\n",
    "plt.plot(domain, p(domain))\n",
    "plt.scatter(X, np.zeros(X.shape[0]), c='r')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(min(domain), max(domain))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the central position (mean) of the Gaussian distribution which best describes the feature values of the data, assuming that they are normally distributed?\n",
    "\n",
    "Let's test out different Gaussian distributions by evaluating the likelihood of our dataset being sampled from each of them, using the formula we derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "mus = np.linspace(min(X), max(X), 50)\n",
    "objectives = np.zeros_like(mus)\n",
    "best_mu = 0\n",
    "best_val = -float('inf') # initialise best value as infinitely bad\n",
    "for idx, m in enumerate(mus):\n",
    "    p = GaussianPDF(m, sigma)\n",
    "    obj = 0\n",
    "    for x in X:\n",
    "        obj += np.log(p(x)) # compute the log likelihood for this example and add it to the objective which we wish to maximise\n",
    "    objectives[idx] = obj\n",
    "    if obj > best_val:\n",
    "        best_val = obj\n",
    "        best_mu = m\n",
    "    # print(obj)\n",
    "\n",
    "print('BEST VAL:', best_val)\n",
    "print('BEST MU:', best_mu)\n",
    "\n",
    "plt.plot(mus, objectives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's look at the supervised case\n",
    "\n",
    "Let's now consider the case where our dataset contains labels as well as just example features.\n",
    "\n",
    "We can follow the same steps to find an objective function for solving supervised regression problems. We simply model the conditional distribution over the labels given features, rather than modelling the distribution of features.\n",
    "\n",
    "![](images/supervised_mle.jpg)\n",
    "\n",
    "So for every input, we are going to have our model output a distribution, rather than a point estimate. \n",
    "\n",
    "We can do this by simply having our model output the parameters for a particular distribution over labels, such as the mean and variance of a Gaussian.\n",
    "\n",
    "In this case we will assume that the variance of these distributions is fixed; it will not affect the argmax of the distribution anyway.\n",
    "\n",
    "## Note the central limit theorem\n",
    "Here, we assume that the distribution over labels **for a particular example** is Gaussian because of the [central limit theorem](https://www.youtube.com/watch?v=YAlJCEDH2uY).\n",
    "The central limit theorem tells us that the mean of a sample of the same measurement will be normally distributed, even if the individual measurements themselves are not normally distributed.\n",
    "So even if an example's observed label is noisy and is affected by random error, the distribution of the mean of labels assigned to it over many attempts will still be normally distributed.\n",
    "\n",
    "For example, if a human labelling hand-written digits clicks an incorrect label with uniformly random probability, the mean of the labels which they assign to this example will be normally distributed about the true mean.\n",
    "\n",
    "## Visualise labels as distributions\n",
    "For now let's assume that we are modelling the labels of a particular example with a normal distribution.\n",
    "Below we will take Gaussians of a fixed size, and plot them over the y-axis, centred with their mean at the position of the true label for now. \n",
    "These Gaussians vary in the positive x-direction, from the feature value, by the probability that this distribution assigns to any given label value.\n",
    "\n",
    "As mentioned, the Gaussians are fixed in size by having a constant standard deviation and are scaled down in size so that they can be visualised easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from get_colors import colors\n",
    "\n",
    "def plot_dist(x, y, color='blue'):\n",
    "    domain = np.linspace(min(Y), max(Y))\n",
    "    p = GaussianPDF(y, 0.3)\n",
    "    distributions = 0.05*p(domain) # scale down the size of the distributions for visualisation purposes\n",
    "    p_plot = distributions + x # place the distribution extending from the x position of the example which it corresponds to\n",
    "    plt.plot(p_plot, domain, c=colors[i%len(colors)]) # plot each distribution of predicted labels against\n",
    "    plt.scatter(np.ones_like(domain)*x, domain, c=colors[i%len(colors)], marker='.', s=1)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    x = X[i]\n",
    "    y = Y[i]\n",
    "    domain = np.linspace(min(Y), max(Y))\n",
    "    p = GaussianPDF(y, 1)\n",
    "    plt.scatter(x, y, c=colors[i%len(colors)])\n",
    "    plot_dist(x, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's actually use our model to produce these distributions over labels for each input example\n",
    "\n",
    "Our model will probably not produce the same Gaussians because it will likely have some error in predicting the label (mean of the Gaussian) for each example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad = (Y[-1] - Y[0]) / (X[-1] - X[0])\n",
    "print(grad)\n",
    "\n",
    "class LinearModel:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.b + self.w * x\n",
    "\n",
    "model = LinearModel(grad, -8)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    color = colors[i%len(colors)]\n",
    "    if i == 0:\n",
    "        plt.scatter(X[i], Y[i], c=color, label='ground truth')\n",
    "        plt.scatter(X[i], model(X[i]), marker='x', c=color, label='prediction')\n",
    "    plt.scatter(X[i], Y[i], c=color)\n",
    "    plt.scatter(X[i], model(X[i]), marker='x')\n",
    "    plot_dist(X[i], model(X[i]))\n",
    "    plt.legend()\n",
    "plt.plot(X, model(X))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression as maximum likelihood\n",
    "\n",
    "Previously we implemented a linear regression algorithm that made a point estimate of the label $y$ given an input $x$.\n",
    "We chose to minimise the mean squared error somewhat arbitrarily. Why did we choose this as our criterion? Why not minimise the absolute error or the mean quartic error?\n",
    "\n",
    "We want to find the parameters of the model that we have defined that best describe the data. That is, the maximum likelihood parameters $\\theta_{ML}$.\n",
    "Somehow we need to link the probability distributions produced by our model parameterisation.\n",
    "\n",
    "Let's revisit the linear regression algorithm from a perspective of maximum likelihood.\n",
    "Rather than predicting a point estimate, we now have the model predict a conditional probability distribution $p(y|x;\\theta)$ over the label $y$ given $x$.\n",
    "That means that our model is predicting the confidence of the label taking one of a range of possible values, rather than predicting the exact value, for each example input.\n",
    "\n",
    "To yield the same linear regression learning algorithm, we assume that the distribution over labels for a given example is a Gaussian. Again, this is warranted by the central limit theorem.\n",
    "\n",
    "## Deriving the mean squared error loss function from maximum likelihood\n",
    "\n",
    "We know that maximum likelihood means maximising the probability of observing the data predicted by our model. Putting the equation for our assumedly Gaussian probability distribution into the objective that we defined earlier transforms it into the same objective that we minimised previously - the mean squared error.\n",
    "\n",
    "![](images/linear_reg_as_mle.jpg)\n",
    "\n",
    "This should highlight why the MSE loss is what we optimise for, rather than any other arbitrary loss function such as absolute error.\n",
    "\n",
    "## Properties of maximum likelihood\n",
    "\n",
    "### The maximum likelihood estimator is a **consistent** estimator\n",
    "This means that as the number of training examples approaches infinity, the maximum likelihood estimate of the parameter converges to the true value of the parameter.\n",
    "\n",
    "### Statistical efficiency\n",
    "\n",
    "\n",
    "## Summary\n",
    "- Maximum Likelihood Estimation is a procedure for finding parameters that best describe a set of data\n",
    "- The reason that we minimise MSE is because it is the objective produced by plugging a Gaussian distribution into the maximum likelihood objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
