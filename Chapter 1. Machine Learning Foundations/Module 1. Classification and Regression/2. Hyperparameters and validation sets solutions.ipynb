{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and validation sets\n",
    "\n",
    "## Prerequisites\n",
    "- [Bias, variance and generalisation]()\n",
    "- [Gradient based optimisation]()\n",
    "\n",
    "## Learning objectives\n",
    "- understand what hyperparameters are\n",
    "- understand what validation sets are, why we need them and how we use them\n",
    "\n",
    "Start by running the code cell below. It will create a random polynomial function which we will try to model, and also sample some data from it which will be examples in our dataset which we can train and test our model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from numpy.polynomial import Polynomial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "_range = 1\n",
    "all_possible_inputs = np.linspace(-_range, _range, 50) # create a range of inputs \n",
    "m = 20 # how many examples do we want?\n",
    "dataset = np.random.choice(all_possible_inputs, size=(m,)) # here we randomly sample some inputs from the true data\n",
    "true_coeffs = (10, -90, 2, 100) # coefficients of our data generating function\n",
    "Y_fn = np.polynomial.polynomial.Polynomial(true_coeffs) # create a polynomial data generating function with these coefficients\n",
    "Y = Y_fn(dataset) # evaluate our data generating function on these inputs\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "plt.scatter(dataset, Y, c='r', label='Examples in our dataset')\n",
    "plt.plot(all_possible_inputs, Y_fn(all_possible_inputs), c='r', label='True data generating function')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's split our dataset into training and testing sets \n",
    "\n",
    "Sklearn has a module called `model selection` which is made for evaluating models.\n",
    "The starting point for any model evaluation is splitting our dataset into a training set and a test set.\n",
    "The module has a function called `train_test_split` which does this for us.\n",
    "\n",
    "It has a keyword argument `test_size` which is the proportion of the dataset which will end up in the test set.\n",
    "Its default value is 0.25.\n",
    "\n",
    "It has a keyword argument `shuffle` which if true shuffles the dataset before splitting to produce random splits.\n",
    "By default, its value is true.\n",
    "\n",
    "Check out the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('len total dataset:', len(dataset))\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.25, shuffle=True)# randomly split data into train and validation sets\n",
    "\n",
    "print('len train_set:', len(train_data))\n",
    "print('len test_set:', len(test_data))\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(train_data, Y_fn(train_data), c='b', label='Examples in our training dataset')\n",
    "plt.scatter(test_data, Y_fn(test_data), c='c', label='Examples in our training dataset')\n",
    "plt.plot(all_possible_inputs, Y_fn(all_possible_inputs), c='r', label='True data generating function')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a polynomial model that performs multivariate regression and test it's performance.\n",
    "\n",
    "The code below creates a polynomial model of the form:\n",
    "\n",
    "## $y = b + w_1 x + w_2 x^2 + ... + w_n x^n$\n",
    "\n",
    "The loss function which it optimises is the mean squared error loss function:\n",
    "\n",
    "## $L = \\frac{1}{m}\\sum_{i=1}^m(\\hat{y} - y)^2$\n",
    "\n",
    "where $w_i$ is the ith weight of the model not including the bias, $b$.\n",
    "\n",
    "We've done this before, so just run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiVariableLinearHypothesis:\n",
    "    def __init__(self, n_features):\n",
    "        self.n_features = n_features\n",
    "        self.b = np.random.randn()\n",
    "        self.w = np.random.randn(n_features)\n",
    "\n",
    "    def __call__(self, X): #input is of shape (n_datapoints, n_vars)\n",
    "        y_hat = np.matmul(X, self.w) + self.b\n",
    "        return y_hat #output is of shape (n_datapoints, 1)\n",
    "\n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w\n",
    "        self.b = new_b\n",
    "        \n",
    "    def calc_deriv(self, X, y_hat, labels):\n",
    "        diffs = y_hat - labels # calculate error\n",
    "        dLdw = 2 * np.array([np.sum(diffs * X[:, i]) / m for i in range(self.n_features)]) # derivative of MSE term of loss \n",
    "            # ^ equivalent to:\n",
    "            # dLdw = np.zeros(len(diffs))\n",
    "            # for i in range(self.n_features):\n",
    "                # this_feature = X[:, i]\n",
    "                # dLdw[i] = 2 * np.sum(diffs * this_feature) / m\n",
    "        dLdw += 2 * weight_decay * self.w # add derivative of weight decay term of loss                      \n",
    "        dLdb = 2 * np.sum(diffs) / m\n",
    "        return dLdw, dLdb\n",
    "            \n",
    "def train(num_epochs, X, Y, H, learning_rate):\n",
    "    for e in range(num_epochs): # for this many complete runs through the dataset\n",
    "        y_hat = H(X) # make predictions\n",
    "        dLdw, dLdb = H.calc_deriv(X, y_hat, Y) # calculate gradient of current loss with respect to model parameters\n",
    "        new_w = H.w - learning_rate * dLdw # compute new model weight using gradient descent update rule\n",
    "        new_b = H.b - learning_rate * dLdb # compute new model bias using gradient descent update rule\n",
    "        H.update_params(new_w, new_b) # update model weight and bias\n",
    "        \n",
    "def test(X, Y, H):\n",
    "    y_hat = H(X) # make predictions\n",
    "    loss = np.sum((y_hat - Y)**2) / len(Y) # calculate mean squared error\n",
    "    loss += weight_decay * np.dot(H.w, H.w)\n",
    "    return loss\n",
    "                           \n",
    "def create_polynomial_inputs(X, order=3):\n",
    "    new_dataset = np.array([X, *[np.power(X, i) for i in range(2, order + 1)]]).T\n",
    "        # ^ equivalent to:\n",
    "        # new_dataset = [X] # initially the dataset just has X in\n",
    "        # for i in range(2, order + 1): # add polynomial terms from 2 and upward (inclusive of specified order)\n",
    "            # new_order_of_feature = np.power(X, i) # raise to new order\n",
    "            # new_dataset.append(new_order_of_feature) # append new order of features to dataset\n",
    "        # new_dataset = np.array(new_dataset) # convert to np array\n",
    "    return new_dataset # new_dataset should be shape [m, order]\n",
    "\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.1\n",
    "highest_order_power = 5 # change this to adjust the model\n",
    "weight_decay = 0\n",
    "\n",
    "X_train = create_polynomial_inputs(train_data, highest_order_power) # create polynomial inputs from train data\n",
    "Y_train = Y_fn(train_data)\n",
    "H = MultiVariableLinearHypothesis(n_features=highest_order_power) # initialise our model\n",
    "    \n",
    "train(num_epochs, X_train, Y_train, H, learning_rate) # train model and plot cost curve\n",
    "\n",
    "X_test = create_polynomial_inputs(test_data, highest_order_power) # create polynomial inputs from train data\n",
    "Y_test = Y_fn(test_data)\n",
    "loss = test(X_test, Y_test, H)\n",
    "print('TEST LOSS:', loss)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "plt.scatter(train_data, Y_fn(train_data), c='r', label='Label')\n",
    "plt.plot(all_possible_inputs, Y_fn(all_possible_inputs), c='r')\n",
    "plt.scatter(train_data, H(X_train), c='b', label='Prediction', marker='x')\n",
    "plt.plot(all_possible_inputs, H(create_polynomial_inputs(all_possible_inputs, highest_order_power)), c='b')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "You've probably already asked questions along the lines of \"What batch size should I use?\", \"What learning rate should I use?\", \"What polynomial terms should my model include?\".\n",
    "\n",
    "The batch size, learning rate and model architecture are all examples of **hyperparameters**. \n",
    "They are not optimised by the learning algorithm itself.\n",
    "\n",
    "Most commonly, a parameter is chosen to be a hyperparameter becuase optimising this parameter on the training set is not appropriate. \n",
    "For example, the number of polynomial terms in a model is a hyperparameter. \n",
    "This is because a model with more polynomial terms means a model with greater capacity. \n",
    "So if optimised directly, the optimal value is as large as possible (so that the model can represent more functions). \n",
    "This would lead to the model overfitting. \n",
    "As such, any parameters that control the capacity of a model should be hyperparameters. \n",
    "\n",
    "Other times, a parameter may be chosen to be a hyperparameter if is challenging to optimise. For example, the batch size. The batch size is challenging to optimise because you will need to fully train and test many different models to compare how changing it affects the final loss on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why should we not tune our hyperparameters based on our model's score on the test set?\n",
    "\n",
    "Previously we have split our data into train and test sets. The test set is for estimating how well our model  will generalise to perform well on unseen examples. It is important that we do not use the test set to make any decisions about the model. If we do, our model would be overfit to the test set because we are both training and testing our hyperparameters on the test data.\n",
    "\n",
    "You may find that a certain set of hyperparameters perform well on the test set, but then fail to perform as well in the wild when the model is being evaluated on other unseen examples because the hyperparameters that you selected were optimised for *this* test data. \n",
    "\n",
    "## What else can we tune them on? \n",
    "\n",
    "Just like we previously split our data into train and test sets. We can split our train set into a final train set and **validation set**. \n",
    "\n",
    "We can then use this split-off validation dataset to validate that the current hyperparameters will make our model to perform well on unseen data - both the validation set and the test set are unseen.\n",
    "\n",
    "## Generalisation error\n",
    "The model's ability to perform well on the validation set determines how well it will generalise to unseen examples.\n",
    "As such, we call the loss attained on the validation set **generalisation error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('len of initial train set', len(train_data))\n",
    "train_data, val_data,  = torch.utils.data.random_split(train_data, [8, 7]) # randomly split data into train and validation sets\n",
    "\n",
    "print('len of remaining train_set:', len(train_data))\n",
    "print('len val_set:', len(val_data))\n",
    "\n",
    "X_val = create_polynomial_inputs(val_data, highest_order_power) # create polynomial inputs from train data\n",
    "Y_val = Y_fn(val_data)\n",
    "\n",
    "def validate(X, Y, H):\n",
    "    y_hat = H(X) # make predictions\n",
    "    loss = np.sum((y_hat - Y)**2) / len(Y) # calculate mean squared error\n",
    "    loss += weight_decay * np.sum(H.w**2) # add weight decay term to loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement a grid search to optimise our hyperparameters\n",
    "\n",
    "Let's systematically try out a range of different learning rates and number of polynomial terms in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE ARE SOME HYPERPARAMETERS THAT WE WILL OPTIMISE\n",
    "learning_rates = [0.001, 0.01, 0.1, 1]#list(np.linspace(0, 0.001, 10))\n",
    "highest_order_powers = list(range(1, 10))\n",
    "\n",
    "losses = np.zeros((len(learning_rates), len(highest_order_powers)))\n",
    "for pow_idx, highest_order_power in enumerate(highest_order_powers):\n",
    "    \n",
    "    X_train = create_polynomial_inputs(train_data, highest_order_power) # create polynomial inputs\n",
    "    Y_train = Y_fn(train_data)\n",
    "    \n",
    "    X_val = create_polynomial_inputs(val_data, highest_order_power) # create polynomial inputs\n",
    "    Y_val = Y_fn(val_data)\n",
    "    \n",
    "    H = MultiVariableLinearHypothesis(n_features=highest_order_power) # initialise our model\n",
    "    for lr_idx, lr in enumerate(learning_rates):\n",
    "        train(num_epochs, X_train, Y_train, H, lr)\n",
    "        loss = validate(X_val, Y_val, H) # validate\n",
    "#         print(f'Weight_decay: {weight_decay}\\t\\tHighest order power: {highest_order_power}\\t\\tLoss: {loss}')\n",
    "        losses[lr_idx][pow_idx] = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to visualise these parameterisations in 3d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm\n",
    "\n",
    "norm = plt.Normalize(losses.min(), losses.max())\n",
    "colors = cm.viridis(norm(losses))\n",
    "rcount, ccount, _ = colors.shape\n",
    "\n",
    "%matplotlib \n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "best_lr_idx, best_order_idx = np.unravel_index(losses.argmin(), losses.shape)\n",
    "print(losses.shape)\n",
    "print(losses)\n",
    "print('best lr idx:', best_lr_idx)\n",
    "print('best order idx:', best_order_idx)\n",
    "best_lr = learning_rates[best_lr_idx]\n",
    "best_order = highest_order_powers[best_order_idx]\n",
    "print('best lr:', best_lr)\n",
    "print('best power:', best_order)\n",
    "ax.plot([best_lr, best_lr], [best_order, best_order], [0, losses.max()], c='#ff822e', label='Optimal hyperparameters')\n",
    "lrs, pows = np.meshgrid(highest_order_powers, learning_rates)\n",
    "print(lrs.shape)\n",
    "print(pows.shape)\n",
    "print(losses.shape)\n",
    "s = ax.plot_surface(pows, lrs, losses, cmap=cm.copper, rcount=rcount, ccount=ccount, facecolors=colors, shade=False)\n",
    "ax.set_xlabel('Learning rate')\n",
    "ax.set_ylabel('Highest order power')\n",
    "ax.set_zlabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a loss surface over a hyperparameter search:\n",
    "![](images/hyperparameters_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- hyperparameters are parameters of our algorithm that don't make sense to optimise directly\n",
    "    - either because they are difficult to optimise because they need to be tested across many different trained models\n",
    "    - or because they \n",
    "- a validation set is another subset of our whole dataset alongside our training and test sets\n",
    "- hyperparameters can be tuned by evaluating performance on the validation set because this tests their generalisation without biasing performance, which would be evaluated on the test set\n",
    "- hyperparameters should not be tuned on the test set because this is prone to overfitting\n",
    "\n",
    "### Challenges\n",
    "- Implement a genetic hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
