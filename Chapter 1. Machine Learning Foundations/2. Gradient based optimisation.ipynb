{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient based optimisation\n",
    "\n",
    "## Prerequisites\n",
    "- [Calculus primer]\n",
    "- [Intro to machine learning]()\n",
    "\n",
    "## Learning objectives\n",
    "- Understand the concepts behind gradient based optimisation and learning\n",
    "- Implement the stochastic gradient descent (SGD) algorithm from scratch in Python\n",
    "\n",
    "## Intro\n",
    "\n",
    "Previously, we saw how we could optimise parameters of our models using random search. This technique had some fundamental flaws though...\n",
    "- we had to specify the range which we would search for parameter values within, and this might not contain the optimal parameter values (e.g. if we search all parameters from 1 to 10, but the ideal parameter value is 15, then we will never find an optimal parameterisation)\n",
    "- we don't use information about a current parameterisation as a heuristic for where to search next. We simply take either a totally random value (random search) or a predetermined value (grid search)\n",
    "- to maintain the same resolution of search, the number of datapoints that you need scales exponentially with the number of parameters. This is very bad (the neural networks which we will use soon can easily have millions of parameters).\n",
    "\n",
    "This notebook will walk through how we can use **gradient based optimisation** as another technique to find model parameterisations that perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is gradient based optimisation all about?\n",
    "\n",
    "Grid search and random search evaluate the loss/error/cost (which tells us how bad our model is) for each different parameterisation that they test. \n",
    "\n",
    "Our loss is just a mathematical function that depends on the parameters of our model (for example, we used the mean squared error (MSE) loss function in the previous notebook).\n",
    "We would like to move our parameters to the point where loss is minimised.\n",
    "\n",
    "If we were to evaluate the value of our loss for every possible different parameterisation of our model, we would produce a **loss surface**. \n",
    "We would like to find the lowest point on this surface. \n",
    "At this point it will have a gradient (steepness) of zero with respect to the parameters.\n",
    "\n",
    "As our parameters move away from that minima in some direction, the gradient will increase in that direction.\n",
    "To get back to the minima, we should hence move our weights in the opposite direction.\n",
    "This tells us that wherever we are, we can decrease the current value of the loss by moving in the opposite \n",
    "direction to the gradient. This is at the core of gradient based optimisation.\n",
    "\n",
    "![](./images/grad-based-optim.jpg)\n",
    "\n",
    "Below is a more complex potential loss surface (vertical axis represents loss value, others represent parameter values).\n",
    "\n",
    "<img style=\"height: 200px\" src='./images/comp-loss-surface.png'/>\n",
    "\n",
    "**Note: because gradient based optimisation depends on us computing the gradient of the loss function, our loss function and model must be fully differentiable (they must be a smooth, continuous function).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"265.500132pt\" version=\"1.1\" viewBox=\"0 0 382.603125 265.500132\" width=\"382.603125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 265.500132 \nL 382.603125 265.500132 \nL 382.603125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 227.943882 \nL 375.403125 227.943882 \nL 375.403125 10.503882 \nL 40.603125 10.503882 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mce2d51225b\" style=\"stroke:#ff0000;\"/>\n    </defs>\n    <g clip-path=\"url(#p66d0e794db)\">\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"302.725813\" xlink:href=\"#mce2d51225b\" y=\"55.695555\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"119.021206\" xlink:href=\"#mce2d51225b\" y=\"186.009089\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"302.725813\" xlink:href=\"#mce2d51225b\" y=\"67.488497\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"342.911196\" xlink:href=\"#mce2d51225b\" y=\"25.617688\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"182.169665\" xlink:href=\"#mce2d51225b\" y=\"139.76558\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"256.799661\" xlink:href=\"#mce2d51225b\" y=\"78.032853\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"124.761975\" xlink:href=\"#mce2d51225b\" y=\"164.586583\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"55.872747\" xlink:href=\"#mce2d51225b\" y=\"210.44696\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"182.169665\" xlink:href=\"#mce2d51225b\" y=\"126.322571\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"251.058892\" xlink:href=\"#mce2d51225b\" y=\"98.204314\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"360.133503\" xlink:href=\"#mce2d51225b\" y=\"20.473066\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"205.132741\" xlink:href=\"#mce2d51225b\" y=\"131.256211\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"268.281199\" xlink:href=\"#mce2d51225b\" y=\"83.257859\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"279.762737\" xlink:href=\"#mce2d51225b\" y=\"68.317316\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"308.466582\" xlink:href=\"#mce2d51225b\" y=\"35.631854\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"274.021968\" xlink:href=\"#mce2d51225b\" y=\"65.448891\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"245.318123\" xlink:href=\"#mce2d51225b\" y=\"97.633178\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"331.429658\" xlink:href=\"#mce2d51225b\" y=\"32.280494\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"337.170427\" xlink:href=\"#mce2d51225b\" y=\"52.684618\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"251.058892\" xlink:href=\"#mce2d51225b\" y=\"83.303598\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"296.985044\" xlink:href=\"#mce2d51225b\" y=\"55.622479\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"61.613516\" xlink:href=\"#mce2d51225b\" y=\"217.974699\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"90.317361\" xlink:href=\"#mce2d51225b\" y=\"194.160799\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"164.947358\" xlink:href=\"#mce2d51225b\" y=\"147.15031\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"164.947358\" xlink:href=\"#mce2d51225b\" y=\"148.591856\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"342.911196\" xlink:href=\"#mce2d51225b\" y=\"39.811303\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"136.243513\" xlink:href=\"#mce2d51225b\" y=\"166.643813\"/>\n     <use style=\"fill:#ff0000;stroke:#ff0000;\" x=\"193.651203\" xlink:href=\"#mce2d51225b\" y=\"139.429197\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m2d553763cf\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.872747\" xlink:href=\"#m2d553763cf\" y=\"227.943882\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(49.510247 242.54232)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.280437\" xlink:href=\"#m2d553763cf\" y=\"227.943882\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(106.917937 242.54232)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.688127\" xlink:href=\"#m2d553763cf\" y=\"227.943882\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(164.325627 242.54232)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"228.095816\" xlink:href=\"#m2d553763cf\" y=\"227.943882\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(221.733316 242.54232)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.503506\" xlink:href=\"#m2d553763cf\" y=\"227.943882\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(279.141006 242.54232)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"342.911196\" xlink:href=\"#m2d553763cf\" y=\"227.943882\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 70 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(336.548696 242.54232)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- X -->\n     <defs>\n      <path d=\"M 6.296875 72.90625 \nL 16.890625 72.90625 \nL 35.015625 45.796875 \nL 53.21875 72.90625 \nL 63.8125 72.90625 \nL 40.375 37.890625 \nL 65.375 0 \nL 54.78125 0 \nL 34.28125 31 \nL 13.625 0 \nL 2.984375 0 \nL 29 38.921875 \nz\n\" id=\"DejaVuSans-88\"/>\n     </defs>\n     <g transform=\"translate(204.578125 256.220445)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-88\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7fc2f2b160\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7fc2f2b160\" y=\"197.013169\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 200.812388)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7fc2f2b160\" y=\"166.010844\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 169.810063)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7fc2f2b160\" y=\"135.008519\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 138.807738)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7fc2f2b160\" y=\"104.006194\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 107.805413)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7fc2f2b160\" y=\"73.003869\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 76.803088)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7fc2f2b160\" y=\"42.001544\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 45 -->\n      <g transform=\"translate(20.878125 45.800763)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7fc2f2b160\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 50 -->\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Y -->\n     <defs>\n      <path d=\"M -0.203125 72.90625 \nL 10.40625 72.90625 \nL 30.609375 42.921875 \nL 50.6875 72.90625 \nL 61.28125 72.90625 \nL 35.5 34.71875 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 34.71875 \nz\n\" id=\"DejaVuSans-89\"/>\n     </defs>\n     <g transform=\"translate(14.798438 122.277789)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-89\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 227.943882 \nL 40.603125 10.503882 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 375.403125 227.943882 \nL 375.403125 10.503882 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 227.943882 \nL 375.403125 227.943882 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 10.503882 \nL 375.403125 10.503882 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p66d0e794db\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"40.603125\" y=\"10.503882\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# DON'T WORRY ABOUT THIS CELL, IT JUST SETS SOME STUFF UP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data():\n",
    "    data = np.genfromtxt('DATA/winequality-red.csv', delimiter=';') ## Import income data and save to variable.\n",
    "    data = data[1:] # remove NaNs\n",
    "    X = data[:, :-1] # get all of the rows and all but the last column (the last column is the labels)\n",
    "    X = X[:, 0]\n",
    "    Y = data[:, -1] # get the last column as the labels\n",
    "    return X, Y\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.show()\n",
    "\n",
    "X, Y = get_data()\n",
    "plot_data(X, Y)\n",
    "\n",
    "def plot_h_vs_y(X, y_hat, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', label='Label')\n",
    "    plt.scatter(X, y_hat, c='b', label='Hypothesis', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Wine acidity')\n",
    "    plt.ylabel('Quality')\n",
    "    plt.show()\n",
    "    \n",
    "# DEFINE MEAN SQUARED ERROR LOSS FUNCTION\n",
    "def L(y_hat, labels):\n",
    "    errors = y_hat - labels # calculate errors\n",
    "    squared_errors = np.square(errors) # square errors\n",
    "    mean_squared_error = np.sum(squared_errors) / (m) # calculate mean \n",
    "    return mean_squared_error # return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Gradient descent is an iterative, gradient based optimisation technique. As indicated above, we simply pass our data through our model and compute the gradient of the loss (which is a vector that points in the direction that our parameters should move to most increase the loss), then adjust the weights by a small amount in the opposite direction.\n",
    "\n",
    "### Gradient descent algorithm\n",
    "- randomly initialise model parameters\n",
    "- while not converged:\n",
    "    - Calculate the cost of this parameterisation (by evaluating how it performs on some examples) and the derivative of our cost with respect to (w.r.t) each parameter. This derivative points in the direction of steepest ascent (the gradient of the loss surface for this parameterisation). \n",
    "    - Update each parameter value by taking a step in the opposite direction. \n",
    "\n",
    "### The learning rate, $\\alpha$\n",
    "\n",
    "We will update our parameters by shifting them in the opposite direction to the gradient. But by what amount should we shift them in that direction?\n",
    "\n",
    "If the step size were some constant value, then our model might need to adjust its weights by some value smaller than this to reach a nearby minima. \n",
    "\n",
    "We know that the gradient at a minima is zero, and at this point we want our parameters to be moved with a step size of zero - so that they remain where they are, at the minima. So let's consider the weights of the model being updated by a step size proportional to the gradient. We call the proportionality constant the **learning rate**, and denote it as $\\alpha$.\n",
    "\n",
    "If the step size were directly equal to the gradient ($\\alpha=1$), gradient descent can fail to converge because the steps are too large (the same problem hence occurs if the learning rate is too large)\n",
    "\n",
    "![title](images/high-lr.jpg)\n",
    "\n",
    "So we include the learning rate to scale down the size of the steps. The learning rate should most likely be less than 1.\n",
    "\n",
    "If the learning rate is too low, then our model can take too long (too many gradient descent iterations) to converge\n",
    "\n",
    "![title](images/low-lr.jpg)\n",
    "\n",
    "You should play around with the learning rate and adjust it until your model converges.\n",
    "\n",
    "![title](images/convergence.jpg)\n",
    "\n",
    "The diagrams shown above visualise how a single weight affects the loss. A model with multiple weights would be optimised in the same way - we would just have more of these functions. We can think of each of the graphs as a cross section through a **loss surface**. A loss surface is shown below which visualises how the criterion of a model might vary with both parameters.\n",
    "\n",
    "<img style=\"height: 200px\" src='./images/comp-loss-surface.png'/>\n",
    "\n",
    "If we know the function that the loss is computed from and it is differentiable, then we can calculate the derivative of the loss with respect to our model parameters by hand.\n",
    "\n",
    "Below is a derivation for computing the rate of change (gradient) of the loss with respect to our model parameters when using a linear model and the mean squared error loss function.\n",
    "![title](images/NN1_single_grad_calc.jpg)\n",
    "\n",
    "Complete the class below to return the derivative of our loss w.r.t the weight and bias by implementing the above equations in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-6197.767481629964 -113.58228688251207\n"
    }
   ],
   "source": [
    "class LinearHypothesis:\n",
    "    def __init__(self): \n",
    "        self.w = np.random.randn() # weight\n",
    "        self.b = np.random.randn() # bias\n",
    "    \n",
    "    def __call__(self, X): # how do we calculate output from an input in our model?\n",
    "        y_hat = self.w*X + self.b\n",
    "        return y_hat\n",
    "    \n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w\n",
    "        self.b = new_b\n",
    "        \n",
    "    def calc_deriv(self, X, y_hat, labels):\n",
    "        m = len(Y) # m = number of examples\n",
    "        diffs = y_hat - labels # calculate errors\n",
    "        dLdw = 2*np.array(np.sum(diffs*X) / m) # calculate derivative of loss with respect to weights\n",
    "        dLdb = 2*np.array(np.sum(diffs)/m) # calculate derivative of loss with respect to bias\n",
    "        return dLdw, dLdb\n",
    "    \n",
    "H = LinearHypothesis() # initialise our model\n",
    "y_hat = H(X) # make prediction\n",
    "dLdw, dLdb = H.calc_deriv(X, y_hat, Y) # calculate gradient of current loss with respect to model parameters\n",
    "\n",
    "print(dLdw, dLdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can complete the derivatives, complete the train function below to iteratively improve our parameter estimates to minimize the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "H = LinearHypothesis()\n",
    "\n",
    "def train(num_epochs, X, Y, H, L, plot_cost_curve=False):\n",
    "    all_costs = [] # initialise empty list of costs to plot later\n",
    "    for e in range(num_epochs): # for this many complete runs through the dataset\n",
    "        y_hat = H(X) # make predictions\n",
    "        cost = L(y_hat, Y) # compute loss \n",
    "        dLdw, dLdb = H.calc_deriv(X, y_hat, Y) # calculate gradient of current loss with respect to model parameters\n",
    "        new_w = H.w - learning_rate * dLdw # compute new model weight using gradient descent update rule\n",
    "        new_b = H.b - learning_rate * dLdb # compute new model bias using gradient descent update rule\n",
    "        H.update_params(new_w, new_b) # update model weight and bias\n",
    "        all_costs.append(cost) # add cost for this batch of examples to the list of costs (for plotting)\n",
    "    if plot_cost_curve: # plot stuff\n",
    "        plt.figure() # make a figure\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.plot(all_costs) # plot costs\n",
    "    print('Final cost:', cost)\n",
    "    print('Weight values:', H.w)\n",
    "    print('Bias values:', H.b)\n",
    "    #return cost, H.w\n",
    "    \n",
    "train(num_epochs, X, Y, H, L, plot_cost_curve=True) # train model and plot cost curve\n",
    "plot_h_vs_y(X, H(X), Y) # plot predictions and true data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why gradient based optimisation?\n",
    "We previously mentioned shortcomings of random search:\n",
    "- our search region not containing an optimal parameterisation for our model\n",
    "- exponential increase in runtime with each additional parameter\n",
    "\n",
    "But beyond these, an advantage of using gradient based optimisation is that it follows a **heuristic** - an indication of how to improve. The heuristic is the gradient, which indicates what might be a good way to improve the weights. \n",
    "Grid and random search are not heuristic search methods. Each time they try a new parameterisation, they don't get any more information about where might be a good next parameterisation. Instead, they simply try a new set of values by choosing totally randomly (random search) or by picking a predetermined value at the next point on the grid (grid search).\n",
    "\n",
    "By using gradient descent, which follows a heuristic indication of where to try next (down the hill), our model can converge in much less iterations compared to grid or random search. If we firstly initialise our parameters near to the optima on the loss surface, gradient descent might only need a few updates, whereas grid or random searches will always take the same amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should we not pass the whole dataset through the model for each update?\n",
    "We know that to perform gradient based optimisation we need to pass inputs through the model (forward pass), and then compute the loss and find how it changes with respect to each of our model's parameters (backward pass). Modern datasets can be absolutely huge. This means that the forward pass can take a long time, as the function which our model represents has to be applied to each and every input given to it for a forward pass.\n",
    "\n",
    "Passing the full dataset through the model at each pass is called **full batch gradient descent**.\n",
    "\n",
    "### Why not just pass a single datapoint to the model for each update?\n",
    "We want our model to perform well on all examples, not just single examples. So we want to compute the loss and associated gradients over several examples to get an average gradient that should lead to better performance across any example, not just this specific one. If we only pass a single example through, the gradient won't be based on a representative sample.\n",
    "\n",
    "Passing single examples through the model at each pass is called **stochastic gradient descent**.\n",
    "\n",
    "## Mini-batch gradient descent\n",
    "The modern way to do training is neither full-batch (whole dataset) or fully stochastic (single datapoint). Instead we use mini-batch training, where we sample several (but not all) datapoints to compute a sample of the gradient, which we then use to update the model. Most optimisation algorithms converge much faster if they are allowed to rapidly compute approximate gradients rather than slowly compute exact gradients. The size of the mini-batch is called the **batch size**. Mini-batches are commonly incorrectly referred to as batches, but it's not that deep. \n",
    "\n",
    "We will experiment with the effect of batch size on the training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from random import shuffle\n",
    "\n",
    "def create_batches(dataset, batch_size=4):\n",
    "    shuffle(dataset) # shuffle the dataset\n",
    "    idx = 0 # initialise starting point in dataset (index of first example to be put into the next batch)\n",
    "    batches = []\n",
    "    while idx < len(dataset): # while starting point index is less than the length of the dataset \n",
    "        if idx + batch_size < len(dataset): # if enough examples remain to make a whole batch\n",
    "            batch = dataset[idx: idx + batch_size] # make a batch from those examples \n",
    "        else: # otherwise\n",
    "            batch = dataset[idx:] # take however many examples remain (less than batch size)\n",
    "        batches.append(batch) # add this batch to the list of batches\n",
    "        idx += batch_size # increment the starting point for the next batch\n",
    "    batches = [np.array(list(zip(*b))) for b in batches] # unzip the batches into lists of inputs and outputs so batch = [all_inputs, all_outputs] rather than batch = [(input_1, output_1), ..., (input_batch_size, output_batch_size)]\n",
    "    return batches\n",
    "\n",
    "dataset = list(zip(X, Y))\n",
    "data_loader = create_batches(dataset, batch_size=4)\n",
    "print(data_loader)\n",
    "print(len(data_loader)) # should be m / batch_size rounded up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update our training function so that it performs mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "H = LinearHypothesis()\n",
    "m = 10000\n",
    "num_updates = 10 * m\n",
    "X, Y = sample_linear_data(m)\n",
    "dataset = list(zip(X, Y))\n",
    "\n",
    "def train(num_updates, data_loader, H, L, plot_cost_curve=False, plot_h=False):\n",
    "    costs = [] # initialise empty list of costs to plot later\n",
    "    update_idx = 0\n",
    "    inference_times = []\n",
    "    update_times = []\n",
    "    while update_idx < num_updates: # for this many complete runs through the dataset\n",
    "        batch_costs = []\n",
    "        for x, y in data_loader:\n",
    "            inference_start = time() # get time at start of inference\n",
    "            y_hat = H(x) # make predictions\n",
    "            inference_times.append(time() - inference_start) # add duration of inference\n",
    "            cost = L(y_hat, y) # compute loss \n",
    "            update_start = time()\n",
    "            dLdw, dLdb = H.calc_deriv(x, y_hat, y) # calculate gradient of current loss with respect to model parameters\n",
    "            new_w = H.w - learning_rate * dLdw # compute new model weight using gradient descent update rule\n",
    "            new_b = H.b - learning_rate * dLdb # compute new model bias using gradient descent update rule\n",
    "            H.update_params(new_w, new_b) # update model weight and bias\n",
    "            update_times.append(time() - update_start)\n",
    "            update_idx += 1\n",
    "            batch_costs.append(cost)\n",
    "            #prop_complete = round((update_idx / num_updates) * 100)     \n",
    "            #print('\\r' + [\"|\", \"/\", \"-\", \"\\\\\"][update_idx % 4], end='')\n",
    "            #print(f'\\r[{prop_complete * \"=\" + (0 - prop_complete) * \"-\"}]', end='')\n",
    "        costs.append(np.mean(batch_costs)) # add cost for this batch of examples to the list of costs (for plotting)\n",
    "    if plot_cost_curve: # plot stuff\n",
    "        plt.figure() # make a figure\n",
    "        plt.ylabel('Cost')\n",
    "        plt.xlabel('Update idx')\n",
    "        plt.plot(costs) # plot costs\n",
    "        plt.show()\n",
    "    if plot_h:\n",
    "        plot_h_vs_y(X, H(X), Y)\n",
    "    print(f'Average inference (prediction) time: {np.mean(inference_times)*1000:.3f} milliseconds')\n",
    "    print(f'Average weight update time: {np.mean(update_times)*1000:.3f} milliseconds')\n",
    "    print('Final cost:', cost)\n",
    "#     print('Weight values:', H.w)\n",
    "#     print('Bias values:', H.b)\n",
    "    print()\n",
    "    \n",
    "\n",
    "print('Full batch training')\n",
    "full_batch_data_loader = create_batches(dataset, batch_size=len(dataset))\n",
    "train(num_updates, full_batch_data_loader, H, L, plot_cost_curve=False)\n",
    "\n",
    "print('Stochastic training')\n",
    "stochastic_data_loader = create_batches(dataset, batch_size=1)\n",
    "train(num_updates, stochastic_data_loader, H, L, plot_cost_curve=False)\n",
    "\n",
    "print('Mini-batch training')\n",
    "mini_batch_data_loader = create_batches(dataset, batch_size=32)\n",
    "train(num_updates, mini_batch_data_loader, H, L, plot_cost_curve=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that stochastic and mini-batch training perform inference (making predictions) >3x faster and make updates 2.5x faster than full batch training. For larger datasets or inputs, these differences would be exaggerated even further. This is because the model has to pass the whole dataset forward to make a prediction and then compute the average gradient from every single one of those values to compute the weight and bias updates.\n",
    "\n",
    "For a larger dataset with more complex example features and/or labels, stochastic gradient descent may not converge, because single examples may rarely produce a gradient that is representative of an update that would reduce the error for all examples, rather than just this example.\n",
    "\n",
    "## Summary\n",
    "- gradient based optimisation is an optimisation technique based on iteratively adjusting the parameters of a model in the direction that will decrease the objective function\n",
    "- stochastic gradient descent is a robust and scalable way to train parametric models\n",
    "    - optimisation time scales linearly with the number of parameters\n",
    "- random search and grid search are far inferior to SGD & other gradient based optimisation techniques. Please don't use them again.\n",
    "- full batch training updates the parameters based on the gradient of the loss for all samples with respect to the model parameters\n",
    "- stochastic training updates the parameters based on the gradient of the loss for a single sample with respect to the model parameters\n",
    "- splitting our datasets into mini-batches can improve training speed\n",
    "    - less examples to process per update\n",
    "    - each update is based on a representative sample of the gradient to follow\n",
    "\n",
    "## Next steps\n",
    "- [Logistic regression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit4e36a22c5ab145cdaac0d5c6b1a34fd0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}