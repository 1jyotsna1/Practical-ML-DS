{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles 1: Random forests\n",
    "\n",
    "## Learning objectives\n",
    "- understand \n",
    "    - bootstrapping datasets\n",
    "    - building ensembles by aggregating predictions\n",
    "    - bagging\n",
    "    - random forests\n",
    "- implement\n",
    "    - your first esemble\n",
    "    - a random forests\n",
    "\n",
    "## Intro - ensembles\n",
    "### \"The collective is smarter than the individual\"\n",
    "\n",
    "\n",
    "If we attempted to build an ensemble with multiple instances of the same model on the same dataset, this wouldn't help. \n",
    "Why?\n",
    "Because they would all be identical (other than the differences induced by any stochastisity in the optimisation process).\n",
    "This means that they would all make the same mistakes, and combining their predictions would not give any improvement.\n",
    "\n",
    "### Why do ensembles work?\n",
    "Mathematically, ensembles work because the mistakes made between models are not correlated.\n",
    "This is because all of the errors are correlated.\n",
    "\n",
    "To make sure the model errors are not correlated, we can't just train the same model on the same dataset, otherwise all of the errors would be the same for each model.\n",
    "We can make the predictions differ (and uncorrelate the errors) in different ways, some of which we discuss below. \n",
    "\n",
    "### Bootstrapping datasets\n",
    "The first way that we can make the models differ is by training them on different data.\n",
    "We can \"bootstrap\" new datasets by sampling with replacement from the existing datasets\n",
    "\n",
    "Let's get our data and make some bootstrapped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from get_colors import colors\n",
    "\n",
    "def get_data(sd=6, m=10, n_features=2, n_clusters=2):\n",
    "    X, Y = sklearn.datasets.make_blobs(n_samples=m, n_features=n_features, centers=n_clusters, cluster_std=sd)\n",
    "    return X, Y\n",
    "\n",
    "def create_bootstrapped_dataset(existing_X, existing_Y, sample_weights=None):\n",
    "    \"\"\"Create a single bootstrapped dataset\"\"\"\n",
    "    idxs = np.random.choice(np.arange(len(existing_X)), replace=True)\n",
    "    return existing_X[idxs], existing_Y[idxs]\n",
    "\n",
    "def create_bootstrapped_datasets(X, Y, n):\n",
    "    \"\"\"Create n bootstrapped datasets\"\"\"\n",
    "    datasets = []\n",
    "    for d in range(n):\n",
    "        x, y = create_bootstrapped_dataset(X, Y)\n",
    "        datasets.append((x, y))\n",
    "    return datasets\n",
    "\n",
    "X, Y = get_data()\n",
    "n_trees = 10\n",
    "bootstrapped_datasets = create_bootstrapped_datasets(X, Y, n_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Random forests\n",
    "Random forests are ensembles of decision trees (many trees make a forest).\n",
    "\n",
    "### Learning on feature subsets\n",
    "Another way that we can make the predictions differ is by only allowing the model to make predictions based on a limited number of the features. \n",
    "That is, we train each model on the features of examples from the dataset that have been projected into a subspace of the feature space.\n",
    "\n",
    "Let's use the sklearn `DecisionTreeClassifier` as our model, and train an ensemble of them on different random subspaces of features to create a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[-7.95532263]\n [13.00869119]\n [ 5.21951674]\n [-1.65583386]\n [-4.43587736]\n [-5.07070575]\n [-6.05277874]\n [ 1.36745376]\n [-1.22256703]\n [11.27625644]]\n[[ -7.95532263  -6.5324316 ]\n [ 13.00869119   6.24425574]\n [  5.21951674   6.51036873]\n [ -1.65583386  -1.3158393 ]\n [ -4.43587736   0.32247454]\n [ -5.07070575   2.00740652]\n [ -6.05277874  -1.4526738 ]\n [  1.36745376  -3.05795278]\n [ -1.22256703   1.28295427]\n [ 11.27625644 -14.02775716]]\n"
    }
   ],
   "source": [
    "def project_into_subspace(X, feature_idxs):\n",
    "    return X[:, feature_idxs]\n",
    "\n",
    "projected_X = project_into_subspace(X, [0])\n",
    "print(projected_X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit4e36a22c5ab145cdaac0d5c6b1a34fd0",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}