{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    /* Jupyter */\n",
    "    .rendered_html tr, .rendered_html th, .rendered_html td {\n",
    "        text-align: left; \n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Types of Data\n",
    "\n",
    "## Learning Objectives\n",
    "- Population and Samples\n",
    "- Descriptive statistics\n",
    " - Measures of central tendency\n",
    " - Measures of dispersion\n",
    "- The difference between Categorical and Continuous variables\n",
    "- How these break down into Ordinal, Nominal, Interval and Ratio\n",
    " - And what descriptive statistics to apply to them\n",
    "- Basic visualiations using Plotly\n",
    "\n",
    "In the previous lesson, we took an extensive dive into cleaning our data. Our next 'data science step' would be to find and deal with missing values, but prior to diving into this, it's relevant to discuss the different types of data you'll be dealing with and their legal summary statistics. The type of data being investigated determines what statistical tools are available/recommended, and understanding the different types is vital in effectively analysing different datasets.\n",
    "\n",
    "We'll start with a brief discussion on what **populations** are and a **sample**'s relation to them. We will then load in a dataset to discuss the difference in the types of variables we are presented with - whether they are **categorical** or **continuous**. Subsequent to breaking these down into a further taxonomy of **ordinal**, **nominal**, **interval**, and **ratio**, we will discuss **descriptive statistics**, specifically focusing on **measures of central tendency** and **measures of dispersion**.\n",
    "\n",
    "## Populations and Samples\n",
    "\n",
    "Firstly, it's important to realise that in the large majority of cases when we're working with data, we do not have access to all the members of the **population**. The population is the set which contains the *entire members* of a specified group. For example:\n",
    "- Hours of sleep for *all* undergraduate students\n",
    "- Education level of *all* customers at US grocery stores\n",
    "- Time spent participating in group sports by *all* boys ages 15-19\n",
    "\n",
    "You have probably realised that collecting data on all members of the population is in a lot of cases, infeasible. Such a task would be expensive in terms of both time and cost. However we often want to make judgements or come to conclusions about entire populations.\n",
    "\n",
    "The solution to this is **sample** data, and typically whenever we see statistics being reported to us regarding an entire population, it is highly likely that these statistics have been calculated on a sample of our population. A small, but *well chosen* sample is able able to accurately provide us with information representative of a whole population.\n",
    "\n",
    "\"Well chosen\" is important. Imagine we wanted to make assumptions regarding the education level of customers at US grocery stores. If we picked our samples almost exclusively from Silicon Valley (I am assuming the average person in Silicon Valley has a higher education level than other cities in the US), our results and analysis would be skewed because we would be making assumptions about the entire US customer population based on results from one area. Therefore, it is important that *samples are representative of the population they are drawn from*. We can generally assume that samples we receive (via datasets) are chosen randomly and exclusively across the constraints of the population - but when it comes round to you having to build your own keep the following guidelines in mind. I have motivated the following table with the use of an example to show how violations of the guidelines could occur.\n",
    "\n",
    "<b>Hours of sleep for all undergraduate students</b>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>Guideline</b></td>\n",
    "        <td><b>Violation</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>All elements in a sample must (by definition) be part of the defined population</td>\n",
    "        <td>Sample includes hours of sleep for postgraduate students</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Sample must be representative of the population</td>\n",
    "        <td>If we only considered the sleep of male students</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Samples of the same population should be independant of each other</td>\n",
    "        <td>\"Refer a friend\"; allowing multiple entries into the sample survey</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Samples should (in most cases) be picked randomly</td>\n",
    "        <td>Focusing efforts on one geographic region; finding participants only through Instagram; surveying participants of one ethnic type</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "So, through the samples that we're working on in a dataset, it is relevant to note that a sample statistics are therefore an *approximation* of the true parameters of the population. Based on our sample at hand (that is, the sample size vs the population size), we may have to accept that there may be some uncertainty or margin of errors in the reported statistics. For information about picking an statistically appropiate sample size, [see here](https://www.itl.nist.gov/div898/handbook/ppc/section3/ppc333.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "Before discussing any more theory, let's load in and quickly look at a dataset of houses in Brazil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../DATA/houses_to_rent.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, from the get go we can identify the following about the columns:\n",
    "- **city**: a boolean column indicating whether the property is in the city or not\n",
    "- **area**: the area (unsure of units - probably $m^2$?) of the property\n",
    "- **rooms**: number of rooms in the property\n",
    "- **bathroom**: number of bathrooms in the property\n",
    "- **parking spaces**: number of parking spaces in the property\n",
    "- **floor**: looks like either the story-floor the property is, *or* how many floor numbers it has\n",
    "- **animal**: whether animals/pets are allowed\n",
    "- **furniture**: whether or not the property is furnished\n",
    "- **hoa, rent amount, property tax, fire insurance**: monthly costs to pay (hoa stands for home owners association)\n",
    "- **total**: total monthly cost\n",
    "\n",
    "It's essential to do an analysis of any dataframe when we receive it - as mentioned in the last lecture, a through investigation and understanding the provided data dictionary or documentation is the quickest and most valuable way to get to grips with what the dataset can offer you.\n",
    "\n",
    "Anyway, about **descriptive statistics**. A descriptive statistic is a summary statistic that quantitatively describes features of a dataset. __Descriptive statistics__ is the process of using and analyzing those statistics. Rather than trying to learn from the data, in descriptive statistics, we simply aim to summarize the data. The most common types of descriptive statistics are measures of __central tendency__ and measures of __dispersion or variability__. Let's use our house price dataset to explore these types of measures. \n",
    "\n",
    "\n",
    "### Measures of Central tendency\n",
    "Central tendency is a central or typical value from the distribution in a set of data points. This measure is based on the idea that data points tend to cluster around a central value.\n",
    "\n",
    "#### Mode\n",
    "In descriptive statistics, we may want to compute the __mode__ of a sample. This represents the observations with the highest __frequency__, which is the number of times it occurs. It is a useful measure when dealing with categorical data, which we will look at later.\n",
    "\n",
    "#### Mean\n",
    "The __mean__ of a sample, also known as the __average__, is a quantity used to estimate the mean of the entire population we are looking at, as known as the __expected value__. The __population__ mean is given by the sum of all members divided by the number of observations (n):\n",
    "\n",
    "$$\\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_{i} $$\n",
    "\n",
    "The sample mean varies ever so slightly - it is actually given by the sum of all observations divided by the number of observations minus one (n-1):\n",
    "\n",
    "$$\\bar{x} = \\mathbb{E}(X) = \\frac{1}{n-1}\\sum_{i=1}^{n}x_{i} $$\n",
    "\n",
    "Where $\\mathbb{E}(\\cdot)$ represents the __expectation operator__ and X represents the sample. We won't go into why $n-1$ is used over $n$ in this course, but if you were curious, please refer to: https://en.wikipedia.org/wiki/Bessel's_correction. Furthermore, since our datasets tend to be large in the number of observations, we will usually omit the $(n-1)$ in favour of $n$, allowing us to use built-in `.mean()`, `.std()` and `.var()` functions.\n",
    "\n",
    "Below we calculate the sample mean of the internal housing area of houses in Brazil. As we do not have access to information for _all_ houses in Brazil, the sample we have will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the sample average\n",
    "area_mean = np.mean(df[\"area\"])\n",
    "\n",
    "print(\"The original sample mean of the internal area of houses in Brazil is\", area_mean)\n",
    "print()\n",
    "print(\"Rest of the dataset:\")\n",
    "df[\"area\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the value we get, it seems to be pretty representative of the dataset. This is the aim of descriptive statistics: to find values that are **representative** of the dataset. However, the sample mean is not robust in the presence of **outliers**, which are values that are much smaller or much larger that most other observations.\n",
    "\n",
    "Below we introduce a large outlier value to our dataset and recalculate the sample mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding outlier to our sample and re-computing the sample mean\n",
    "df.loc[len(df)] = 0\n",
    "df.at[len(df)-1, \"area\"] = 100000\n",
    "print(\"The new sample mean of the internal area of houses in Brazil is\", np.mean(df[\"area\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, even one large outlier is already enough to change our sample mean. How do we know whether we are calculating statistics with an outlier or not though? One useful and practical way is to visualise our data using a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.box(df, y=\"area\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't really see our boxplot properly due to our artificially added house. Lets remove this and try again. Note that this is a process in identifying outliers of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know we added our data into the last row - we'll drop this, replot our boxplot and see if there are any other things classified as outliers\n",
    "df.drop(len(df)-1, axis=0, inplace=True)\n",
    "fig = px.box(df, y=\"area\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so with our artifical house removed, we've now replotted the box plot. Actually, we can see the presence of two other extremely high values which could be skewing the mean which was returned earlier. As we progress through this notebook we will more formally introduce the boxplot and how to read it. But for now, we can assume that these values are outliers, and in this case, they are safe to remove. To do so requires identifying which rows contain these values.\n",
    "\n",
    "We see two of these ridiculously high values and thus to find the rows, we can search our dataframe for the rows which have two biggest areas contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_largest = df.nlargest(2, \"area\")\n",
    "two_largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = df[~df.isin(two_largest)].dropna(how=\"all\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, y=\"area\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! Now we see our boxplot with more rigour. If we hover over it, we can see what each of the 'lines' we have represent:\n",
    "- min\n",
    "- q1\n",
    "- median\n",
    "- q3\n",
    "- upper fence\n",
    "- max\n",
    "\n",
    "Let's look at these further to understand what these measures mean, before tying it all back to the boxplot.\n",
    "\n",
    "#### Median\n",
    "The __median__ of a sample is the middle value of a sequence of observations arranged in ascending order. If we have an odd number of observations, it is the single middle value, and if we have an even number of observations it is the average between the two middle values. The median of internal areas of houses is computed below for both the original housing data and the sample with an outlier introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the median\n",
    "median = np.median(df[\"area\"])\n",
    "\n",
    "print(\"Sample median of the internal area of houses is\", median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Dispersion (Variability)\n",
    "__Dispersion__, also known as __variability__, is a measure of how stretched or squeezed the distribution of our data is. \n",
    "\n",
    "#### Range\n",
    "The __range__ of a sample is the size of the smallest interval in which we can fit _all_ our observations, and can be computed as the difference between the maximum and minimum values out of all observations:\n",
    "\n",
    "$$ range = x_{max} - x_{min} $$\n",
    "\n",
    "Since it only depends on two values of the dataset, it is generally used for smaller datasets. It is also not a robust measure in the presence of outliers. The range of the internal area of houses in Brazil is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the range of the dataset\n",
    "housing_area_range = np.amax(df[\"area\"]) - np.amin(df[\"area\"])\n",
    "print(\"The sample range is\", housing_area_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance and standard deviation\n",
    "The __variance__ of a population is defined as the average of the square of the difference between the mean of the data and each observation. It is essentially the answer to the question: \"On average, how far are the observations from the mean?\". The square of the difference mainly serves to make sure that all values are positive. The variance of population can be estimated through the __sample variance__, computed as follows:\n",
    "\n",
    "$$s^{2} = \\mathbb{E}[(X-\\bar{x})^{2}] = \\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i} - \\bar{x})^{2} $$\n",
    "\n",
    "(note that the $N-1$ comes from Bessel's correction which was introduced earlier. We continue under the same protocol as outlined earlier)\n",
    "\n",
    "However, since we are taking the square of the differences, if we scale our data by a constant, _a_, the variance of our dataset is scaled by $a^{2}$, meaning that the sample variance is not linear in scale. Thus, what is often used instead of the variance is the square root of the variance, known as the __standard deviation__. A useful property is that, approximately, 68% of the data is within 1 standard deviation from the mean, 95% of the data within 2 standard deviations from the mean and 99.7% of the data is within 3 standard deviations of the mean. \n",
    "\n",
    "Both the standard deviation and variance are computed below for the internal area of houses in Brazil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing variance and standard deviation\n",
    "sample_variance = np.var(df[\"area\"])\n",
    "sample_std = np.std(df[\"area\"])\n",
    "\n",
    "print(\"Sample Variance:\", sample_variance)\n",
    "print(\"Sample standard deviation:\", sample_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quartiles and Interquartile Ranges\n",
    "\n",
    "Before we have seen that the median splits our data into separate sections of the same length. Because of how it is defined, we have 50% of the data before the median (first-half) and 50% after (second-half). We can split our dataset further by finding the median of the first-half and the median of the second-half, giving us four distinct subsets of observations, all of equal length. Below is a diagram explaining this process in more detail, where:\n",
    "- Q1 is the median of the first-half, known as the __lower quartile__\n",
    "- Q2 is the median of the full dataset\n",
    "- Q3 is the median of the second-half, known as the __upper quartile__\n",
    "\n",
    "Q1, Q2 and Q3 are the __quartiles__ of the sample.\n",
    "\n",
    "<img src=https://www.onlinemathlearning.com/image-files/xmedian-quartiles.png.pagespeed.ic.fzcCJEohbz.webp />\n",
    "\n",
    "By subtracting Q3 by Q1, we get what is known as the __interquartile range (IQR)__. It is a measure of dispersion that, unlike the _range,_ is unaffected by outliers. Below, we compute the different quartiles and compute a __boxplot__ of the data generated below. Let's calculate the IQR of our area data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "areas = np.sort(df[\"area\"])\n",
    "Q1 = areas[floor(len(areas) * 0.25)]\n",
    "Q2 = areas[floor(len(areas) * 0.5)]\n",
    "Q3 = areas[floor(len(areas) * 0.75)]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(\"Lower Quartile:\", Q1)\n",
    "print(\"Second Quartile/Median:\", Q2)\n",
    "print(\"Upper Quartile:\", Q3)\n",
    "print(\"Interquartile Range:\", IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can validate that these figures are what has been returned to us by the boxplot. However, from the boxplot, we also see something called \"upper fence\". Boxplots will have their uppermost lines at either the maximum value of the data, or the upper fence (and similarly for the lowermost lines). Plotly cleverly works out which choice is preferrable. We can calculate the upper and lower fences by:\n",
    "\n",
    "$$\n",
    "\\text{Upper Fence} = Q3 + (1.5 * IQR) \\\\\n",
    "\\text{Lower Fence} = Q1 - (1.5 * IQR)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thus...\n",
    "upper_fence = Q3 + (1.5 * IQR)\n",
    "print(\"Upper Fence:\", upper_fence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1600 area value seems to be a huge outlier based on the boxplot earlier. \n",
    "## Remove the row and replot the boxplot\n",
    "largest = df.nlargest(1, \"area\")\n",
    "df = df[~df.isin(largest)].dropna(how=\"all\")\n",
    "fig = px.box(df, y=\"area\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Data\n",
    "\n",
    "But let's talk a bit about the some of the columns in our dataframe. We mentioned **categorical** data in our last lesson, but let's expand on it before discussing **continuous** data.\n",
    "\n",
    "### Categorical Data\n",
    "\n",
    "__Categorical data__ refers to data that has a _finite_ number of possible categories we can observe. If we are collecting data on someone's country of births, we know that there are only 195 possibilites. Likewise, if we are collecting data on the highest place each player reached in a US Open championship, there are only 156 possible places. Both of these are examples of categorical data.\n",
    "\n",
    "#### Nominal Data\n",
    "In the case of country of birth being collected as data, there is no inherent ordering. This type of data is known as __nominal data__, and is the classification of data such as name, gender and ethnicity. \n",
    "\n",
    "#### Ordinal Data\n",
    "On the other hand, we know that a player that reached 1st place in the US Open scored higher than those in 2nd or 3rd place, so there is an inherent ordering of our data. This type of data is known as __ordinal data__, and refers to data that has an order, but the distance between possibilities is not uniform throughout. The difference between 1st and 2nd place is not necessarily the same as the difference between 2nd and 3rd, 3rd and 4th, and so on. Given this, ordinal scales can be used to measure non-numeric features like happiness, customer satisfaction or even the  difficulty of ski slopes!\n",
    "\n",
    "When dealing with categorical data, we can calculate how many times a category occurs, known as the __frequency__. We can also determine the __relative frequency__ by dividng the frequency of a category by the total number of observations. Using frequencies, we can calculate the _mode_ of the data set. Given that nominal data is not numerical and has no inherent order, we cannot compute any other statistics on it. With ordinal data, on the other hand, given the inherent ordering, we can also calculate the _median,_ and consequently, the _interquartile range._ We will view how to visualize these data types in a later section.\n",
    "\n",
    "We will now compute the descriptive statistics of the \"furniture\" feature, which is nominal, and the \"rooms\" feature, which is ordinal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.convert_dtypes()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When dealing with nominal data, we can convert the datatype to a category and then extract the mode quite with ease\n",
    "df[\"furniture\"] = df[\"furniture\"].astype(\"category\")\n",
    "df[\"furniture\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see some statistics regarding our `furniture` column. That is, the most common entry (the **mode**), is `not furnished`, which appears 4495/6077 times (i.e. a relative frequency of ~74%).\n",
    "\n",
    "Computing the descriptive statistics of the rooms data is also as simple as using `.describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rooms\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data\n",
    "__Numerical data__ refers to data that we can carry out different operations on, and can either be __discrete__ or __continuous__. The common distinction is that continuous data is something you __measure__ (weight, height, temperature) and discrete data is something you __count__ (population, number of candies in a bag). Discrete data contains separate values, whereas continuous data can take any value in a given range. For example, if I am measuring how many US Opens a tennis player has won in their career, the number can only be 0,1,2,3,4,5,6 or 7, but nothing in between, having then a _finite_ set of possibilities, being a discrete variable. If am measuring the height of previous US Open winners, even if with the range 150-215cm, there are _infinitely_ many possible heights that can be observed, making it a continuous variable.\n",
    "\n",
    "Numerical data can be further sub-divided into __interval data__ and __ratio data__. Interval values represent ordered values that have the same units, meaning that unlike ordinal data, we know the difference between different values. However, interval data does not have an __absolute zero__. This means that we can add or subtract this data, but we cannot multiply or divide it. An example of interval data is temperature. Temperature has no absolute zero, as in there no such concept as 'no temperature'. This means that I can say that 32ºC is 16ºC warmer than 16ºC, but I CANNOT say that 32ºC is twice as warm as 16ºC.\n",
    "\n",
    "On the other hand, ratio data has an absolute zero. For instance, height is a type of ratio data. Since 0cm is the absolute zero, we can add, subtract, multiply and divide this data type. Given this, we can make statements such as \"2 metres is twice as tall as 1 metre\".\n",
    "\n",
    "For numerical data, we can apply all the descriptive statistics we have covered so far! We apply these below to the \"total\" price sample, which is a type of ratio data given it has an absolute zero. We will review ways to visualize these data types in later sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before returning the descriptive statistics, we need to clean the column first.\n",
    "# Let's drop all the NAs (if any) and then convert the column to an int after removing \"R$\" and \",\"\n",
    "# We should probably also check whether hoa + rent amount + property tax + fire insurance = total\n",
    "df = df.dropna(axis=0, subset=[\"total\"])\n",
    "# \\ to escape the $ sign\n",
    "df[\"total\"] = df[\"total\"].str.replace(\",\", \"\").str.replace(\"R\\$\", \"\").astype(\"int64\")\n",
    "df[\"total\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hoa\"][35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's validate our numerical dates!\n",
    "cols_to_check = [\"hoa\", \"rent amount\", \"property tax\", \"fire insurance\"]\n",
    "for col in cols_to_check:\n",
    "    print(col)\n",
    "    df[col] = df[col].str.replace(\",\", \"\").str.replace(\"R\\$\", \"\")\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "sum_cols = df[cols_to_check].sum(axis=1)\n",
    "sum_cols != df[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_which_correct = sum_cols == df[\"total\"]\n",
    "df = df[total_which_correct]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"total\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
