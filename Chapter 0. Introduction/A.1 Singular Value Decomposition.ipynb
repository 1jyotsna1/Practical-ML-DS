{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "The __singular value decomposition__ (SVD) is a highly relevant method in data science and machine learning, and is where we can bring together all we had learned in previous sections about vectors and matrices. SVD is a method that allows us to decompose a rectangular matrix into a different form. It is tailored to a given problem, as it depends on the data inside a matrix for each instance. Given that we can represent the data in a new form, it is useful for data reduction, and even the basis of __principal component analysis (PCA)__, which we will cover in a later section.\n",
    "\n",
    "SVD is regarded as one of the most important techniques in linear algebra. It is used in the Google rank algorithm to find the best matches for your search, recommender systems like Netflix and Amazon, and even facial recognition with a SVD representation fo human faces. It is also scalable to any size data set, hence why even Google can do it!!\n",
    "\n",
    "We can apply this method to any rectangular matrix. If we consider a MxN rectangular matrix, A, SVD allows us to decompose it in the following form:\n",
    "\n",
    "$$A = U\\Sigma V^{T} = \n",
    "\\begin{bmatrix}\n",
    "| & | &   & | \\\\ \\mathbf{\\vec{u_{1}}} & \\mathbf{\\vec{u_{2}}} & ... & \\mathbf{\\vec{u_{N}}} \\\\ | & | &   & | \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{1} & 0 & ... & 0 \\\\ 0 & \\sigma_{2} &  & 0 \\\\ \\vdots &  & \\ddots & \\vdots \\\\ 0 & ... & ... & \\sigma_{M} \\\\ -- & -- & -- & -- \\\\ 0 & 0 & ... & 0 \\\\ \\vdots & \\vdots &  & \\vdots \\\\ 0 & 0 & ... & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "| & | &   & | \\\\ \\mathbf{\\vec{v_{1}}} & \\mathbf{\\vec{v_{2}}} & ... & \\mathbf{\\vec{v_{N}}} \\\\ | & | &   & | \n",
    "\\end{bmatrix} ^{T}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- __U__ is a NxN square, orthogonal matrix, known as the __right singular vectors__\n",
    "- __V__ is a MxM square, orthogonal matrix, known as the __left singular vectors__\n",
    "- $\\mathbf{\\Sigma}$ is a MxN diagonal matrix, containing the __singular values__ along its diagonal\n",
    "\n",
    "These three matrices are unique for any given rectangular matrix (except by flipping the sign of each vector). So how do we carry out SVD? This is where our understanding of eigenvalues and eigenvectors comes in, where construct each of the three matrices individually:\n",
    "- U: composed of the the eigenvectors of $AA^{T}$\n",
    "- $V^{T}$: composed of the eigenvectors of $A^{T}A$\n",
    "- $\\Sigma$ is composed of the square root of the corresponding eigenvalues of $AA^{T}$ or $A^{T}A$\n",
    "\n",
    "An interesting property is that the eigenvalues of $AA^{T}$ and $A^{T}A$ are always the same so we can pick either one of them. Let us now compute the SVD of the simple matrix below. We will first use NumPy to carry out the invidiual steps, the confirm our answers with the in-built SVD function. \n",
    "\n",
    "We will start by determining the matrix U\n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & 3 & 2 \\\\ 2 & 2 & 1 \\end{bmatrix} $$\n",
    "\n",
    "We can use NumPy to compute $AA^{T}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 10]\n",
      " [10  9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define our original matrix\n",
    "A = np.array([[1,3,2],\n",
    "              [2,2,1]])\n",
    "A_T = np.transpose(A)\n",
    "\n",
    "AA_T = np.matmul(A,A_T)\n",
    "print(AA_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that:\n",
    "$$ AA^{T} = \\begin{bmatrix} 14 & 10 \\\\ 10 & 9 \\end{bmatrix}$$\n",
    "\n",
    "We can compute its eigenvectors and eigenvalues using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [21.80776406  1.19223594]\n",
      "Eigenvectors: [[ 0.78820544 -0.61541221]\n",
      " [ 0.61541221  0.78820544]]\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "w,v = np.linalg.eig(AA_T)\n",
    "print(\"Eigenvalues:\",w)\n",
    "print(\"Eigenvectors:\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to carry out orthonormalization to make all the columns of our matrix orthonormal with respect to each other. We will use the previously encountered Gram-Schmidt orthonormalization process, as shown below:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.78820544 -0.61541221]\n",
      " [-0.61541221  0.78820544]]\n"
     ]
    }
   ],
   "source": [
    "Q = np.linalg.qr(v)\n",
    "print(Q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our first matrix, U:\n",
    "$$U = \\begin{bmatrix} -0.788 & -0.615 \\\\ -0.615 & 0.788 \\end{bmatrix}$$\n",
    "\n",
    "The same procedure can be applied to determine the matrix, $V^{T}$, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  7  4]\n",
      " [ 7 13  8]\n",
      " [ 4  8  5]]\n",
      "\n",
      "Eigenvalues: [2.18077641e+01 1.19223594e+00 1.21630936e-15]\n",
      "Eigenvectors: [[ 0.4323517   0.88011958  0.19611614]\n",
      " [ 0.76992171 -0.24711681 -0.58834841]\n",
      " [ 0.46935336 -0.4053675   0.78446454]]\n",
      "Orthonormal matrix: [[-0.4323517   0.88011958  0.19611614]\n",
      " [-0.76992171 -0.24711681 -0.58834841]\n",
      " [-0.46935336 -0.4053675   0.78446454]]\n"
     ]
    }
   ],
   "source": [
    "# Computing the augmented matrix\n",
    "A_TA = np.matmul(A_T,A)\n",
    "print(A_TA)\n",
    "print()\n",
    "\n",
    "# Computing eigenvalues and eigenvectors\n",
    "w,v = np.linalg.eig(A_TA)\n",
    "Q = np.linalg.qr(v)\n",
    "print(\"Eigenvalues:\",w)\n",
    "print(\"Eigenvectors:\",v)\n",
    "print(\"Orthonormal matrix:\",Q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct $V^{T}$ from the above eigenvectors, and $\\Sigma$ from the above eigenvalues:\n",
    "$$ V = \\begin{bmatrix} -0.432 & 0.880 & 0.196 \\\\ -0.769 & -0.247 & -0.588 \\\\ -0.469 & -0.405 & 0.784 \\end{bmatrix}, \\;\n",
    "V^{T} = \\begin{bmatrix} -0.432 & -0.769 & -0.469 \\\\ 0.880 & -0.247 & -0.405 \\\\ 0.196 & -0.588 & 0.784 \\end{bmatrix}$$\n",
    "$$ \\Sigma = \\begin{bmatrix} \\sqrt{21.808} & 0 & 0 \\\\0 & \\sqrt{1.192} & 0 \\end{bmatrix} = \n",
    "\\begin{bmatrix} 4.670 & 0 & 0 \\\\0 & 1.092 & 0 \\end{bmatrix}$$\n",
    "\n",
    "We can even confirm our result, as the matrix product of the three matrices we have constructed should simply return A, and the in-built SVD function should return the same three matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96563268 2.9957695  1.99789114]\n",
      " [1.97211363 1.99606394 0.99849057]]\n",
      "\n",
      "[[-0.78820544 -0.61541221]\n",
      " [-0.61541221  0.78820544]]\n",
      "[4.66987838 1.09189557]\n",
      "[[-0.4323517  -0.76992171 -0.46935336]\n",
      " [ 0.88011958 -0.24711681 -0.4053675 ]\n",
      " [ 0.19611614 -0.58834841  0.78446454]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing our matrices\n",
    "U = np.array([[-0.788,-0.615],[-0.615,0.788]])\n",
    "V = np.array([[-0.423,0.880,0.196],[-0.769,-0.247,-0.588],[-0.469,-0.405,0.784]])\n",
    "S = np.array([[4.670,0,0],[0,1.092,0]])\n",
    "\n",
    "# Checking whether we get the original matrix\n",
    "A_check = np.matmul(U,np.matmul(S,np.transpose(V)))\n",
    "print(A_check)\n",
    "print()\n",
    "\n",
    "\n",
    "# Checking whether we get the same matrices with the in-built SVD function\n",
    "U,S,V_T = np.linalg.svd(A)\n",
    "print(U)\n",
    "print(S)\n",
    "print(V_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decomposition contains all the pieces that we need to put our original matrix back together. The singular values($\\sigma_{i}$) are arranged in a hierarchical way such that:\n",
    "$$\\sigma_{1} > \\sigma_{2} > ... > \\sigma_{M}$$\n",
    "\n",
    "Since diagonal matrices scale each row of a matrix by the respective diagonal component, when we carry out SVD, __these singular values represent the importance of the vectors in U and V__. Given this structure, the vectors in U and V are also arranged according to their importance. This allows us to carry out what is known as the __reduced singular value decomposition__. By only keeping the first R components of the SVD representation, where R < M, R < N, given that R is large enough, we can obtain an __approximation__ of the original matrix, $\\hat{A}$, that is good enough given our application of SVD with a smaller number of coefficients.\n",
    "\n",
    "Take the example below, we have a 10x10 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 5 8 7 2 3 6 0 7 2]\n",
      " [6 1 4 2 8 7 6 6 6 3]\n",
      " [1 1 0 6 6 1 6 3 8 9]\n",
      " [2 3 8 2 9 3 5 5 6 6]\n",
      " [6 0 9 4 0 9 6 1 3 3]\n",
      " [2 8 8 5 0 0 5 3 7 2]\n",
      " [3 3 4 0 2 1 5 0 4 3]\n",
      " [9 0 3 4 5 2 8 9 3 7]\n",
      " [0 8 8 6 6 3 8 2 2 9]\n",
      " [3 7 9 9 5 3 9 4 4 9]]\n"
     ]
    }
   ],
   "source": [
    "# Defining our matrix\n",
    "# row = np.linspace(1,10,10)\n",
    "# A = np.empty(0)\n",
    "# for i in range(10):\n",
    "#     A = np.append(A,(i+1)*row)\n",
    "# A = np.reshape(A,(10,10))\n",
    "# print(A)\n",
    "A = np.random.randint(0,10,(10,10))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.35613753  4.88572832  8.01346662  5.88982544  0.76385942  3.23888009\n",
      "   6.83698114  0.50342527  7.60840887  2.5136414 ]\n",
      " [ 5.92122618 -0.14045263  5.18560269  1.66646194  7.59662965  6.17326749\n",
      "   5.55648707  6.02389052  6.34498469  3.79422221]\n",
      " [ 1.17017307  1.15180494 -0.14000493  6.34299005  6.38446667  1.02489476\n",
      "   5.85396492  2.86104868  7.7938558   8.74968165]\n",
      " [ 1.85174133  4.03214416  6.90101221  1.89621773  8.91557226  3.86453421\n",
      "   5.68615274  5.16332284  5.90527391  5.47384921]\n",
      " [ 6.34386294  0.44534268  8.57977822  4.69010787  0.77838193  9.15984127\n",
      "   5.75637248  0.7334978   2.56576217  2.43862738]\n",
      " [ 2.42749056  7.95012152  8.10523192  5.77382641  0.85619601 -0.26325814\n",
      "   4.41719373  2.64078418  6.59263803  1.66643483]\n",
      " [ 3.50869902  3.14396471  3.94628583  0.84292582  2.94147388  0.85880994\n",
      "   4.3396326  -0.38054782  3.53135161  2.61435536]\n",
      " [ 9.06881221  0.23501203  2.7521852   4.19764589  5.22513525  2.13758974\n",
      "   8.04148422  8.93956961  2.85866256  6.750123  ]\n",
      " [-0.1013773   7.39136883  8.58282271  5.85250509  5.81485064  2.56853426\n",
      "   7.92093403  2.01325815  2.16278361  9.29913113]\n",
      " [ 2.87471467  7.03560373  8.99474896  8.62118887  4.58532554  3.11720577\n",
      "   9.09134806  4.16095123  4.20335409  9.28056548]]\n"
     ]
    }
   ],
   "source": [
    "# Computing the SVD \n",
    "U,S,V_T = np.linalg.svd(A)\n",
    "\n",
    "# Number of components we wanna keep (R)\n",
    "R = 9\n",
    "\n",
    "# Only keeping the first 5 components\n",
    "U_hat = U[:,0:R]\n",
    "S_hat = S[0:R]\n",
    "V_T_hat = V_T[0:R,:]\n",
    "\n",
    "# Constructing the diagonal matrix\n",
    "sigma_hat = np.zeros((R,R))\n",
    "for i in range(R):\n",
    "    sigma_hat[i,i] = S_hat[i]\n",
    "\n",
    "# Computing our original matrix approximation from the reduced SVD\n",
    "A_hat = np.matmul(U_hat,np.matmul(sigma_hat,V_T_hat))\n",
    "print(A_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for the simple case above, only one single component was enough to fully restore the original matrix. This is what makes SVD such a powerful technique in data reduction. Another example below is in image compression, where we can use SVD to compress an image into a much smaller amount of information.\n",
    "# ADD IMAGE COMPRESSION SVD EXAMPLE FOR A NICE VISUAL UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
