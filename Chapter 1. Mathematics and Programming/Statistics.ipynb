{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics**\n",
    "\n",
    "* Talk about expectations. https://en.wikipedia.org/wiki/Expected_value\n",
    "* Variance: show a distribution. Conceptually cover what variance is\n",
    "* Variance: show how variance is calculated and start calculating it \n",
    "* Covariance: show covariance of 2 variables (as height goes up so does weight). Then start showing how you measure this \n",
    "* Talk about correlation and why covariance and correlation are separate things.\n",
    "* Another important concept in stats is probability. \n",
    "* Show 4 different ways of saying the same probability\n",
    "* Why is probability and expectation linked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics and Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variables and Descriptive Statistics\n",
    "A __random variable__ is defined as a variable whose values depend on outcomes of random events. For example, when we carry out an experiment, we never get exactly the same result twice, maybe due to some measurement errors or some flaws in the system we are using to measure. But what is __randomness__? \n",
    "\n",
    "Some argue that this variation is not random and simply due to high-complexity processes that we are in no place of following accurately. For example, Robert Brown, a famous physicist, saw that pollen particles in water seem to bounce around randomly by bouncing about the water molecules. Some would argue that this was not random, and that if we knew where __all__ water molecules were and where they were going at all times, we could predict exactly where they would be later on. However, this is impossible to do in practice, and there was far more success when treating these as random forces!\n",
    "\n",
    "How do we generate a random value? We can use in-built functions, such as the ones from the __random__ and __numpy__ modules to generate random values. Numpy comes with the added functionality of creating vectors and matrices of random values in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random module: 2 0.5837918125826533\n",
      "numpy module: 2.166479591364384 -0.022214476588090237\n",
      "\n",
      "[-2.7526112   1.33383026  1.42281287 -0.55642387  0.15020474]\n",
      "[[1.54968805 3.74922869]\n",
      " [2.45683014 3.33119574]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Generating randomness with random!\n",
    "random_number = random.randint(0,4) # random integer between 0 and 4\n",
    "random_number2 = random.random() # random floating point value between 0 and 1\n",
    "print(\"random module:\", random_number, random_number2)\n",
    "\n",
    "# Generating randomness with numpy!\n",
    "random_number = np.random.uniform(0,4) # random value from uniform distribution between 0 and 4\n",
    "random_number2 = np.random.normal(0,1) # random value from standard normal distribution\n",
    "print(\"numpy module:\",random_number, random_number2)\n",
    "\n",
    "print()\n",
    "\n",
    "# Generating a random vector and matrix\n",
    "random_vector = np.random.normal(0,1,5) #Â 1x5 vector from standard normal distribution\n",
    "random_matrix = np.random.uniform(0,4,(2,2)) # 2x2 matrixrandom values from uniform distribution between 0 and 4\n",
    "print(random_vector)\n",
    "print(random_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An __experiment__ is a process whose outcome is uncertain, with all possible outcomes being its __sample space__. So in __descriptive statistics__, when we want measure a quantity, we can treat the quantity as a random variable. For example, let us consider the length of foxes in the UK. We know that there are scientific justfications for different foxes having different lengths, but if we treat fox height as a random variable and consider all differences to be random, we can extract very useful information, such as __how much the population varies, what is the most frequent height or what value is the most representative of this population__.\n",
    "\n",
    "A __population__ is described as a well-defined collection of objects we are interested in and a __sample__ is a subset of this population that we can get data from. We normally only have access to information from a sample of an entire population, and we use this data to estimate the properties of the population itself. Some of the simplest properties, are given in the equations below:\n",
    "\n",
    "$$ \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i}$$\n",
    "$$ s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}$$\n",
    "Where $\\bar{x}$ is the __sample mean__, s is the __sample standard deviation__ and $s^{2}$ the __sample variance__. These are __estimates__ of the population mean($\\mu$), standard deviation($\\sigma$) and variance($\\sigma^{2}$). The population mean, also known as __expected value__, is the value that is the most representative of the population. Another way to look at it is that given the sample size of n, __the expected value is the unique value that if you add it to itself n times, it is the same as the sum of all your samples__. By analysing the variance equation, we can see that the variance is the expected value of the square of the difference between the samples and the mean. Intuitively, this means that variace is a measure of the spread of the data. \n",
    "\n",
    "Let us consider the example of fox length with the given data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Python: mean = 0.67 , variance = 0.0045\n",
      "Numpy: mean = 0.67 , variance = 0.0045\n"
     ]
    }
   ],
   "source": [
    "# Rounds to a given decimal place\n",
    "def round_dp(number, decimalplaces):\n",
    "    factor = 10**decimalplaces\n",
    "    return round(number*factor)/factor\n",
    "    \n",
    "\n",
    "# UK fox heights(m)\n",
    "fox_lengths = [0.6,0.65,0.7,0.62,0.75,0.79,0.72,0.63,0.65,0.59] # sample size = 10\n",
    "\n",
    "# Calculating mean\n",
    "sample_mean = 0\n",
    "for sample in fox_lengths:\n",
    "    sample_mean += sample\n",
    "sample_mean = sample_mean/len(fox_lengths)\n",
    "\n",
    "#Calculating variance\n",
    "sample_variance = 0\n",
    "for sample in fox_lengths:\n",
    "    sample_variance = sample_variance + (sample - sample_mean)**2\n",
    "sample_variance = sample_variance/(len(fox_lengths)-1)\n",
    "\n",
    "print(\"Pure Python: mean =\",round_dp(sample_mean,4),\", variance =\",round_dp(sample_variance,4))\n",
    "\n",
    "# Calculating mean and variance in numpy\n",
    "fox_lengths = np.array(fox_lengths)\n",
    "sample_mean = np.mean(fox_lengths)\n",
    "sample_variance = np.var(fox_lengths,ddof=1)\n",
    "\n",
    "print(\"Numpy: mean =\",round_dp(sample_mean,4),\", variance =\",round_dp(sample_variance,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the the average fox length is 67cm and the length variance is 0.45cm. How can we interpret these results? What if I ask you, given these quantities, what is more likely: to find a fox with a length of 65 centimetres or one with the length of 5 metres? I think most people would agree that 65cm is far more likely, since it's closer to the sample mean and the variance of the population isn't large enough to lead to such large difference in length! \n",
    "\n",
    "So while we cannot predict what the exact length of a random fox we find in the wild, we can determine what heights are __more likely__ than others. But how do we quantify ___how likely___ a value is to take place?\n",
    "\n",
    "\n",
    "##  Introduction to Probability\n",
    "__Probability__ is a measure of how likely an outcome is to occur given all other possible outcomes and the given circumstances. Therefore, when dealing with random variables, we do not concern ourselves with __what will happen__, but instead with __the probability of given outcome(s), also known as events, occuring__. Probability is formally defined as:\n",
    "\n",
    "$$ \\text{Probability of an outcome} = \\frac{\\text{Number of wanted outcomes}}{\\text{Number of possible outcomes}} $$\n",
    "\n",
    "Intuitively, we know that the probability of something occurring has to be somewhere between 0, where the outcome _cannot_ occur, and 1, where the outcome _will_ occur. Given the events A and B, we can also define $P(A\\cup B)$ as the probability of ___either___ A or B taking place and $P(A\\cap B)$ as the probability of ___both___ A and B taking place. In terms of digital logic, __union__( $\\cap$ ) corresponds to the __or__ operator and __intersection__( $\\cup$ ) to the __and__ operator. Using these, we can define key properties of probability theory: \n",
    "\n",
    "$$1. P(A) = 1-P(A') $$\n",
    "$$2. \\sum_{i=1}^{N}P(A_{i}) = 1$$\n",
    "$$3. P(A\\cap B) = P(A) + P(B) - P(A\\cup B)$$\n",
    "\n",
    "A' is the __complement__ of A, and represents all events that are not included in A. The complement behaves like the __not__ operator, meaning the first property implies that the probability of an event taking place is equal to 1 minus the probability of the event __NOT__ taking place. If the probability of raining tomorrow is 0.8, then the probability that it will NOT rain tomorrow is 1 - 0.8 = 0.2. The second property implies that the sum of the probability of any of the possible events must equal to 1. This makes sense, since out of all possible outcomes, at least one must take place. For example, there is a probability of 1 that it will either rain or not tomorrow, and there is a probability of 1 that the length of a fox we find in the wild will be between 0 and infinity! The third property shows that the probability of events A or B occuring is given by the sum of the probability of A with the probability of B, subtracted by the probability of both events taking place. This is because the by adding the two individual probabilities, the intersection between them is accounted for twice. If two events A and B cannot occur simultaneously, they are known as __mutually exclusive__\n",
    "\n",
    "## Conditional Probability\n",
    "While quantities are random, probabilities are the measure of the likelihood of an event occuring. Under different circumstances, things may become more or less likely to happen. Given that it is sunny now, the probability of raining soon is smaller than if it was cloudy. This leads to the field of __conditional probability__, which is the probability of an event taking place ___given___ another event has occured. If we consider $P(A|B)$ to represent the probability of A occuring _given_ B has occured, we can define it with the equation below.\n",
    "\n",
    "$$P(A|B) = \\frac{P(A\\cap B)}{P(B)}$$\n",
    "\n",
    "While there is no formal proof for the equation above, we can make intuitive sense of it. There is a probability associated with the uncertainty of whether B occurs or not. So if we assume that A is __dependent__ on B, meaning the probability of A differs depending on whether B does or does not take place, knowing that B has already happened removes some uncertainty from whether A and B will occur propertional to how likely B was to occur in the first place.\n",
    "\n",
    "For example, let's say that I'm a birdwatcher following a rare yellow flamingo. My goal is to find the flamingo, and take a picture of him that we can publish on the _BirdsBirdsBirds Weekly_ magazine. For my success, both the event of finding it and getting a good picture need to take place. However, if I have already found the yellow flamingo, the probability of getting a good picture that day increases proportionally to how hard it was to find it in the first place!\n",
    "\n",
    "We can also establish the __law of total probability__, which is given by:\n",
    "\n",
    "$$P(B) = \\sum_{i=1}^{N}P(B|A_{i})P(A_{i}) $$\n",
    "\n",
    "Where N is the total number of prior outcomes, $A_{i}$ is the $i^{th}$ possible outcome prior to B, where all of the prior events are mutually exclusive (cannot both occur at the same time). If we analyse the equation more closely, we can see that the sum of the probabilities of $A_{i}$ and B both occuring for all possible prior events $A_{i}$ are the individual components that make up the probability of B taking place. \n",
    "\n",
    "For example, let's assume that if I don't find a flamingo, I can still get a picture of one from one of my sources. The probability of me getting a picture of a yellow flamingo is just the sum of the probability of _finding_ a flamingo and getting a picture and _not finding_ a flamingo and getting a picture.\n",
    "\n",
    "The law of total probability enables us to derive another incredibly useful theorem, known as __Bayes' Theorem__, which is used for revising predictions (updating probabilities) given additional evidence. Bayes Theorem is given as follows:\n",
    "\n",
    "$$P(A_{j}|B) = \\frac{P(A_{j}\\cap B)}{P(B)} = \\frac{P(B\\cap A_{j})}{P(B)} =  \\frac{P(B|A_{j})P(A_{j})}{\\sum_{i=1}^{N}P(B|A_{i})P(A_{i})} $$\n",
    "\n",
    "This equation dictates that the probability of that the prior outcome $A_{j}$ has taken place given that the event B has now taken place is given by the ratio between the probability that $A_{j}$ and B occured and the probability that B followed any possible event $A_{i}$.\n",
    "\n",
    "We will now look at a quick example to understand these concepts in practice, shown in the __tree diagram__ below, with the properties:\n",
    "- Imagine a type of bolt that can be produced either in factory A or factory B. They sometimes end up defective.\n",
    "- 60% of bolts are produced in A and 40% of bolts are produced in B\n",
    "- 2% of bolts produced in A are defective and 4% of bolts produced in B are defective\n",
    "\n",
    "<img src=\"tree.png\" alt=\"tree-diagram\"\n",
    "\ttitle=\"Tree diagram of the bolt production process\" width=\"750px\" height=\"500px\" />\n",
    "    \n",
    "Given the diagram above and the process, we can answer the following questions:\n",
    "1. What is the probability that the bolt is from factory A and it is defective? <br>\n",
    "$P(A\\cap D) = P(D|A)P(A) = 0.02\\cdot 0.6 = 0.012 $\n",
    "2. Using the law of total probability, what is the probability that a bolt is defective? <br>\n",
    "$P(D) = \\sum_{i=1}^{N}P(D|A_{i})P(A_{i}) = P(D|A)P(A) + P(D|B)P(B) = 0.02\\cdot 0.6 + 0.04\\cdot 0.4 = 0.028 $\n",
    "3. Using Bayes Theorem, what is the probability that a bolt is from factory B, given that it is defective? <br>\n",
    "$P(B|D) = \\frac{P(D|B)P(B)}{\\sum_{i=1}^{N}P(D|A_{i})P(A_{i})} = \\frac{0.04\\cdot 0.4}{0.04\\cdot 0.4 + 0.02\\cdot 0.6} = 0.57$\n",
    "4. Using Bayes Theorem, what is the probability that a bolt is from factory A, given that it is defective? <br>\n",
    "$P(A|D) = \\frac{P(D|A)P(A)}{\\sum_{i=1}^{N}P(D|A_{i})P(A_{i})} = \\frac{0.02\\cdot 0.6}{0.04\\cdot 0.4 + 0.02\\cdot 0.6} = 0.43$\n",
    "\n",
    "So with the laws of probability we've gone over, we were able to calculate the probability that a bolt is defective no matter where it came from originally and even what factories are responsible for the majority of the defective bolts, which in this case if factory B!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
