{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Dropout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "266c42006a0f4f55a92f4b922e921952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e5a8f90f8d004997b828552f74a996e9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b3b956d3e59447d0a5a5574913607763",
              "IPY_MODEL_a6e46d02ea704e9e91eeceb1cc2872e0"
            ]
          }
        },
        "e5a8f90f8d004997b828552f74a996e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3b956d3e59447d0a5a5574913607763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0275d70dac7447e6b0c96cf16d901129",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_214a3771a822493193bfba9a710dc0d7"
          }
        },
        "a6e46d02ea704e9e91eeceb1cc2872e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a167b1b90ce4b2a8844f80c42ef5651",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:20&lt;00:00, 1057905.49it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_746e09eabfed43a8a5528b3f4a56d41a"
          }
        },
        "0275d70dac7447e6b0c96cf16d901129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "214a3771a822493193bfba9a710dc0d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a167b1b90ce4b2a8844f80c42ef5651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "746e09eabfed43a8a5528b3f4a56d41a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d82a79e11c69401e8be66fa0405218c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_24a7c25769ea489d82a8894860a15459",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a4c18cb6115c4f898e7704c73e306323",
              "IPY_MODEL_1ebf45bbf8c243dea6b8d642d2f8b9f4"
            ]
          }
        },
        "24a7c25769ea489d82a8894860a15459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4c18cb6115c4f898e7704c73e306323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6b18cfb48b6f44aa921053b7a009c8cd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2c6b17d08cc4b69b0d8fb960ec876ed"
          }
        },
        "1ebf45bbf8c243dea6b8d642d2f8b9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2de18948ef104bf6834dcd47c780bd4b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:01&lt;00:00, 32435.98it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d80f597b6d5f401fb9afa3c4c64ce00e"
          }
        },
        "6b18cfb48b6f44aa921053b7a009c8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2c6b17d08cc4b69b0d8fb960ec876ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2de18948ef104bf6834dcd47c780bd4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d80f597b6d5f401fb9afa3c4c64ce00e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "366ea980a4674d08b02ea87629bf3da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c74d36e0f9d348f38d4e2cf8d1e3381a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_128c1d95cffe4a309cf156d4ba9854cb",
              "IPY_MODEL_f76e11e84b424edf8c67c2955e1ae394"
            ]
          }
        },
        "c74d36e0f9d348f38d4e2cf8d1e3381a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "128c1d95cffe4a309cf156d4ba9854cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fab65c2c90294c3bb2f9df8ce634f0e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9d99879b65442158737e2146d8a8d35"
          }
        },
        "f76e11e84b424edf8c67c2955e1ae394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_13735e1ad6c54b0090187fa79ed342c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:00&lt;00:00, 2246854.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_485fdcb89a0047ada59baafa8546bc39"
          }
        },
        "fab65c2c90294c3bb2f9df8ce634f0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9d99879b65442158737e2146d8a8d35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13735e1ad6c54b0090187fa79ed342c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "485fdcb89a0047ada59baafa8546bc39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa37f8b445844f1296bdb8f5a7310dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_55ee560a12674fa29001196857938f34",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fc2eb2eea31d445b915147e8cba1a55e",
              "IPY_MODEL_1ce5eedfede74710948e7d20c36709cb"
            ]
          }
        },
        "55ee560a12674fa29001196857938f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc2eb2eea31d445b915147e8cba1a55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7fffacedcfb14213891aba02b2dc2ccb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_16dc990ddcb0463eabf30128119eb0f0"
          }
        },
        "1ce5eedfede74710948e7d20c36709cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0bc527c70d04f28b5c9d80e8187f82b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 15627.30it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a53f08e609c24ae1b2d4d0ae43fd8a4b"
          }
        },
        "7fffacedcfb14213891aba02b2dc2ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "16dc990ddcb0463eabf30128119eb0f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0bc527c70d04f28b5c9d80e8187f82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a53f08e609c24ae1b2d4d0ae43fd8a4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKyXAcU1By3l",
        "colab_type": "text"
      },
      "source": [
        "# Dropout Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibIqmDynBy3m",
        "colab_type": "text"
      },
      "source": [
        "### **Prerequisites**\n",
        "* Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffx3HatiBy3n",
        "colab_type": "text"
      },
      "source": [
        "***What will we cover in this notebook?***\n",
        "\n",
        "* Explain dropout in nn\n",
        "* Implementation in PyTorch\n",
        "-----\n",
        "1. What is dropout and why do we need it? What was the idea behind it?\n",
        "2. How is dropout similar to ensembling a large variety of neural network architectures?\n",
        "3. Implementation in PyTorch on a basic neural network\n",
        "4. Visualising the difference between the non-dropout and dropout implemented models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nijNnGn9By3o",
        "colab_type": "text"
      },
      "source": [
        "### 1. What was the idea behind Dropout?\n",
        "   * explain why we need dropout? Which problem leads us to do dropout\n",
        "   * show a dropout example on a basic neural network\n",
        "   * explain dropout parameters\n",
        "   * important aspects of dropout\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4dW0KQQEy_c",
        "colab_type": "text"
      },
      "source": [
        "Let's assume that we want to have biceps and do sports for 4 days in a week.\n",
        "In the first week,v "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI90VAZcBy3p",
        "colab_type": "text"
      },
      "source": [
        "### 2. Similarity between dropout and ensembles methods\n",
        "   * explain what the ensemble method is\n",
        "   * list the similary between them\n",
        "   * what makes dropout unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHAOhDYsBy3p",
        "colab_type": "text"
      },
      "source": [
        "![ensemble.png](attachment:ensemble.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66sQs4SKBy3q",
        "colab_type": "text"
      },
      "source": [
        "### 3. Implementation in PyTorch on a basic neural network \n",
        "* Copy the code in Neural Network Repo\n",
        "* Implement dropout method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsr9zfC-By3r",
        "colab_type": "text"
      },
      "source": [
        "### 4. Visualizing the difference between a non-dropout and dropout implemented nn\n",
        "* Copy the nn repo model performance metrics etc\n",
        "* Explain the metrics used\n",
        "* Evaluate the dropout model\n",
        "* Compare training curves\n",
        "* Compare model performance metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Ty3EwfBy3s",
        "colab_type": "text"
      },
      "source": [
        "### **Non-dropout implementation can be found below:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xRIWITEBy3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927,
          "referenced_widgets": [
            "266c42006a0f4f55a92f4b922e921952",
            "e5a8f90f8d004997b828552f74a996e9",
            "b3b956d3e59447d0a5a5574913607763",
            "a6e46d02ea704e9e91eeceb1cc2872e0",
            "0275d70dac7447e6b0c96cf16d901129",
            "214a3771a822493193bfba9a710dc0d7",
            "2a167b1b90ce4b2a8844f80c42ef5651",
            "746e09eabfed43a8a5528b3f4a56d41a",
            "d82a79e11c69401e8be66fa0405218c4",
            "24a7c25769ea489d82a8894860a15459",
            "a4c18cb6115c4f898e7704c73e306323",
            "1ebf45bbf8c243dea6b8d642d2f8b9f4",
            "6b18cfb48b6f44aa921053b7a009c8cd",
            "a2c6b17d08cc4b69b0d8fb960ec876ed",
            "2de18948ef104bf6834dcd47c780bd4b",
            "d80f597b6d5f401fb9afa3c4c64ce00e",
            "366ea980a4674d08b02ea87629bf3da7",
            "c74d36e0f9d348f38d4e2cf8d1e3381a",
            "128c1d95cffe4a309cf156d4ba9854cb",
            "f76e11e84b424edf8c67c2955e1ae394",
            "fab65c2c90294c3bb2f9df8ce634f0e8",
            "f9d99879b65442158737e2146d8a8d35",
            "13735e1ad6c54b0090187fa79ed342c1",
            "485fdcb89a0047ada59baafa8546bc39",
            "aa37f8b445844f1296bdb8f5a7310dc3",
            "55ee560a12674fa29001196857938f34",
            "fc2eb2eea31d445b915147e8cba1a55e",
            "1ce5eedfede74710948e7d20c36709cb",
            "7fffacedcfb14213891aba02b2dc2ccb",
            "16dc990ddcb0463eabf30128119eb0f0",
            "c0bc527c70d04f28b5c9d80e8187f82b",
            "a53f08e609c24ae1b2d4d0ae43fd8a4b"
          ]
        },
        "outputId": "bce87115-f757-4937-adc3-5ab2da9fb941"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "crop_size = 22\n",
        "\n",
        "traintransforms = []\n",
        "traintransforms.append(transforms.CenterCrop(crop_size))\n",
        "traintransforms.append(transforms.ToTensor())\n",
        "traintransforms = transforms.Compose(traintransforms)\n",
        "\n",
        "# GET THE TRAINING DATASET\n",
        "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
        "                            transform=traintransforms,          # transform the data from a PIL image to a tensor\n",
        "                            train=True,                               # is this training data?\n",
        "                            download=True                             # should i download it if it's not already here?\n",
        "                           )\n",
        "\n",
        "testtransforms = []\n",
        "testtransforms.append(transforms.RandomCrop(crop_size))\n",
        "testtransforms.append(transforms.ToTensor())\n",
        "testtransforms = transforms.Compose(testtransforms)\n",
        "\n",
        "# GET THE TEST DATASET\n",
        "test_data = datasets.MNIST(root='MNIST-data',\n",
        "                           transform=testtransforms,\n",
        "                           train=False,\n",
        "                          )\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
        "x = train_data[np.random.randint(0, 300)][0]    # get the first example\n",
        "plt.imshow(x[0].numpy())\n",
        "plt.show()\n",
        "x = test_data[np.random.randint(0, 300)][0]    # get the first example\n",
        "plt.imshow(x[0].numpy())\n",
        "plt.show()\n",
        "\n",
        "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
        "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "266c42006a0f4f55a92f4b922e921952",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST-data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d82a79e11c69401e8be66fa0405218c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "366ea980a4674d08b02ea87629bf3da7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST-data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa37f8b445844f1296bdb8f5a7310dc3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD4CAYAAADl7fPiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATTklEQVR4nO3df7BcZX3H8feH/KIJBAmUCAkGlBgbVIITQBAR5GdSBtBaDLQarU78AS1YrUXtiMrY0XGQFkEwYAo4gFgFjDUNpKkddEZifhBIAgkJaWhyiYkSJCIQuLnf/rEnzn1udrlP9uzuPTd8XjOZu3vOd8951sQP5+w+9/kqIjAz22WfgR6AmVWLQ8HMEg4FM0s4FMws4VAws8TQgR5APcM1IvZl1EAPw2yv9SJ/4KXYoXr7KhkK+zKKE3T6QA/DbK+1KBY23Ffq9kHSOZLWSFon6Yo6+0dIuqvYv0jSEWXOZ2bt13QoSBoCXA9MAyYDF0ma3KfsI8AzEXEUcA3w9WbPZ2adUeZK4XhgXUSsj4iXgO8D5/epOR+4tXj8Q+B0SXXvY8ysGsqEwjhgY6/nm4ptdWsioht4Fjio3sEkzZK0RNKSl9lRYlhmVkZlvpKMiNkRMTUipg5jxEAPx+xVq0wodAGH93o+vthWt0bSUOAA4OkS5zSzNisTCouBiZKOlDQcmAHM7VMzF5hZPH4f8N/hX8s0q7Sm5ylERLekS4H7gCHAnIhYJekrwJKImAt8F/iepHXANmrBYWYVpir+h3u0xoQnL5m1z6JYyPbYVvebwMp80Ghm1eBQMLOEQ8HMEg4FM0s4FMws4VAws4RDwcwSDgUzSzgUzCxRyeXYrP22/O1JWXVvvXhlVt3m5w/Iqpv3pnuz6jZ1v5BV9+77P5VVN3L9sKy6CXdv7bdm55p1WccarHylYGYJh4KZJRwKZpZwKJhZwqFgZgmHgpklyvR9OFzSzyQ9KmmVpMvq1Jwq6VlJy4s/Xyw3XDNrtzLzFLqBT0fEMkn7A0slLYiIR/vU/Twizi1xHjProKavFCJic0QsKx7/HniM3fs+mNkg05IZjUWPyGOBRXV2nyjpYeAp4DMRsarBMWYBswD2ZWQrhvWqNOSoI7PqFl/xray6HnrKDKfO8fKMH/onWXWrp3+7+cHUc2n/JcfesNudcl1H/tuGrLrurqey6jqldChI2g/4EXB5RGzvs3sZMCEinpM0HbgXmFjvOBExG5gNtYVby47LzJpTtuv0MGqBcHtE3N13f0Rsj4jnisfzgGGSDi5zTjNrrzLfPohaX4fHIuKbDWpeu6uhrKTji/O5Q5RZhZW5fXgH8AFghaTlxbbPA68DiIgbqXWF+oSkbuAFYIY7RJlVW5kOUb8AXrGtfERcB1zX7DnMrPM8o9HMEg4FM0s4FMws4VAws4TXaBxgO6Ydl1XXdVreX9VXL7ijzHAq46SHLsqq+8KkeVl100Y+U2Y4iYc/kTcb9MQtGdMjgYNuqtaMRl8pmFnCoWBmCYeCmSUcCmaWcCiYWcKhYGYJh4KZJRwKZpZwKJhZwjMa2+ilBRP6rblt0jVZxxo7ZETZ4fRR7f8ejDn38ay6m8adklX3D984JKtu5btuyqrL8Zd/919ZdQ8sPDqrrnv9hhKjyVftfxlm1nGlQ0HSBkkrimYvS+rsl6RrJa2T9Iikt5U9p5m1T6tuH06LiN822DeN2grOE4ETgBuKn2ZWQZ24fTgfuC1qHgReI+nQDpzXzJrQilAI4H5JS4uGLn2NAzb2er6JOp2kJM2StETSkpfZ0YJhmVkzWnH7cHJEdEk6BFggaXVEPLCnB3EzGLNqKH2lEBFdxc+twD3A8X1KuoDDez0fX2wzswoq2yFqVNFxGkmjgLOAlX3K5gIfLL6FeDvwbERsLnNeM2ufsrcPY4F7iiZQQ4E7ImK+pI/DHxvCzAOmA+uA54EPlzxn2wwZPTqrbs2XJ2fVPXH0jf3WvBx5jVS37Hwhq27ZjrxJOp+9c2ZW3TvPfiSr7tvj8+4Yp6++IKtun+RjqMZym7O+/uK8uuPu/WC/NQ8dd3vWsf5+zOqsuh9ee2xW3Zhzs8pKKxUKEbEeOKbO9ht7PQ7gkjLnMbPO8YxGM0s4FMws4VAws4RDwcwSDgUzSzgUzCzhUDCzhEPBzBJejq2XLTPylsV69MJrs+pejv4zd+aGM7KOtfmqo7Lqhs9fnFU3gV9m1T31ncOy6iZ97pOZdY9m1fVkVbXeYVf1/3fWMzfv9/V6BuxdlOMrBTNLOBTMLOFQMLOEQ8HMEg4FM0s4FMws0XQoSJpU9HrY9We7pMv71Jwq6dleNV8sP2Qza6em5ylExBpgCoCkIdTWXbynTunPI6JDa8aYWVmtun04HXgiIp5s0fHMbIC0akbjDODOBvtOlPQw8BTwmYhYVa+o6BkxC2BfRrZoWDVDx+XNyjv6w3WH1rRPbuq/+emzF+at0Ti8K2+mYqvlroE48dK8uqrP8Yulrf03kOP6o/PWfPzyhPdm1XU/mbe+ZSOt6CU5HDgP+Pc6u5cBEyLiGOBbwL2NjhMRsyNiakRMHUarOyybWa5W3D5MA5ZFxJa+OyJie0Q8VzyeBwyTdHALzmlmbdKKULiIBrcOkl6rYv13SccX53u6Bec0szYp9ZlC0QDmTOBjvbb17vnwPuATkrqBF4AZxZLvZlZRZfs+/AE4qM+23j0frgOuK3MOM+ssz2g0s4RDwcwSDgUzSzgUzCzxqlij8X8/dERW3d2v+9eWnrdrev+TsHY+nTcT0PZexwzPq1t9+bisuqM+NcAzGs1s7+JQMLOEQ8HMEg4FM0s4FMws4VAws4RDwcwSDgUzSzgUzCzxqpjRuN9Jv8mq2yczI/+v+4Wsup1Pb8uqs8HljfM+nlX3+PQb+y/aAz2jdrb0eI34SsHMElmhIGmOpK2SVvbaNkbSAklri58HNnjtzKJmraSZrRq4mbVH7pXCLcA5fbZdASyMiInAwuJ5QtIY4ErgBOB44MpG4WFm1ZAVChHxAND3Bvl84Nbi8a3ABXVeejawICK2RcQzwAJ2Dxczq5AyHzSOjYjNxeNfA2Pr1IwDev8e56Zi227a2QzGzPK15IPGYoXmUqs0uxmMWTWUCYUtkg4FKH5urVPTBRze6/n4YpuZVVSZUJgL7Po2YSbw4zo19wFnSTqw+IDxrGKbmVVU7leSdwK/BCZJ2iTpI8DXgDMlrQXOKJ4jaaqkmwEiYhtwFbC4+POVYpuZVVTWB40RcVGDXafXqV0CfLTX8znAnKZG14/cbtL/9MafZtX1ZPZEPmP+p7Lq3sjAdIq29ho3P+8Cu2d6i3tsd6i3mmc0mlnCoWBmCYeCmSUcCmaWcCiYWcKhYGYJh4KZJRwKZpZwKJhZYlCv0Rj75f2K9dkjn23peV//gxbPVLNBZfTKpwd6CG3lKwUzSzgUzCzhUDCzhEPBzBIOBTNLOBTMLNFvKDRoBPMNSaslPSLpHkmvafDaDZJWSFouaUkrB25m7ZFzpXALu/dqWAC8OSLeCjwOfO4VXn9aREyJiKnNDdHMOqnfUKjXCCYi7o+I7uLpg9RWaTazvUArZjT+DXBXg30B3C8pgO9ExOxGBxlMzWBGbPxdVl1negTbq8Wkm1/Mqiu7lGOpUJD0BaAbuL1ByckR0SXpEGCBpNXFlcduisCYDTBaYzq0RKWZ9dX0tw+SPgScC/xV0SFqNxHRVfzcCtxDrcmsmVVYU6Eg6Rzgs8B5EfF8g5pRkvbf9ZhaI5iV9WrNrDpyvpKs1wjmOmB/arcEyyXdWNQeJmle8dKxwC8kPQz8CvhpRMxvy7sws5bp9zOFBo1gvtug9ilgevF4PXBMqdGZWcd5RqOZJRwKZpZwKJhZwqFgZolBvUbjY585MKtunxZn3+azxmbVHfL4Ey09r1XDui+NyqrL/Xd30kONmrqnxixekVVXlq8UzCzhUDCzhEPBzBIOBTNLOBTMLOFQMLOEQ8HMEg4FM0s4FMwsMahnNI6bn5dpPdNb3CX6zG3910Bt1Qnb66w6ZU5WXQ+Dszt5s30fviSpq1hgZbmk6Q1ee46kNZLWSbqilQM3s/Zotu8DwDVFP4cpETGv705JQ4DrgWnAZOAiSZPLDNbM2q+pvg+ZjgfWRcT6iHgJ+D5wfhPHMbMOKvNB46VF27g5kur9uuI4YGOv55uKbWZWYc2Gwg3AG4ApwGbg6rIDkTRL0hJJS15mR9nDmVmTmgqFiNgSETsjoge4ifr9HLqAw3s9H19sa3TM2RExNSKmDmNEM8MysxZotu/Dob2evof6/RwWAxMlHSlpODADmNvM+cysc/qdp1D0fTgVOFjSJuBK4FRJU6i1rdsAfKyoPQy4OSKmR0S3pEuB+4AhwJyIWNWWd2FmLdO2vg/F83nAbl9Xmll1DeoZjQc8uLH/IuA/n89by3HayGey6h6c+r2suvM4LqvOquOJq9+eUbW0tSf90UGtPV5J/t0HM0s4FMws4VAws4RDwcwSDgUzSzgUzCzhUDCzhEPBzBKDevJSd9dTWXX//Pi0rLppU+4oM5zdnPfo0/3W/OQDp2QdK5Z6hngZeZOSYOn7r8moGp51rKP/Z1ZW3cQ7H8qq69Tibr5SMLOEQ8HMEg4FM0s4FMws4VAws4RDwcwSOSsvzQHOBbZGxJuLbXcBk4qS1wC/i4gpdV67Afg9sBPojoipLRq3mbVJzjyFW6g1QLtt14aIeP+ux5KuBp59hdefFhG/bXaAZtZZOcuxPSDpiHr7JAm4EHh3a4dlZgOl7IzGdwJbImJtg/0B3C8pgO9ExOxGB5I0C5gFsC8jSw4rdfBl3Vl1/zEvb1ms80blLdv28QOe7Ldm1tzb+q0BeNMPLsmqm3Rj3kXZzjXrsupabchBY7Lqdr4hr2/QX9y6MKvuw6Ovz6rLma14/wujso501DdfyqrrefHFrLpOKRsKFwF3vsL+kyOiS9IhwAJJq4s2dLspAmM2wGiNiZLjMrMmNf3tg6ShwHuBuxrVRERX8XMrcA/1m8aYWYWU+UryDGB1RGyqt1PSKEn773oMnEX9pjFmViH9hkLRDOaXwCRJmyR9pNg1gz63DpIOk7Srz8NY4BeSHgZ+Bfw0Iua3buhm1g7NNoMhIj5UZ9sfm8FExHrgmJLjM7MO84xGM0s4FMws4VAws4RDwcwSiqjePKHRGhMn6PSOn3f910/Mqlv519dm1e2Tkbk9HVt5L/XO5RcPyHn/fHzeWpOfP3hFVl2r//f74tb+mwI//NGjs45V5XU1F8VCtsc21dvnKwUzSzgUzCzhUDCzhEPBzBIOBTNLOBTMLOFQMLOEQ8HMEg4FM0t4RmMvQ8cdllX3/Fvy1g987tJXWuS65guT5vVbAzBtZN66kLlyZlvCwM24zB3f0h15x7v4J5lrXP7j8n5rqramYjM8o9HMsuWsvHS4pJ9JelTSKkmXFdvHSFogaW3x88AGr59Z1KyVNLPVb8DMWivnSqEb+HRETAbeDlwiaTJwBbAwIiYCC4vnCUljgCuBE6gt2nplo/Aws2roNxQiYnNELCse/x54DBgHnA/cWpTdClxQ5+VnAwsiYltEPAMsAM5pxcDNrD32qO9D0SnqWGARMDYiNhe7fk1toda+xgEbez3fVGyrd+y2NYMxs3zZHzRK2g/4EXB5RGzvvS9qX2GU+hojImZHxNSImDqMEWUOZWYlZIWCpGHUAuH2iLi72LxF0qHF/kOBrXVe2gUc3uv5+GKbmVVUzrcPAr4LPBYR3+y1ay6w69uEmcCP67z8PuAsSQcWHzCeVWwzs4rKuVJ4B/AB4N2Slhd/pgNfA86UtJZat6ivAUiaKulmgIjYBlwFLC7+fKXYZmYV5RmNA+zXl52UVbd98stZde94S6MG4KlFG47Iqlv5rpuy6o5f/MGsuu1b9suq+7Or8/7boR15nZ27n9zYf9GriGc0mlk2h4KZJRwKZpZwKJhZwqFgZgmHgpklHApmlnAomFnCoWBmiUrOaJT0G+DJPpsPBn47AMNpJb+H6tgb3keZ9zAhIv603o5KhkI9kpZExNSBHkcZfg/VsTe8j3a9B98+mFnCoWBmicEUCrMHegAt4PdQHXvD+2jLexg0nymYWWcMpisFM+sAh4KZJSofCpLOkbRG0jpJuzWcGSwkbZC0oljObslAjyeHpDmStkpa2WtbVmewKmnwPr4kqavPEoOVVbZT256odChIGgJcD0wDJgMXFd2pBqvTImLKIPp+/BZ2b97Tb2ewCrqF+k2Irin+PqZERF6n34HTdKe2PVXpUKDWam5dRKyPiJeA71PrTGUdEBEPAH0XS8zpDFYpDd7HoFKyU9seqXooZHeYGgQCuF/S0qIb1mCV0xlssLhU0iPF7UXlb4N2aaJT2x6peijsTU6OiLdRuxW6RNIpAz2gslrRGWwA3QC8AZgCbAauHtjh5Gl3pzaofijsNR2mIqKr+LkVuIfardFglNMZrPIiYktE7IyIHuAmBsHfR4lObXuk6qGwGJgo6UhJw4EZ1DpTDSqSRknaf9djap2yVr7yqyorpzNY5e36P1LhPVT876Nkp7Y9O1fVZzQWXxX9CzAEmBMRXx3gIe0xSa+ndnUAtU7fdwyG9yHpTuBUar+iuwW4ErgX+AHwOmq/3n5h1bt+NXgfp1K7dQhgA/CxXvfmlSPpZODnwAqgp9j8eWqfK7T076PyoWBmnVX12wcz6zCHgpklHApmlnAomFnCoWBmCYeCmSUcCmaW+H9k7sbZOvyBugAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJs8IsSbIc5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "# MAKE TRAINING DATALOADER\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# MAKE VALIDATION DATALOADER\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_data,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# MAKE TEST DATALOADER\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89VVbzpsIhvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F \n",
        "\n",
        "class NeuralNetworkClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()    # initialise parent module\n",
        "        self.layer1 = torch.nn.Linear(crop_size*crop_size, 225)\n",
        "        self.layer2 = torch.nn.Linear(225, 100)\n",
        "        self.layer3 = torch.nn.Linear(100, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, crop_size*crop_size)\n",
        "        x = self.layer1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer3(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "def get_n_params(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return n_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVGuZzejIlBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a12de18d-fdae-45e2-9cfe-173847b73d75"
      },
      "source": [
        "learning_rate = 0.0005\n",
        "myNeuralNetwork = NeuralNetworkClass()\n",
        "print('Number of parameters in model:', get_n_params(myNeuralNetwork))\n",
        "\n",
        "# CREATE OUR OPTIMISER\n",
        "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
        "    myNeuralNetwork.parameters(),          # what should it optimise?\n",
        "    lr=learning_rate                       # using what learning rate?\n",
        ")\n",
        "\n",
        "# CREATE OUR CRITERION\n",
        "criterion = torch.nn.CrossEntropyLoss()             # callable class that compares our predictions to our labels and returns our loss\n",
        "\n",
        "# SET UP TRAINING VISUALISATION\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()                            # we will use this to show our models performance on a graph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters in model: 132735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZN2tvdpIn7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3486be81-391b-4e6c-9ff1-5c1db6fac4a9"
      },
      "source": [
        "# TRAINING LOOP\n",
        "def train(model, epochs):\n",
        "    model.train()                                  # put the model into training mode (more on this later)\n",
        "    for epoch in range(epochs):\n",
        "        for idx, minibatch in enumerate(train_loader):\n",
        "            inputs, labels = minibatch\n",
        "            prediction = model(inputs)             # pass the data forward through the model\n",
        "            loss = criterion(prediction, labels)   # compute the loss\n",
        "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
        "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
        "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
        "            optimiser.step()                       # update the model's parameters\n",
        "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
        "            \n",
        "            \n",
        "train(myNeuralNetwork, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tBatch: 0 \tLoss: tensor(2.3032, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 1 \tLoss: tensor(2.3012, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 2 \tLoss: tensor(2.2994, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 3 \tLoss: tensor(2.2990, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 4 \tLoss: tensor(2.2969, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 5 \tLoss: tensor(2.2955, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 6 \tLoss: tensor(2.2927, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 7 \tLoss: tensor(2.2915, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 8 \tLoss: tensor(2.2870, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 9 \tLoss: tensor(2.2849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 10 \tLoss: tensor(2.2828, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 11 \tLoss: tensor(2.2805, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 12 \tLoss: tensor(2.2768, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 13 \tLoss: tensor(2.2744, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 14 \tLoss: tensor(2.2650, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 15 \tLoss: tensor(2.2556, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 16 \tLoss: tensor(2.2568, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 17 \tLoss: tensor(2.2505, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 18 \tLoss: tensor(2.2411, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 19 \tLoss: tensor(2.2332, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 20 \tLoss: tensor(2.2273, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 21 \tLoss: tensor(2.2166, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 22 \tLoss: tensor(2.2226, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 23 \tLoss: tensor(2.2065, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 24 \tLoss: tensor(2.1771, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 25 \tLoss: tensor(2.1686, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 26 \tLoss: tensor(2.1606, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 27 \tLoss: tensor(2.1510, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 28 \tLoss: tensor(2.1393, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 29 \tLoss: tensor(2.1142, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 30 \tLoss: tensor(2.1143, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 31 \tLoss: tensor(2.1110, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 32 \tLoss: tensor(2.1065, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 33 \tLoss: tensor(2.1038, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 34 \tLoss: tensor(2.0034, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 35 \tLoss: tensor(2.0397, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 36 \tLoss: tensor(2.0166, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 37 \tLoss: tensor(2.0296, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 38 \tLoss: tensor(2.0405, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 39 \tLoss: tensor(2.0137, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 40 \tLoss: tensor(1.9979, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 41 \tLoss: tensor(1.9506, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 42 \tLoss: tensor(1.9677, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 43 \tLoss: tensor(1.8414, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 44 \tLoss: tensor(1.9662, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 45 \tLoss: tensor(1.9612, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 46 \tLoss: tensor(1.9168, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 47 \tLoss: tensor(1.8847, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 48 \tLoss: tensor(1.8752, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 49 \tLoss: tensor(1.8419, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 50 \tLoss: tensor(1.8480, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 51 \tLoss: tensor(1.8589, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 52 \tLoss: tensor(1.8767, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 53 \tLoss: tensor(1.8542, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 54 \tLoss: tensor(1.8724, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 55 \tLoss: tensor(1.8449, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 56 \tLoss: tensor(1.8397, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 57 \tLoss: tensor(1.8163, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 58 \tLoss: tensor(1.8728, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 59 \tLoss: tensor(1.8382, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 60 \tLoss: tensor(1.8039, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 61 \tLoss: tensor(1.7971, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 62 \tLoss: tensor(1.8054, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 63 \tLoss: tensor(1.8220, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 64 \tLoss: tensor(1.7967, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 65 \tLoss: tensor(1.8270, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 66 \tLoss: tensor(1.7543, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 67 \tLoss: tensor(1.7375, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 68 \tLoss: tensor(1.8273, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 69 \tLoss: tensor(1.8164, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 70 \tLoss: tensor(1.7722, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 71 \tLoss: tensor(1.7096, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 72 \tLoss: tensor(1.7257, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 73 \tLoss: tensor(1.7240, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 74 \tLoss: tensor(1.6951, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 75 \tLoss: tensor(1.7455, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 76 \tLoss: tensor(1.7478, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 77 \tLoss: tensor(1.7339, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 78 \tLoss: tensor(1.7756, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 79 \tLoss: tensor(1.7259, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 80 \tLoss: tensor(1.7696, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 81 \tLoss: tensor(1.7744, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 82 \tLoss: tensor(1.7495, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 83 \tLoss: tensor(1.7382, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 84 \tLoss: tensor(1.7214, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 85 \tLoss: tensor(1.6639, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 86 \tLoss: tensor(1.7318, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 87 \tLoss: tensor(1.7064, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 88 \tLoss: tensor(1.6769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 89 \tLoss: tensor(1.7261, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 90 \tLoss: tensor(1.6925, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 91 \tLoss: tensor(1.7221, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 92 \tLoss: tensor(1.6951, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 93 \tLoss: tensor(1.6595, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 94 \tLoss: tensor(1.7005, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 95 \tLoss: tensor(1.6720, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 96 \tLoss: tensor(1.7353, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 97 \tLoss: tensor(1.6360, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 98 \tLoss: tensor(1.7152, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 99 \tLoss: tensor(1.6972, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 100 \tLoss: tensor(1.7222, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 101 \tLoss: tensor(1.6812, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 102 \tLoss: tensor(1.6911, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 103 \tLoss: tensor(1.6730, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 104 \tLoss: tensor(1.6742, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 105 \tLoss: tensor(1.6582, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 106 \tLoss: tensor(1.6494, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 107 \tLoss: tensor(1.6395, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 108 \tLoss: tensor(1.6416, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 109 \tLoss: tensor(1.6405, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 110 \tLoss: tensor(1.6749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 111 \tLoss: tensor(1.6599, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 112 \tLoss: tensor(1.6883, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 113 \tLoss: tensor(1.6252, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 114 \tLoss: tensor(1.7035, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 115 \tLoss: tensor(1.6849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 116 \tLoss: tensor(1.6638, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 117 \tLoss: tensor(1.6513, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 118 \tLoss: tensor(1.6450, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 119 \tLoss: tensor(1.6493, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 120 \tLoss: tensor(1.6439, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 121 \tLoss: tensor(1.6968, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 122 \tLoss: tensor(1.7074, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 123 \tLoss: tensor(1.6235, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 124 \tLoss: tensor(1.6736, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 125 \tLoss: tensor(1.6416, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 126 \tLoss: tensor(1.6123, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 127 \tLoss: tensor(1.6908, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 128 \tLoss: tensor(1.6529, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 129 \tLoss: tensor(1.5955, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 130 \tLoss: tensor(1.6490, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 131 \tLoss: tensor(1.6549, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 132 \tLoss: tensor(1.6682, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 133 \tLoss: tensor(1.6328, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 134 \tLoss: tensor(1.6060, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 135 \tLoss: tensor(1.6466, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 136 \tLoss: tensor(1.6100, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 137 \tLoss: tensor(1.6101, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 138 \tLoss: tensor(1.6165, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 139 \tLoss: tensor(1.5705, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 140 \tLoss: tensor(1.5909, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 141 \tLoss: tensor(1.5985, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 142 \tLoss: tensor(1.6241, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 143 \tLoss: tensor(1.6186, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 144 \tLoss: tensor(1.6692, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 145 \tLoss: tensor(1.5880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 146 \tLoss: tensor(1.6352, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 147 \tLoss: tensor(1.6509, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 148 \tLoss: tensor(1.6189, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 149 \tLoss: tensor(1.6009, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 150 \tLoss: tensor(1.5880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 151 \tLoss: tensor(1.6255, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 152 \tLoss: tensor(1.5971, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 153 \tLoss: tensor(1.5949, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 154 \tLoss: tensor(1.5923, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 155 \tLoss: tensor(1.5899, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 156 \tLoss: tensor(1.6425, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 157 \tLoss: tensor(1.5635, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 158 \tLoss: tensor(1.6085, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 159 \tLoss: tensor(1.6009, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 160 \tLoss: tensor(1.6278, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 161 \tLoss: tensor(1.6119, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 162 \tLoss: tensor(1.5993, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 163 \tLoss: tensor(1.5884, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 164 \tLoss: tensor(1.6233, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 165 \tLoss: tensor(1.5969, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 166 \tLoss: tensor(1.5809, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 167 \tLoss: tensor(1.5945, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 168 \tLoss: tensor(1.6034, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 169 \tLoss: tensor(1.5995, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 170 \tLoss: tensor(1.5864, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 171 \tLoss: tensor(1.5641, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 172 \tLoss: tensor(1.6022, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 173 \tLoss: tensor(1.6179, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 174 \tLoss: tensor(1.6038, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 175 \tLoss: tensor(1.5505, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 176 \tLoss: tensor(1.5796, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 177 \tLoss: tensor(1.6320, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 178 \tLoss: tensor(1.6389, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 179 \tLoss: tensor(1.6361, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 180 \tLoss: tensor(1.5784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 181 \tLoss: tensor(1.6187, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 182 \tLoss: tensor(1.5801, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 183 \tLoss: tensor(1.5990, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 184 \tLoss: tensor(1.5882, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 185 \tLoss: tensor(1.5849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 186 \tLoss: tensor(1.5826, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 187 \tLoss: tensor(1.5674, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 188 \tLoss: tensor(1.6084, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 189 \tLoss: tensor(1.6032, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 190 \tLoss: tensor(1.6340, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 191 \tLoss: tensor(1.6108, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 192 \tLoss: tensor(1.6366, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 193 \tLoss: tensor(1.5944, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 194 \tLoss: tensor(1.6003, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 195 \tLoss: tensor(1.5933, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 196 \tLoss: tensor(1.5509, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 197 \tLoss: tensor(1.5598, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 198 \tLoss: tensor(1.5876, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 199 \tLoss: tensor(1.5746, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 200 \tLoss: tensor(1.5891, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 201 \tLoss: tensor(1.5920, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 202 \tLoss: tensor(1.5961, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 203 \tLoss: tensor(1.6346, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 204 \tLoss: tensor(1.5769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 205 \tLoss: tensor(1.5990, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 206 \tLoss: tensor(1.5686, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 207 \tLoss: tensor(1.6172, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 208 \tLoss: tensor(1.6151, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 209 \tLoss: tensor(1.5649, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 210 \tLoss: tensor(1.5805, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 211 \tLoss: tensor(1.5868, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 212 \tLoss: tensor(1.5766, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 213 \tLoss: tensor(1.5901, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 214 \tLoss: tensor(1.5926, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 215 \tLoss: tensor(1.5751, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 216 \tLoss: tensor(1.5784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 217 \tLoss: tensor(1.5765, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 218 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 219 \tLoss: tensor(1.6007, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 220 \tLoss: tensor(1.5708, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 221 \tLoss: tensor(1.5562, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 222 \tLoss: tensor(1.6393, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 223 \tLoss: tensor(1.5777, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 224 \tLoss: tensor(1.5445, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 225 \tLoss: tensor(1.6308, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 226 \tLoss: tensor(1.5634, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 227 \tLoss: tensor(1.6052, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 228 \tLoss: tensor(1.5915, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 229 \tLoss: tensor(1.5560, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 230 \tLoss: tensor(1.5608, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 231 \tLoss: tensor(1.6050, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 232 \tLoss: tensor(1.5977, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 233 \tLoss: tensor(1.5920, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 234 \tLoss: tensor(1.5948, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 235 \tLoss: tensor(1.5720, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 236 \tLoss: tensor(1.6313, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 237 \tLoss: tensor(1.6362, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 238 \tLoss: tensor(1.6182, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 239 \tLoss: tensor(1.6174, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 240 \tLoss: tensor(1.5598, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 241 \tLoss: tensor(1.5844, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 242 \tLoss: tensor(1.6219, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 243 \tLoss: tensor(1.5865, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 244 \tLoss: tensor(1.5557, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 245 \tLoss: tensor(1.5848, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 246 \tLoss: tensor(1.5576, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 247 \tLoss: tensor(1.5668, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 248 \tLoss: tensor(1.5917, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 249 \tLoss: tensor(1.5701, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 250 \tLoss: tensor(1.5720, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 251 \tLoss: tensor(1.6295, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 252 \tLoss: tensor(1.5785, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 253 \tLoss: tensor(1.5492, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 254 \tLoss: tensor(1.5578, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 255 \tLoss: tensor(1.5931, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 256 \tLoss: tensor(1.5749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 257 \tLoss: tensor(1.5713, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 258 \tLoss: tensor(1.5815, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 259 \tLoss: tensor(1.5862, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 260 \tLoss: tensor(1.5678, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 261 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 262 \tLoss: tensor(1.5696, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 263 \tLoss: tensor(1.5875, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 264 \tLoss: tensor(1.5754, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 265 \tLoss: tensor(1.5448, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 266 \tLoss: tensor(1.5572, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 267 \tLoss: tensor(1.5671, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 268 \tLoss: tensor(1.5650, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 269 \tLoss: tensor(1.5775, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 270 \tLoss: tensor(1.5783, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 271 \tLoss: tensor(1.6041, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 272 \tLoss: tensor(1.6095, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 273 \tLoss: tensor(1.5505, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 274 \tLoss: tensor(1.5867, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 275 \tLoss: tensor(1.5613, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 276 \tLoss: tensor(1.5424, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 277 \tLoss: tensor(1.5815, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 278 \tLoss: tensor(1.5239, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 279 \tLoss: tensor(1.6055, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 280 \tLoss: tensor(1.5584, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 281 \tLoss: tensor(1.5779, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 282 \tLoss: tensor(1.5746, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 283 \tLoss: tensor(1.5737, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 284 \tLoss: tensor(1.5394, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 285 \tLoss: tensor(1.5758, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 286 \tLoss: tensor(1.5556, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 287 \tLoss: tensor(1.5688, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 288 \tLoss: tensor(1.5646, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 289 \tLoss: tensor(1.5685, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 290 \tLoss: tensor(1.5600, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 291 \tLoss: tensor(1.5567, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 292 \tLoss: tensor(1.6010, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 293 \tLoss: tensor(1.5531, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 294 \tLoss: tensor(1.6123, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 295 \tLoss: tensor(1.5568, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 296 \tLoss: tensor(1.6003, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 297 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 298 \tLoss: tensor(1.5682, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 299 \tLoss: tensor(1.5565, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 300 \tLoss: tensor(1.5494, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 301 \tLoss: tensor(1.5656, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 302 \tLoss: tensor(1.5568, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 303 \tLoss: tensor(1.5765, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 304 \tLoss: tensor(1.5733, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 305 \tLoss: tensor(1.5552, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 306 \tLoss: tensor(1.5639, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 307 \tLoss: tensor(1.5624, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 308 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 309 \tLoss: tensor(1.6125, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 310 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 311 \tLoss: tensor(1.6050, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 312 \tLoss: tensor(1.5984, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 313 \tLoss: tensor(1.6298, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 314 \tLoss: tensor(1.5837, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 315 \tLoss: tensor(1.5702, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 316 \tLoss: tensor(1.5274, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 317 \tLoss: tensor(1.5523, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 318 \tLoss: tensor(1.6091, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 319 \tLoss: tensor(1.5502, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 320 \tLoss: tensor(1.6016, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 321 \tLoss: tensor(1.5597, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 322 \tLoss: tensor(1.5807, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 323 \tLoss: tensor(1.5308, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 324 \tLoss: tensor(1.5526, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 325 \tLoss: tensor(1.5953, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 326 \tLoss: tensor(1.5938, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 327 \tLoss: tensor(1.5719, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 328 \tLoss: tensor(1.5750, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 329 \tLoss: tensor(1.5893, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 330 \tLoss: tensor(1.6103, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 331 \tLoss: tensor(1.5631, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 332 \tLoss: tensor(1.6041, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 333 \tLoss: tensor(1.5694, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 334 \tLoss: tensor(1.5363, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 335 \tLoss: tensor(1.5908, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 336 \tLoss: tensor(1.5607, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 337 \tLoss: tensor(1.5858, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 338 \tLoss: tensor(1.5553, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 339 \tLoss: tensor(1.5811, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 340 \tLoss: tensor(1.5730, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 341 \tLoss: tensor(1.5460, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 342 \tLoss: tensor(1.6177, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 343 \tLoss: tensor(1.5707, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 344 \tLoss: tensor(1.5377, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 345 \tLoss: tensor(1.5741, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 346 \tLoss: tensor(1.5752, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 347 \tLoss: tensor(1.5571, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 348 \tLoss: tensor(1.5641, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 349 \tLoss: tensor(1.5822, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 350 \tLoss: tensor(1.5826, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 351 \tLoss: tensor(1.5709, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 352 \tLoss: tensor(1.5862, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 353 \tLoss: tensor(1.5941, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 354 \tLoss: tensor(1.5583, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 355 \tLoss: tensor(1.5665, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 356 \tLoss: tensor(1.5579, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 357 \tLoss: tensor(1.5728, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 358 \tLoss: tensor(1.5831, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 359 \tLoss: tensor(1.5595, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 360 \tLoss: tensor(1.5790, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 361 \tLoss: tensor(1.5377, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 362 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 363 \tLoss: tensor(1.5694, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 364 \tLoss: tensor(1.5495, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 365 \tLoss: tensor(1.5616, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 366 \tLoss: tensor(1.5507, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 367 \tLoss: tensor(1.5517, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 368 \tLoss: tensor(1.5629, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 369 \tLoss: tensor(1.5857, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 370 \tLoss: tensor(1.5894, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 371 \tLoss: tensor(1.5510, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 372 \tLoss: tensor(1.5659, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 373 \tLoss: tensor(1.5881, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 374 \tLoss: tensor(1.5804, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 375 \tLoss: tensor(1.5810, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 376 \tLoss: tensor(1.5423, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 377 \tLoss: tensor(1.5493, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 378 \tLoss: tensor(1.5851, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 379 \tLoss: tensor(1.5557, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 380 \tLoss: tensor(1.5803, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 381 \tLoss: tensor(1.5145, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 382 \tLoss: tensor(1.5734, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 383 \tLoss: tensor(1.5756, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 384 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 385 \tLoss: tensor(1.5630, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 386 \tLoss: tensor(1.5495, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 387 \tLoss: tensor(1.5761, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 388 \tLoss: tensor(1.6086, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 389 \tLoss: tensor(1.5802, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 390 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 0 \tLoss: tensor(1.5706, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 1 \tLoss: tensor(1.6001, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 2 \tLoss: tensor(1.5798, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 3 \tLoss: tensor(1.5439, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 4 \tLoss: tensor(1.5515, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 5 \tLoss: tensor(1.5278, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 6 \tLoss: tensor(1.5738, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 7 \tLoss: tensor(1.6014, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 8 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 9 \tLoss: tensor(1.5988, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 10 \tLoss: tensor(1.6003, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 11 \tLoss: tensor(1.5539, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 12 \tLoss: tensor(1.5279, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 13 \tLoss: tensor(1.5973, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 14 \tLoss: tensor(1.5571, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 15 \tLoss: tensor(1.5529, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 16 \tLoss: tensor(1.5459, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 17 \tLoss: tensor(1.5424, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 18 \tLoss: tensor(1.5628, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 19 \tLoss: tensor(1.5361, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 20 \tLoss: tensor(1.5557, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 21 \tLoss: tensor(1.5285, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 22 \tLoss: tensor(1.4969, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 23 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 24 \tLoss: tensor(1.5415, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 25 \tLoss: tensor(1.6349, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 26 \tLoss: tensor(1.5634, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 27 \tLoss: tensor(1.5584, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 28 \tLoss: tensor(1.5558, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 29 \tLoss: tensor(1.5279, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 30 \tLoss: tensor(1.5989, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 31 \tLoss: tensor(1.5483, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 32 \tLoss: tensor(1.5517, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 33 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 34 \tLoss: tensor(1.5340, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 35 \tLoss: tensor(1.5615, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 36 \tLoss: tensor(1.5429, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 37 \tLoss: tensor(1.5616, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 38 \tLoss: tensor(1.5371, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 39 \tLoss: tensor(1.5482, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 40 \tLoss: tensor(1.5732, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 41 \tLoss: tensor(1.5740, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 42 \tLoss: tensor(1.5770, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 43 \tLoss: tensor(1.5707, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 44 \tLoss: tensor(1.5624, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 45 \tLoss: tensor(1.5725, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 46 \tLoss: tensor(1.5236, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 47 \tLoss: tensor(1.5648, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 48 \tLoss: tensor(1.5511, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 49 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 50 \tLoss: tensor(1.5498, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 51 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 52 \tLoss: tensor(1.5359, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 53 \tLoss: tensor(1.5541, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 54 \tLoss: tensor(1.5716, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 55 \tLoss: tensor(1.5303, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 56 \tLoss: tensor(1.5464, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 57 \tLoss: tensor(1.5647, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 58 \tLoss: tensor(1.5884, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 59 \tLoss: tensor(1.5554, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 60 \tLoss: tensor(1.5298, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 61 \tLoss: tensor(1.5391, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 62 \tLoss: tensor(1.5868, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 63 \tLoss: tensor(1.5494, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 64 \tLoss: tensor(1.5562, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 65 \tLoss: tensor(1.5857, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 66 \tLoss: tensor(1.5689, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 67 \tLoss: tensor(1.5549, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 68 \tLoss: tensor(1.5524, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 69 \tLoss: tensor(1.5742, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 70 \tLoss: tensor(1.5744, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 71 \tLoss: tensor(1.5635, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 72 \tLoss: tensor(1.5659, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 73 \tLoss: tensor(1.5627, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 74 \tLoss: tensor(1.5637, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 75 \tLoss: tensor(1.5441, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 76 \tLoss: tensor(1.5393, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 77 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 78 \tLoss: tensor(1.5626, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 79 \tLoss: tensor(1.5167, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 80 \tLoss: tensor(1.5623, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 81 \tLoss: tensor(1.5546, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 82 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 83 \tLoss: tensor(1.6135, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 84 \tLoss: tensor(1.5179, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 85 \tLoss: tensor(1.5340, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 86 \tLoss: tensor(1.5529, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 87 \tLoss: tensor(1.5970, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 88 \tLoss: tensor(1.5322, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 89 \tLoss: tensor(1.5747, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 90 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 91 \tLoss: tensor(1.5348, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 92 \tLoss: tensor(1.5508, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 93 \tLoss: tensor(1.5406, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 94 \tLoss: tensor(1.5538, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 95 \tLoss: tensor(1.5286, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 96 \tLoss: tensor(1.5559, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 97 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 98 \tLoss: tensor(1.5278, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 99 \tLoss: tensor(1.5601, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 100 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 101 \tLoss: tensor(1.5177, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 102 \tLoss: tensor(1.5122, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 103 \tLoss: tensor(1.5494, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 104 \tLoss: tensor(1.5658, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 105 \tLoss: tensor(1.5475, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 106 \tLoss: tensor(1.5471, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 107 \tLoss: tensor(1.5788, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 108 \tLoss: tensor(1.5612, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 109 \tLoss: tensor(1.5739, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 110 \tLoss: tensor(1.5523, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 111 \tLoss: tensor(1.5686, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 112 \tLoss: tensor(1.5552, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 113 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 114 \tLoss: tensor(1.5283, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 115 \tLoss: tensor(1.5515, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 116 \tLoss: tensor(1.5701, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 117 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 118 \tLoss: tensor(1.5950, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 119 \tLoss: tensor(1.5672, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 120 \tLoss: tensor(1.5156, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 121 \tLoss: tensor(1.5456, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 122 \tLoss: tensor(1.5688, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 123 \tLoss: tensor(1.5462, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 124 \tLoss: tensor(1.5528, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 125 \tLoss: tensor(1.5702, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 126 \tLoss: tensor(1.5896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 127 \tLoss: tensor(1.5663, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 128 \tLoss: tensor(1.5645, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 129 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 130 \tLoss: tensor(1.5866, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 131 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 132 \tLoss: tensor(1.5378, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 133 \tLoss: tensor(1.5253, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 134 \tLoss: tensor(1.5427, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 135 \tLoss: tensor(1.5307, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 136 \tLoss: tensor(1.5561, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 137 \tLoss: tensor(1.5620, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 138 \tLoss: tensor(1.5369, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 139 \tLoss: tensor(1.5315, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 140 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 141 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 142 \tLoss: tensor(1.5556, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 143 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 144 \tLoss: tensor(1.5341, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 145 \tLoss: tensor(1.5696, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 146 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 147 \tLoss: tensor(1.5383, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 148 \tLoss: tensor(1.5717, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 149 \tLoss: tensor(1.5675, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 150 \tLoss: tensor(1.5720, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 151 \tLoss: tensor(1.5766, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 152 \tLoss: tensor(1.5440, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 153 \tLoss: tensor(1.5292, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 154 \tLoss: tensor(1.5533, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 155 \tLoss: tensor(1.5479, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 156 \tLoss: tensor(1.5687, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 157 \tLoss: tensor(1.5307, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 158 \tLoss: tensor(1.5828, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 159 \tLoss: tensor(1.5802, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 160 \tLoss: tensor(1.5282, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 161 \tLoss: tensor(1.5554, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 162 \tLoss: tensor(1.5346, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 163 \tLoss: tensor(1.5357, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 164 \tLoss: tensor(1.5447, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 165 \tLoss: tensor(1.5183, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 166 \tLoss: tensor(1.5460, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 167 \tLoss: tensor(1.5272, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 168 \tLoss: tensor(1.5620, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 169 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 170 \tLoss: tensor(1.5829, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 171 \tLoss: tensor(1.5463, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 172 \tLoss: tensor(1.5656, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 173 \tLoss: tensor(1.5621, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 174 \tLoss: tensor(1.5731, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 175 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 176 \tLoss: tensor(1.5760, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 177 \tLoss: tensor(1.5460, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 178 \tLoss: tensor(1.5353, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 179 \tLoss: tensor(1.5270, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 180 \tLoss: tensor(1.5768, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 181 \tLoss: tensor(1.5941, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 182 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 183 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 184 \tLoss: tensor(1.5071, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 185 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 186 \tLoss: tensor(1.5472, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 187 \tLoss: tensor(1.5433, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 188 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 189 \tLoss: tensor(1.5426, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 190 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 191 \tLoss: tensor(1.5281, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 192 \tLoss: tensor(1.5423, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 193 \tLoss: tensor(1.5509, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 194 \tLoss: tensor(1.5555, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 195 \tLoss: tensor(1.5955, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 196 \tLoss: tensor(1.5271, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 197 \tLoss: tensor(1.5125, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 198 \tLoss: tensor(1.5555, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 199 \tLoss: tensor(1.5771, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 200 \tLoss: tensor(1.5280, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 201 \tLoss: tensor(1.5652, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 202 \tLoss: tensor(1.5324, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 203 \tLoss: tensor(1.5484, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 204 \tLoss: tensor(1.5895, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 205 \tLoss: tensor(1.5338, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 206 \tLoss: tensor(1.5564, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 207 \tLoss: tensor(1.5114, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 208 \tLoss: tensor(1.5438, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 209 \tLoss: tensor(1.5367, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 210 \tLoss: tensor(1.5399, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 211 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 212 \tLoss: tensor(1.5375, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 213 \tLoss: tensor(1.5413, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 214 \tLoss: tensor(1.5401, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 215 \tLoss: tensor(1.5413, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 216 \tLoss: tensor(1.5163, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 217 \tLoss: tensor(1.5681, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 218 \tLoss: tensor(1.5449, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 219 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 220 \tLoss: tensor(1.5649, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 221 \tLoss: tensor(1.5607, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 222 \tLoss: tensor(1.5591, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 223 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 224 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 225 \tLoss: tensor(1.5686, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 226 \tLoss: tensor(1.5511, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 227 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 228 \tLoss: tensor(1.5504, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 229 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 230 \tLoss: tensor(1.6126, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 231 \tLoss: tensor(1.5648, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 232 \tLoss: tensor(1.5309, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 233 \tLoss: tensor(1.5476, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 234 \tLoss: tensor(1.5578, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 235 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 236 \tLoss: tensor(1.5337, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 237 \tLoss: tensor(1.5228, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 238 \tLoss: tensor(1.5440, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 239 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 240 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 241 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 242 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 243 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 244 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 245 \tLoss: tensor(1.5505, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 246 \tLoss: tensor(1.5333, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 247 \tLoss: tensor(1.5282, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 248 \tLoss: tensor(1.5429, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 249 \tLoss: tensor(1.5608, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 250 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 251 \tLoss: tensor(1.4990, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 252 \tLoss: tensor(1.5156, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 253 \tLoss: tensor(1.5792, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 254 \tLoss: tensor(1.5475, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 255 \tLoss: tensor(1.5401, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 256 \tLoss: tensor(1.5726, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 257 \tLoss: tensor(1.5562, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 258 \tLoss: tensor(1.5581, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 259 \tLoss: tensor(1.5109, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 260 \tLoss: tensor(1.5747, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 261 \tLoss: tensor(1.5183, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 262 \tLoss: tensor(1.5606, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 263 \tLoss: tensor(1.5056, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 264 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 265 \tLoss: tensor(1.5326, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 266 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 267 \tLoss: tensor(1.5482, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 268 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 269 \tLoss: tensor(1.6027, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 270 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 271 \tLoss: tensor(1.5674, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 272 \tLoss: tensor(1.5491, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 273 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 274 \tLoss: tensor(1.5822, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 275 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 276 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 277 \tLoss: tensor(1.5308, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 278 \tLoss: tensor(1.5375, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 279 \tLoss: tensor(1.5132, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 280 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 281 \tLoss: tensor(1.5308, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 282 \tLoss: tensor(1.5139, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 283 \tLoss: tensor(1.5070, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 284 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 285 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 286 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 287 \tLoss: tensor(1.5769, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 288 \tLoss: tensor(1.5799, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 289 \tLoss: tensor(1.5441, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 290 \tLoss: tensor(1.5663, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 291 \tLoss: tensor(1.5094, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 292 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 293 \tLoss: tensor(1.5588, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 294 \tLoss: tensor(1.5422, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 295 \tLoss: tensor(1.5574, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 296 \tLoss: tensor(1.5540, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 297 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 298 \tLoss: tensor(1.5465, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 299 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 300 \tLoss: tensor(1.5167, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 301 \tLoss: tensor(1.5788, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 302 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 303 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 304 \tLoss: tensor(1.5377, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 305 \tLoss: tensor(1.5469, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 306 \tLoss: tensor(1.5275, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 307 \tLoss: tensor(1.5398, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 308 \tLoss: tensor(1.5510, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 309 \tLoss: tensor(1.5817, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 310 \tLoss: tensor(1.5275, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 311 \tLoss: tensor(1.5487, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 312 \tLoss: tensor(1.5648, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 313 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 314 \tLoss: tensor(1.5436, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 315 \tLoss: tensor(1.5576, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 316 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 317 \tLoss: tensor(1.6005, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 318 \tLoss: tensor(1.5541, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 319 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 320 \tLoss: tensor(1.5285, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 321 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 322 \tLoss: tensor(1.5377, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 323 \tLoss: tensor(1.5361, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 324 \tLoss: tensor(1.5312, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 325 \tLoss: tensor(1.5594, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 326 \tLoss: tensor(1.5685, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 327 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 328 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 329 \tLoss: tensor(1.5332, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 330 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 331 \tLoss: tensor(1.5591, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 332 \tLoss: tensor(1.5346, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 333 \tLoss: tensor(1.5582, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 334 \tLoss: tensor(1.5462, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 335 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 336 \tLoss: tensor(1.5532, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 337 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 338 \tLoss: tensor(1.5581, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 339 \tLoss: tensor(1.5487, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 340 \tLoss: tensor(1.5733, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 341 \tLoss: tensor(1.5638, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 342 \tLoss: tensor(1.5555, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 343 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 344 \tLoss: tensor(1.5392, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 345 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 346 \tLoss: tensor(1.5175, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 347 \tLoss: tensor(1.5262, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 348 \tLoss: tensor(1.5551, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 349 \tLoss: tensor(1.5381, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 350 \tLoss: tensor(1.5045, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 351 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 352 \tLoss: tensor(1.5170, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 353 \tLoss: tensor(1.5554, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 354 \tLoss: tensor(1.5746, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 355 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 356 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 357 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 358 \tLoss: tensor(1.5849, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 359 \tLoss: tensor(1.5498, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 360 \tLoss: tensor(1.5510, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 361 \tLoss: tensor(1.5538, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 362 \tLoss: tensor(1.5627, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 363 \tLoss: tensor(1.5155, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 364 \tLoss: tensor(1.5721, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 365 \tLoss: tensor(1.5320, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 366 \tLoss: tensor(1.5806, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 367 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 368 \tLoss: tensor(1.5346, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 369 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 370 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 371 \tLoss: tensor(1.5243, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 372 \tLoss: tensor(1.5258, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 373 \tLoss: tensor(1.5609, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 374 \tLoss: tensor(1.5272, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 375 \tLoss: tensor(1.5475, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 376 \tLoss: tensor(1.5579, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 377 \tLoss: tensor(1.5207, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 378 \tLoss: tensor(1.5594, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 379 \tLoss: tensor(1.5581, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 380 \tLoss: tensor(1.5618, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 381 \tLoss: tensor(1.5205, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 382 \tLoss: tensor(1.5200, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 383 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 384 \tLoss: tensor(1.5396, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 385 \tLoss: tensor(1.5313, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 386 \tLoss: tensor(1.5528, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 387 \tLoss: tensor(1.5230, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 388 \tLoss: tensor(1.5075, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 389 \tLoss: tensor(1.5342, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 390 \tLoss: tensor(1.5026, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ1CAG2xIt-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4c8278f8-514b-4a4a-9d4f-fa9608961b60"
      },
      "source": [
        "def calc_accuracy(model, dataloader):\n",
        "    num_correct = 0\n",
        "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
        "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
        "        predictions = model(inputs)\n",
        "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
        "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
        "        num_correct += int(sum(predictions == labels))\n",
        "    percent_correct = num_correct / num_examples * 100\n",
        "    return percent_correct\n",
        "\n",
        "\n",
        "print('Train Accuracy:', calc_accuracy(myNeuralNetwork, train_loader))\n",
        "print('Validation Accuracy:', calc_accuracy(myNeuralNetwork, val_loader))\n",
        "\n",
        "print('Test Accuracy:', calc_accuracy(myNeuralNetwork, test_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 93.17800000000001\n",
            "Validation Accuracy: 92.89\n",
            "Test Accuracy: 46.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtLxAls2JrLn",
        "colab_type": "text"
      },
      "source": [
        "As we see above, our model overfits. It performs very good on training data, but it is poor on test data. So that means, we have a overfitting problem.\n",
        "Let's try dropout and see if we can overcome this issue:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE4wQp9ADHUV",
        "colab_type": "text"
      },
      "source": [
        "**Dropout Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7ohCJIPDMRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "8dab710f-d518-45f6-bb2c-1fe6cf93dd03"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "crop_size = 22\n",
        "\n",
        "traintransforms = []\n",
        "traintransforms.append(transforms.CenterCrop(crop_size))\n",
        "traintransforms.append(transforms.ToTensor())\n",
        "traintransforms = transforms.Compose(traintransforms)\n",
        "\n",
        "# GET THE TRAINING DATASET\n",
        "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
        "                            transform=traintransforms,          # transform the data from a PIL image to a tensor\n",
        "                            train=True,                               # is this training data?\n",
        "                            download=True                             # should i download it if it's not already here?\n",
        "                           )\n",
        "\n",
        "testtransforms = []\n",
        "testtransforms.append(transforms.RandomCrop(crop_size))\n",
        "testtransforms.append(transforms.ToTensor())\n",
        "testtransforms = transforms.Compose(testtransforms)\n",
        "\n",
        "# GET THE TEST DATASET\n",
        "test_data = datasets.MNIST(root='MNIST-data',\n",
        "                           transform=testtransforms,\n",
        "                           train=False,\n",
        "                          )\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
        "x = train_data[np.random.randint(0, 300)][0]    # get the first example\n",
        "plt.imshow(x[0].numpy())\n",
        "plt.show()\n",
        "x = test_data[np.random.randint(0, 300)][0]    # get the first example\n",
        "plt.imshow(x[0].numpy())\n",
        "plt.show()\n",
        "\n",
        "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
        "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh0RTz2ADMmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "# MAKE TRAINING DATALOADER\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# MAKE VALIDATION DATALOADER\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_data,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# MAKE TEST DATALOADER\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX6_CNxDDMaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F \n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNetworkClassDropout(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()    # initialise parent module\n",
        "        self.layer1 = torch.nn.Linear(crop_size*crop_size, 225)\n",
        "        self.layer2 = torch.nn.Linear(225, 100)\n",
        "        self.layer3 = torch.nn.Linear(100, 10)\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "        self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, crop_size*crop_size)\n",
        "        x = self.layer1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.dropout2(x)       \n",
        "        x = F.relu(x)\n",
        "        x = self.layer3(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "def get_n_params(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return n_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K29OXl5DMEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b572e97a-8d06-4d38-89bc-0e1b8a867b8c"
      },
      "source": [
        "learning_rate = 0.0005\n",
        "myNeuralNetworkDropout = NeuralNetworkClassDropout()\n",
        "print('Number of parameters in model:', get_n_params(myNeuralNetworkDropout))\n",
        "\n",
        "# CREATE OUR OPTIMISER\n",
        "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
        "    myNeuralNetworkDropout.parameters(),          # what should it optimise?\n",
        "    lr=learning_rate                       # using what learning rate?\n",
        ")\n",
        "\n",
        "# CREATE OUR CRITERION\n",
        "criterion = torch.nn.CrossEntropyLoss()             # callable class that compares our predictions to our labels and returns our loss\n",
        "\n",
        "# SET UP TRAINING VISUALISATION\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()                            # we will use this to show our models performance on a graph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters in model: 132735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gNE2tMJMVEt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2151a7d7-dd33-4acb-ce72-e56c7fef2428"
      },
      "source": [
        "# TRAINING LOOP\n",
        "def train(model, epochs):\n",
        "    model.train()                                  # put the model into training mode (more on this later)\n",
        "    for epoch in range(epochs):\n",
        "        for idx, minibatch in enumerate(train_loader):\n",
        "            inputs, labels = minibatch\n",
        "            prediction = model(inputs)             # pass the data forward through the model\n",
        "            loss = criterion(prediction, labels)   # compute the loss\n",
        "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
        "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
        "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
        "            optimiser.step()                       # update the model's parameters\n",
        "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
        "            \n",
        "            \n",
        "train(myNeuralNetworkDropout, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tBatch: 0 \tLoss: tensor(2.3021, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 1 \tLoss: tensor(2.3022, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 2 \tLoss: tensor(2.3005, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 3 \tLoss: tensor(2.3003, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 4 \tLoss: tensor(2.2991, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 5 \tLoss: tensor(2.2981, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 6 \tLoss: tensor(2.2965, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 7 \tLoss: tensor(2.2943, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 8 \tLoss: tensor(2.2953, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 9 \tLoss: tensor(2.2917, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 10 \tLoss: tensor(2.2902, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 11 \tLoss: tensor(2.2924, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 12 \tLoss: tensor(2.2852, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 13 \tLoss: tensor(2.2855, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 14 \tLoss: tensor(2.2808, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 15 \tLoss: tensor(2.2856, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 16 \tLoss: tensor(2.2699, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 17 \tLoss: tensor(2.2692, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 18 \tLoss: tensor(2.2718, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 19 \tLoss: tensor(2.2721, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 20 \tLoss: tensor(2.2632, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 21 \tLoss: tensor(2.2633, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 22 \tLoss: tensor(2.2510, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 23 \tLoss: tensor(2.2083, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 24 \tLoss: tensor(2.2183, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 25 \tLoss: tensor(2.2136, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 26 \tLoss: tensor(2.2195, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 27 \tLoss: tensor(2.2126, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 28 \tLoss: tensor(2.2225, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 29 \tLoss: tensor(2.1942, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 30 \tLoss: tensor(2.1749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 31 \tLoss: tensor(2.1802, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 32 \tLoss: tensor(2.1818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 33 \tLoss: tensor(2.1670, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 34 \tLoss: tensor(2.1530, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 35 \tLoss: tensor(2.1204, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 36 \tLoss: tensor(2.1141, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 37 \tLoss: tensor(2.0879, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 38 \tLoss: tensor(2.1144, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 39 \tLoss: tensor(2.0825, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 40 \tLoss: tensor(2.0920, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 41 \tLoss: tensor(2.0695, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 42 \tLoss: tensor(2.0946, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 43 \tLoss: tensor(2.0692, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 44 \tLoss: tensor(2.0474, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 45 \tLoss: tensor(2.0592, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 46 \tLoss: tensor(2.0576, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 47 \tLoss: tensor(2.0407, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 48 \tLoss: tensor(2.0110, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 49 \tLoss: tensor(1.9744, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 50 \tLoss: tensor(2.0228, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 51 \tLoss: tensor(1.9590, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 52 \tLoss: tensor(1.9318, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 53 \tLoss: tensor(1.9831, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 54 \tLoss: tensor(1.9775, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 55 \tLoss: tensor(1.9749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 56 \tLoss: tensor(1.9284, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 57 \tLoss: tensor(1.9529, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 58 \tLoss: tensor(1.8741, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 59 \tLoss: tensor(1.8811, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 60 \tLoss: tensor(1.8391, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 61 \tLoss: tensor(1.8968, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 62 \tLoss: tensor(1.9334, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 63 \tLoss: tensor(1.8858, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 64 \tLoss: tensor(1.8992, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 65 \tLoss: tensor(1.8320, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 66 \tLoss: tensor(1.8342, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 67 \tLoss: tensor(1.8390, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 68 \tLoss: tensor(1.8594, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 69 \tLoss: tensor(1.8371, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 70 \tLoss: tensor(1.9064, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 71 \tLoss: tensor(1.8595, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 72 \tLoss: tensor(1.8725, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 73 \tLoss: tensor(1.7767, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 74 \tLoss: tensor(1.8489, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 75 \tLoss: tensor(1.8100, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 76 \tLoss: tensor(1.7988, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 77 \tLoss: tensor(1.8124, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 78 \tLoss: tensor(1.7927, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 79 \tLoss: tensor(1.7939, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 80 \tLoss: tensor(1.7896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 81 \tLoss: tensor(1.8395, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 82 \tLoss: tensor(1.8067, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 83 \tLoss: tensor(1.8119, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 84 \tLoss: tensor(1.8301, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 85 \tLoss: tensor(1.7969, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 86 \tLoss: tensor(1.7749, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 87 \tLoss: tensor(1.7981, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 88 \tLoss: tensor(1.8187, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 89 \tLoss: tensor(1.7636, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 90 \tLoss: tensor(1.7709, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 91 \tLoss: tensor(1.7954, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 92 \tLoss: tensor(1.8178, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 93 \tLoss: tensor(1.7260, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 94 \tLoss: tensor(1.7801, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 95 \tLoss: tensor(1.7597, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 96 \tLoss: tensor(1.7883, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 97 \tLoss: tensor(1.7187, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 98 \tLoss: tensor(1.8137, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 99 \tLoss: tensor(1.7863, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 100 \tLoss: tensor(1.7251, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 101 \tLoss: tensor(1.7667, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 102 \tLoss: tensor(1.7750, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 103 \tLoss: tensor(1.7607, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 104 \tLoss: tensor(1.7115, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 105 \tLoss: tensor(1.7336, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 106 \tLoss: tensor(1.7439, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 107 \tLoss: tensor(1.7560, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 108 \tLoss: tensor(1.7604, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 109 \tLoss: tensor(1.7438, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 110 \tLoss: tensor(1.7471, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 111 \tLoss: tensor(1.7303, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 112 \tLoss: tensor(1.7310, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 113 \tLoss: tensor(1.7714, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 114 \tLoss: tensor(1.7514, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 115 \tLoss: tensor(1.7295, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 116 \tLoss: tensor(1.7142, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 117 \tLoss: tensor(1.8176, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 118 \tLoss: tensor(1.7247, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 119 \tLoss: tensor(1.7536, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 120 \tLoss: tensor(1.7674, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 121 \tLoss: tensor(1.7361, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 122 \tLoss: tensor(1.7128, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 123 \tLoss: tensor(1.6955, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 124 \tLoss: tensor(1.7596, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 125 \tLoss: tensor(1.7500, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 126 \tLoss: tensor(1.7127, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 127 \tLoss: tensor(1.6960, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 128 \tLoss: tensor(1.7442, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 129 \tLoss: tensor(1.7309, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 130 \tLoss: tensor(1.7375, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 131 \tLoss: tensor(1.7231, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 132 \tLoss: tensor(1.7883, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 133 \tLoss: tensor(1.7226, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 134 \tLoss: tensor(1.7579, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 135 \tLoss: tensor(1.7294, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 136 \tLoss: tensor(1.7488, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 137 \tLoss: tensor(1.7197, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 138 \tLoss: tensor(1.7003, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 139 \tLoss: tensor(1.7200, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 140 \tLoss: tensor(1.6969, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 141 \tLoss: tensor(1.6423, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 142 \tLoss: tensor(1.7606, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 143 \tLoss: tensor(1.6733, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 144 \tLoss: tensor(1.6890, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 145 \tLoss: tensor(1.7219, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 146 \tLoss: tensor(1.7614, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 147 \tLoss: tensor(1.6973, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 148 \tLoss: tensor(1.7526, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 149 \tLoss: tensor(1.6786, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 150 \tLoss: tensor(1.6646, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 151 \tLoss: tensor(1.6949, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 152 \tLoss: tensor(1.7197, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 153 \tLoss: tensor(1.6577, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 154 \tLoss: tensor(1.7090, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 155 \tLoss: tensor(1.7053, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 156 \tLoss: tensor(1.7169, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 157 \tLoss: tensor(1.7173, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 158 \tLoss: tensor(1.7097, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 159 \tLoss: tensor(1.6907, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 160 \tLoss: tensor(1.6937, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 161 \tLoss: tensor(1.7235, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 162 \tLoss: tensor(1.7129, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 163 \tLoss: tensor(1.7318, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 164 \tLoss: tensor(1.6718, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 165 \tLoss: tensor(1.7234, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 166 \tLoss: tensor(1.6696, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 167 \tLoss: tensor(1.6470, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 168 \tLoss: tensor(1.6969, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 169 \tLoss: tensor(1.7025, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 170 \tLoss: tensor(1.6753, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 171 \tLoss: tensor(1.6939, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 172 \tLoss: tensor(1.6723, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 173 \tLoss: tensor(1.6233, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 174 \tLoss: tensor(1.6702, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 175 \tLoss: tensor(1.6495, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 176 \tLoss: tensor(1.7144, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 177 \tLoss: tensor(1.7101, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 178 \tLoss: tensor(1.6867, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 179 \tLoss: tensor(1.7438, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 180 \tLoss: tensor(1.6795, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 181 \tLoss: tensor(1.6994, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 182 \tLoss: tensor(1.6774, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 183 \tLoss: tensor(1.6864, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 184 \tLoss: tensor(1.7180, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 185 \tLoss: tensor(1.6590, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 186 \tLoss: tensor(1.6867, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 187 \tLoss: tensor(1.7648, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 188 \tLoss: tensor(1.7044, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 189 \tLoss: tensor(1.6692, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 190 \tLoss: tensor(1.6548, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 191 \tLoss: tensor(1.6751, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 192 \tLoss: tensor(1.6477, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 193 \tLoss: tensor(1.6479, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 194 \tLoss: tensor(1.7440, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 195 \tLoss: tensor(1.6717, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 196 \tLoss: tensor(1.6609, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 197 \tLoss: tensor(1.6502, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 198 \tLoss: tensor(1.6566, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 199 \tLoss: tensor(1.6875, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 200 \tLoss: tensor(1.6660, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 201 \tLoss: tensor(1.6521, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 202 \tLoss: tensor(1.6843, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 203 \tLoss: tensor(1.7332, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 204 \tLoss: tensor(1.7171, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 205 \tLoss: tensor(1.6485, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 206 \tLoss: tensor(1.6525, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 207 \tLoss: tensor(1.6618, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 208 \tLoss: tensor(1.6630, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 209 \tLoss: tensor(1.6537, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 210 \tLoss: tensor(1.6468, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 211 \tLoss: tensor(1.6381, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 212 \tLoss: tensor(1.6653, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 213 \tLoss: tensor(1.6432, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 214 \tLoss: tensor(1.5997, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 215 \tLoss: tensor(1.6866, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 216 \tLoss: tensor(1.6610, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 217 \tLoss: tensor(1.6054, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 218 \tLoss: tensor(1.6635, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 219 \tLoss: tensor(1.7114, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 220 \tLoss: tensor(1.5912, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 221 \tLoss: tensor(1.6224, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 222 \tLoss: tensor(1.6532, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 223 \tLoss: tensor(1.6727, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 224 \tLoss: tensor(1.6566, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 225 \tLoss: tensor(1.6031, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 226 \tLoss: tensor(1.5949, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 227 \tLoss: tensor(1.6717, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 228 \tLoss: tensor(1.6981, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 229 \tLoss: tensor(1.6281, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 230 \tLoss: tensor(1.5890, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 231 \tLoss: tensor(1.5926, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 232 \tLoss: tensor(1.6131, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 233 \tLoss: tensor(1.6511, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 234 \tLoss: tensor(1.6287, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 235 \tLoss: tensor(1.5864, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 236 \tLoss: tensor(1.6147, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 237 \tLoss: tensor(1.6180, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 238 \tLoss: tensor(1.6549, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 239 \tLoss: tensor(1.6971, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 240 \tLoss: tensor(1.6380, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 241 \tLoss: tensor(1.6338, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 242 \tLoss: tensor(1.6959, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 243 \tLoss: tensor(1.5770, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 244 \tLoss: tensor(1.6197, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 245 \tLoss: tensor(1.6464, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 246 \tLoss: tensor(1.6306, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 247 \tLoss: tensor(1.6487, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 248 \tLoss: tensor(1.6462, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 249 \tLoss: tensor(1.5841, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 250 \tLoss: tensor(1.6561, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 251 \tLoss: tensor(1.6808, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 252 \tLoss: tensor(1.6146, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 253 \tLoss: tensor(1.6265, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 254 \tLoss: tensor(1.5999, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 255 \tLoss: tensor(1.6112, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 256 \tLoss: tensor(1.6291, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 257 \tLoss: tensor(1.6077, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 258 \tLoss: tensor(1.6432, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 259 \tLoss: tensor(1.6518, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 260 \tLoss: tensor(1.6361, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 261 \tLoss: tensor(1.6167, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 262 \tLoss: tensor(1.5858, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 263 \tLoss: tensor(1.6331, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 264 \tLoss: tensor(1.6335, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 265 \tLoss: tensor(1.6157, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 266 \tLoss: tensor(1.6351, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 267 \tLoss: tensor(1.6680, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 268 \tLoss: tensor(1.6298, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 269 \tLoss: tensor(1.6492, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 270 \tLoss: tensor(1.6416, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 271 \tLoss: tensor(1.6400, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 272 \tLoss: tensor(1.6234, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 273 \tLoss: tensor(1.6548, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 274 \tLoss: tensor(1.6630, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 275 \tLoss: tensor(1.6369, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 276 \tLoss: tensor(1.6111, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 277 \tLoss: tensor(1.6032, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 278 \tLoss: tensor(1.6258, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 279 \tLoss: tensor(1.6238, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 280 \tLoss: tensor(1.6300, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 281 \tLoss: tensor(1.6237, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 282 \tLoss: tensor(1.5991, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 283 \tLoss: tensor(1.6557, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 284 \tLoss: tensor(1.6217, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 285 \tLoss: tensor(1.6544, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 286 \tLoss: tensor(1.6360, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 287 \tLoss: tensor(1.6089, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 288 \tLoss: tensor(1.6084, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 289 \tLoss: tensor(1.6544, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 290 \tLoss: tensor(1.6330, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 291 \tLoss: tensor(1.6664, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 292 \tLoss: tensor(1.6084, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 293 \tLoss: tensor(1.6297, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 294 \tLoss: tensor(1.7174, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 295 \tLoss: tensor(1.5818, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 296 \tLoss: tensor(1.6428, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 297 \tLoss: tensor(1.6472, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 298 \tLoss: tensor(1.6007, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 299 \tLoss: tensor(1.6302, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 300 \tLoss: tensor(1.6035, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 301 \tLoss: tensor(1.6669, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 302 \tLoss: tensor(1.6388, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 303 \tLoss: tensor(1.6237, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 304 \tLoss: tensor(1.6694, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 305 \tLoss: tensor(1.6359, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 306 \tLoss: tensor(1.6060, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 307 \tLoss: tensor(1.6273, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 308 \tLoss: tensor(1.5442, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 309 \tLoss: tensor(1.6250, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 310 \tLoss: tensor(1.6195, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 311 \tLoss: tensor(1.6659, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 312 \tLoss: tensor(1.6264, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 313 \tLoss: tensor(1.6376, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 314 \tLoss: tensor(1.6911, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 315 \tLoss: tensor(1.6005, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 316 \tLoss: tensor(1.6192, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 317 \tLoss: tensor(1.6033, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 318 \tLoss: tensor(1.6442, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 319 \tLoss: tensor(1.5883, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 320 \tLoss: tensor(1.5759, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 321 \tLoss: tensor(1.6039, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 322 \tLoss: tensor(1.6266, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 323 \tLoss: tensor(1.5914, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 324 \tLoss: tensor(1.6110, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 325 \tLoss: tensor(1.5726, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 326 \tLoss: tensor(1.6533, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 327 \tLoss: tensor(1.6319, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 328 \tLoss: tensor(1.5810, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 329 \tLoss: tensor(1.6406, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 330 \tLoss: tensor(1.6445, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 331 \tLoss: tensor(1.5809, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 332 \tLoss: tensor(1.5662, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 333 \tLoss: tensor(1.6274, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 334 \tLoss: tensor(1.5677, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 335 \tLoss: tensor(1.6841, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 336 \tLoss: tensor(1.6030, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 337 \tLoss: tensor(1.5842, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 338 \tLoss: tensor(1.5934, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 339 \tLoss: tensor(1.6212, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 340 \tLoss: tensor(1.6231, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 341 \tLoss: tensor(1.5807, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 342 \tLoss: tensor(1.6180, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 343 \tLoss: tensor(1.5923, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 344 \tLoss: tensor(1.6192, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 345 \tLoss: tensor(1.6064, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 346 \tLoss: tensor(1.6245, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 347 \tLoss: tensor(1.6031, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 348 \tLoss: tensor(1.6270, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 349 \tLoss: tensor(1.6051, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 350 \tLoss: tensor(1.5665, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 351 \tLoss: tensor(1.5942, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 352 \tLoss: tensor(1.5494, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 353 \tLoss: tensor(1.6340, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 354 \tLoss: tensor(1.6109, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 355 \tLoss: tensor(1.5954, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 356 \tLoss: tensor(1.6089, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 357 \tLoss: tensor(1.6045, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 358 \tLoss: tensor(1.5813, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 359 \tLoss: tensor(1.6372, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 360 \tLoss: tensor(1.6106, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 361 \tLoss: tensor(1.6120, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 362 \tLoss: tensor(1.6248, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 363 \tLoss: tensor(1.6107, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 364 \tLoss: tensor(1.6014, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 365 \tLoss: tensor(1.6451, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 366 \tLoss: tensor(1.5904, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 367 \tLoss: tensor(1.5786, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 368 \tLoss: tensor(1.6240, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 369 \tLoss: tensor(1.6449, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 370 \tLoss: tensor(1.5888, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 371 \tLoss: tensor(1.6106, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 372 \tLoss: tensor(1.6138, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 373 \tLoss: tensor(1.5614, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 374 \tLoss: tensor(1.5941, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 375 \tLoss: tensor(1.6169, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 376 \tLoss: tensor(1.5973, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 377 \tLoss: tensor(1.5747, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 378 \tLoss: tensor(1.6112, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 379 \tLoss: tensor(1.5728, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 380 \tLoss: tensor(1.6287, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 381 \tLoss: tensor(1.6000, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 382 \tLoss: tensor(1.5949, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 383 \tLoss: tensor(1.6050, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 384 \tLoss: tensor(1.5668, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 385 \tLoss: tensor(1.5815, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 386 \tLoss: tensor(1.6195, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 387 \tLoss: tensor(1.5995, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 388 \tLoss: tensor(1.6109, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 389 \tLoss: tensor(1.6117, grad_fn=<NllLossBackward>)\n",
            "Epoch: 0 \tBatch: 390 \tLoss: tensor(1.5665, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 0 \tLoss: tensor(1.6111, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 1 \tLoss: tensor(1.5885, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 2 \tLoss: tensor(1.6154, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 3 \tLoss: tensor(1.6132, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 4 \tLoss: tensor(1.6087, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 5 \tLoss: tensor(1.5880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 6 \tLoss: tensor(1.6086, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 7 \tLoss: tensor(1.5997, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 8 \tLoss: tensor(1.5586, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 9 \tLoss: tensor(1.6140, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 10 \tLoss: tensor(1.6097, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 11 \tLoss: tensor(1.5812, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 12 \tLoss: tensor(1.5807, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 13 \tLoss: tensor(1.5770, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 14 \tLoss: tensor(1.6169, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 15 \tLoss: tensor(1.5953, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 16 \tLoss: tensor(1.6009, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 17 \tLoss: tensor(1.5814, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 18 \tLoss: tensor(1.5636, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 19 \tLoss: tensor(1.6207, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 20 \tLoss: tensor(1.6490, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 21 \tLoss: tensor(1.6141, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 22 \tLoss: tensor(1.5791, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 23 \tLoss: tensor(1.5579, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 24 \tLoss: tensor(1.5896, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 25 \tLoss: tensor(1.5722, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 26 \tLoss: tensor(1.6368, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 27 \tLoss: tensor(1.5901, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 28 \tLoss: tensor(1.6151, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 29 \tLoss: tensor(1.6307, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 30 \tLoss: tensor(1.5717, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 31 \tLoss: tensor(1.5678, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 32 \tLoss: tensor(1.5793, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 33 \tLoss: tensor(1.6270, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 34 \tLoss: tensor(1.6321, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 35 \tLoss: tensor(1.5860, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 36 \tLoss: tensor(1.6381, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 37 \tLoss: tensor(1.6202, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 38 \tLoss: tensor(1.5975, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 39 \tLoss: tensor(1.5883, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 40 \tLoss: tensor(1.6308, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 41 \tLoss: tensor(1.6086, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 42 \tLoss: tensor(1.6215, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 43 \tLoss: tensor(1.6370, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 44 \tLoss: tensor(1.5564, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 45 \tLoss: tensor(1.5976, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 46 \tLoss: tensor(1.5859, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 47 \tLoss: tensor(1.6017, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 48 \tLoss: tensor(1.5930, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 49 \tLoss: tensor(1.5694, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 50 \tLoss: tensor(1.5403, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 51 \tLoss: tensor(1.6244, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 52 \tLoss: tensor(1.6326, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 53 \tLoss: tensor(1.5665, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 54 \tLoss: tensor(1.5945, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 55 \tLoss: tensor(1.5824, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 56 \tLoss: tensor(1.5727, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 57 \tLoss: tensor(1.5810, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 58 \tLoss: tensor(1.6143, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 59 \tLoss: tensor(1.6019, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 60 \tLoss: tensor(1.5685, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 61 \tLoss: tensor(1.6046, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 62 \tLoss: tensor(1.6413, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 63 \tLoss: tensor(1.5711, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 64 \tLoss: tensor(1.5599, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 65 \tLoss: tensor(1.6011, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 66 \tLoss: tensor(1.5596, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 67 \tLoss: tensor(1.5725, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 68 \tLoss: tensor(1.5738, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 69 \tLoss: tensor(1.6046, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 70 \tLoss: tensor(1.6051, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 71 \tLoss: tensor(1.6272, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 72 \tLoss: tensor(1.5877, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 73 \tLoss: tensor(1.5421, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 74 \tLoss: tensor(1.6196, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 75 \tLoss: tensor(1.5998, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 76 \tLoss: tensor(1.5768, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 77 \tLoss: tensor(1.6153, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 78 \tLoss: tensor(1.5751, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 79 \tLoss: tensor(1.5674, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 80 \tLoss: tensor(1.5806, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 81 \tLoss: tensor(1.5836, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 82 \tLoss: tensor(1.5915, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 83 \tLoss: tensor(1.5881, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 84 \tLoss: tensor(1.5865, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 85 \tLoss: tensor(1.6217, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 86 \tLoss: tensor(1.5757, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 87 \tLoss: tensor(1.5821, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 88 \tLoss: tensor(1.5689, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 89 \tLoss: tensor(1.5900, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 90 \tLoss: tensor(1.6039, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 91 \tLoss: tensor(1.5900, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 92 \tLoss: tensor(1.5384, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 93 \tLoss: tensor(1.5933, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 94 \tLoss: tensor(1.5746, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 95 \tLoss: tensor(1.6065, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 96 \tLoss: tensor(1.6244, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 97 \tLoss: tensor(1.5808, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 98 \tLoss: tensor(1.5259, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 99 \tLoss: tensor(1.5557, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 100 \tLoss: tensor(1.5723, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 101 \tLoss: tensor(1.6002, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 102 \tLoss: tensor(1.5851, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 103 \tLoss: tensor(1.5586, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 104 \tLoss: tensor(1.5900, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 105 \tLoss: tensor(1.5773, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 106 \tLoss: tensor(1.5912, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 107 \tLoss: tensor(1.5690, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 108 \tLoss: tensor(1.5835, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 109 \tLoss: tensor(1.5828, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 110 \tLoss: tensor(1.6072, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 111 \tLoss: tensor(1.5748, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 112 \tLoss: tensor(1.5783, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 113 \tLoss: tensor(1.5788, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 114 \tLoss: tensor(1.6029, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 115 \tLoss: tensor(1.5988, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 116 \tLoss: tensor(1.5741, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 117 \tLoss: tensor(1.5713, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 118 \tLoss: tensor(1.6011, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 119 \tLoss: tensor(1.5654, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 120 \tLoss: tensor(1.6043, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 121 \tLoss: tensor(1.5570, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 122 \tLoss: tensor(1.5695, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 123 \tLoss: tensor(1.5497, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 124 \tLoss: tensor(1.5895, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 125 \tLoss: tensor(1.6060, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 126 \tLoss: tensor(1.5338, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 127 \tLoss: tensor(1.5822, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 128 \tLoss: tensor(1.5510, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 129 \tLoss: tensor(1.5529, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 130 \tLoss: tensor(1.6055, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 131 \tLoss: tensor(1.5721, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 132 \tLoss: tensor(1.5890, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 133 \tLoss: tensor(1.5950, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 134 \tLoss: tensor(1.5802, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 135 \tLoss: tensor(1.5416, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 136 \tLoss: tensor(1.5713, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 137 \tLoss: tensor(1.5917, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 138 \tLoss: tensor(1.6010, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 139 \tLoss: tensor(1.5545, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 140 \tLoss: tensor(1.6223, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 141 \tLoss: tensor(1.5685, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 142 \tLoss: tensor(1.5547, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 143 \tLoss: tensor(1.5885, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 144 \tLoss: tensor(1.5869, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 145 \tLoss: tensor(1.5959, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 146 \tLoss: tensor(1.5628, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 147 \tLoss: tensor(1.5813, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 148 \tLoss: tensor(1.5741, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 149 \tLoss: tensor(1.5817, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 150 \tLoss: tensor(1.5558, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 151 \tLoss: tensor(1.5778, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 152 \tLoss: tensor(1.6065, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 153 \tLoss: tensor(1.5468, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 154 \tLoss: tensor(1.5718, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 155 \tLoss: tensor(1.5756, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 156 \tLoss: tensor(1.5929, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 157 \tLoss: tensor(1.5328, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 158 \tLoss: tensor(1.6229, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 159 \tLoss: tensor(1.5816, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 160 \tLoss: tensor(1.5956, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 161 \tLoss: tensor(1.5893, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 162 \tLoss: tensor(1.5677, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 163 \tLoss: tensor(1.6203, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 164 \tLoss: tensor(1.5610, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 165 \tLoss: tensor(1.5663, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 166 \tLoss: tensor(1.5636, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 167 \tLoss: tensor(1.5761, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 168 \tLoss: tensor(1.5633, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 169 \tLoss: tensor(1.5661, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 170 \tLoss: tensor(1.6244, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 171 \tLoss: tensor(1.5938, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 172 \tLoss: tensor(1.5941, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 173 \tLoss: tensor(1.5636, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 174 \tLoss: tensor(1.5860, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 175 \tLoss: tensor(1.5979, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 176 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 177 \tLoss: tensor(1.6184, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 178 \tLoss: tensor(1.6177, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 179 \tLoss: tensor(1.5773, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 180 \tLoss: tensor(1.5647, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 181 \tLoss: tensor(1.5817, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 182 \tLoss: tensor(1.6013, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 183 \tLoss: tensor(1.5175, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 184 \tLoss: tensor(1.5894, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 185 \tLoss: tensor(1.5766, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 186 \tLoss: tensor(1.6076, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 187 \tLoss: tensor(1.5595, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 188 \tLoss: tensor(1.5500, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 189 \tLoss: tensor(1.5719, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 190 \tLoss: tensor(1.5753, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 191 \tLoss: tensor(1.5734, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 192 \tLoss: tensor(1.6080, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 193 \tLoss: tensor(1.6292, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 194 \tLoss: tensor(1.5401, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 195 \tLoss: tensor(1.6043, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 196 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 197 \tLoss: tensor(1.5814, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 198 \tLoss: tensor(1.5893, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 199 \tLoss: tensor(1.5665, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 200 \tLoss: tensor(1.6019, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 201 \tLoss: tensor(1.5768, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 202 \tLoss: tensor(1.5733, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 203 \tLoss: tensor(1.6186, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 204 \tLoss: tensor(1.5911, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 205 \tLoss: tensor(1.6070, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 206 \tLoss: tensor(1.6323, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 207 \tLoss: tensor(1.6136, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 208 \tLoss: tensor(1.5422, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 209 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 210 \tLoss: tensor(1.6035, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 211 \tLoss: tensor(1.5532, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 212 \tLoss: tensor(1.6439, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 213 \tLoss: tensor(1.5953, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 214 \tLoss: tensor(1.5778, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 215 \tLoss: tensor(1.5675, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 216 \tLoss: tensor(1.5692, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 217 \tLoss: tensor(1.6115, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 218 \tLoss: tensor(1.6082, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 219 \tLoss: tensor(1.5856, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 220 \tLoss: tensor(1.5855, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 221 \tLoss: tensor(1.5926, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 222 \tLoss: tensor(1.5760, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 223 \tLoss: tensor(1.5438, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 224 \tLoss: tensor(1.5576, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 225 \tLoss: tensor(1.5901, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 226 \tLoss: tensor(1.5636, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 227 \tLoss: tensor(1.6268, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 228 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 229 \tLoss: tensor(1.5421, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 230 \tLoss: tensor(1.5984, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 231 \tLoss: tensor(1.5734, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 232 \tLoss: tensor(1.5921, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 233 \tLoss: tensor(1.5745, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 234 \tLoss: tensor(1.5981, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 235 \tLoss: tensor(1.5443, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 236 \tLoss: tensor(1.5647, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 237 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 238 \tLoss: tensor(1.5196, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 239 \tLoss: tensor(1.5571, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 240 \tLoss: tensor(1.5492, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 241 \tLoss: tensor(1.5948, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 242 \tLoss: tensor(1.6316, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 243 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 244 \tLoss: tensor(1.5994, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 245 \tLoss: tensor(1.5690, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 246 \tLoss: tensor(1.5765, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 247 \tLoss: tensor(1.5350, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 248 \tLoss: tensor(1.5592, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 249 \tLoss: tensor(1.6034, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 250 \tLoss: tensor(1.5958, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 251 \tLoss: tensor(1.5410, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 252 \tLoss: tensor(1.5683, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 253 \tLoss: tensor(1.5866, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 254 \tLoss: tensor(1.5675, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 255 \tLoss: tensor(1.5296, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 256 \tLoss: tensor(1.6057, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 257 \tLoss: tensor(1.5880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 258 \tLoss: tensor(1.5718, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 259 \tLoss: tensor(1.5825, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 260 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 261 \tLoss: tensor(1.5546, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 262 \tLoss: tensor(1.5716, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 263 \tLoss: tensor(1.5846, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 264 \tLoss: tensor(1.5795, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 265 \tLoss: tensor(1.5953, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 266 \tLoss: tensor(1.6153, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 267 \tLoss: tensor(1.5915, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 268 \tLoss: tensor(1.5645, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 269 \tLoss: tensor(1.5644, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 270 \tLoss: tensor(1.5822, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 271 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 272 \tLoss: tensor(1.5446, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 273 \tLoss: tensor(1.5441, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 274 \tLoss: tensor(1.5766, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 275 \tLoss: tensor(1.5580, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 276 \tLoss: tensor(1.5485, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 277 \tLoss: tensor(1.5493, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 278 \tLoss: tensor(1.5574, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 279 \tLoss: tensor(1.6027, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 280 \tLoss: tensor(1.6095, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 281 \tLoss: tensor(1.6149, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 282 \tLoss: tensor(1.5549, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 283 \tLoss: tensor(1.5873, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 284 \tLoss: tensor(1.5689, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 285 \tLoss: tensor(1.6000, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 286 \tLoss: tensor(1.5469, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 287 \tLoss: tensor(1.5880, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 288 \tLoss: tensor(1.5455, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 289 \tLoss: tensor(1.5887, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 290 \tLoss: tensor(1.5465, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 291 \tLoss: tensor(1.5869, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 292 \tLoss: tensor(1.5862, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 293 \tLoss: tensor(1.5675, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 294 \tLoss: tensor(1.5907, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 295 \tLoss: tensor(1.5434, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 296 \tLoss: tensor(1.5435, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 297 \tLoss: tensor(1.5500, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 298 \tLoss: tensor(1.5335, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 299 \tLoss: tensor(1.5753, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 300 \tLoss: tensor(1.5575, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 301 \tLoss: tensor(1.5968, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 302 \tLoss: tensor(1.5962, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 303 \tLoss: tensor(1.5377, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 304 \tLoss: tensor(1.5166, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 305 \tLoss: tensor(1.6007, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 306 \tLoss: tensor(1.5961, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 307 \tLoss: tensor(1.5668, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 308 \tLoss: tensor(1.5637, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 309 \tLoss: tensor(1.5654, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 310 \tLoss: tensor(1.5784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 311 \tLoss: tensor(1.5797, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 312 \tLoss: tensor(1.5894, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 313 \tLoss: tensor(1.5619, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 314 \tLoss: tensor(1.5757, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 315 \tLoss: tensor(1.5661, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 316 \tLoss: tensor(1.5785, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 317 \tLoss: tensor(1.5713, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 318 \tLoss: tensor(1.5529, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 319 \tLoss: tensor(1.5712, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 320 \tLoss: tensor(1.5442, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 321 \tLoss: tensor(1.6375, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 322 \tLoss: tensor(1.5671, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 323 \tLoss: tensor(1.5570, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 324 \tLoss: tensor(1.5794, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 325 \tLoss: tensor(1.5391, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 326 \tLoss: tensor(1.5565, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 327 \tLoss: tensor(1.5705, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 328 \tLoss: tensor(1.5696, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 329 \tLoss: tensor(1.5653, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 330 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 331 \tLoss: tensor(1.5737, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 332 \tLoss: tensor(1.6091, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 333 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 334 \tLoss: tensor(1.5649, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 335 \tLoss: tensor(1.5644, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 336 \tLoss: tensor(1.5716, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 337 \tLoss: tensor(1.6162, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 338 \tLoss: tensor(1.5904, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 339 \tLoss: tensor(1.5684, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 340 \tLoss: tensor(1.5572, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 341 \tLoss: tensor(1.5721, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 342 \tLoss: tensor(1.5413, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 343 \tLoss: tensor(1.5837, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 344 \tLoss: tensor(1.5428, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 345 \tLoss: tensor(1.5673, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 346 \tLoss: tensor(1.5619, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 347 \tLoss: tensor(1.5501, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 348 \tLoss: tensor(1.5983, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 349 \tLoss: tensor(1.5657, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 350 \tLoss: tensor(1.6019, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 351 \tLoss: tensor(1.5802, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 352 \tLoss: tensor(1.5302, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 353 \tLoss: tensor(1.5936, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 354 \tLoss: tensor(1.5649, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 355 \tLoss: tensor(1.5975, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 356 \tLoss: tensor(1.5610, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 357 \tLoss: tensor(1.5554, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 358 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 359 \tLoss: tensor(1.5636, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 360 \tLoss: tensor(1.5744, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 361 \tLoss: tensor(1.5311, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 362 \tLoss: tensor(1.5369, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 363 \tLoss: tensor(1.5981, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 364 \tLoss: tensor(1.5691, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 365 \tLoss: tensor(1.6085, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 366 \tLoss: tensor(1.5714, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 367 \tLoss: tensor(1.5374, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 368 \tLoss: tensor(1.6102, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 369 \tLoss: tensor(1.5873, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 370 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 371 \tLoss: tensor(1.5695, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 372 \tLoss: tensor(1.5589, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 373 \tLoss: tensor(1.5717, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 374 \tLoss: tensor(1.5907, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 375 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 376 \tLoss: tensor(1.5884, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 377 \tLoss: tensor(1.5784, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 378 \tLoss: tensor(1.5514, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 379 \tLoss: tensor(1.5777, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 380 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 381 \tLoss: tensor(1.5603, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 382 \tLoss: tensor(1.5970, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 383 \tLoss: tensor(1.5638, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 384 \tLoss: tensor(1.5283, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 385 \tLoss: tensor(1.6292, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 386 \tLoss: tensor(1.5921, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 387 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 388 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 389 \tLoss: tensor(1.5411, grad_fn=<NllLossBackward>)\n",
            "Epoch: 1 \tBatch: 390 \tLoss: tensor(1.5903, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9klR6wREMmn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9c0d57ef-4640-4618-90ff-ccc2f1497b77"
      },
      "source": [
        "def calc_accuracy(model, dataloader):\n",
        "    num_correct = 0\n",
        "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
        "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
        "        predictions = model(inputs)\n",
        "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
        "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
        "        num_correct += int(sum(predictions == labels))\n",
        "    percent_correct = num_correct / num_examples * 100\n",
        "    return percent_correct\n",
        "\n",
        "\n",
        "print('Train Accuracy:', calc_accuracy(myNeuralNetworkDropout, train_loader))\n",
        "print('Validation Accuracy:', calc_accuracy(myNeuralNetworkDropout, val_loader))\n",
        "print('Test Accuracy:', calc_accuracy(myNeuralNetworkDropout, test_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 90.75800000000001\n",
            "Validation Accuracy: 90.86999999999999\n",
            "Test Accuracy: 43.580000000000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSYOGP4bBy4B",
        "colab_type": "text"
      },
      "source": [
        "### 5. Summary\n",
        "***List the key points like:***\n",
        "\n",
        "* dropout definition in a sentence\n",
        "* ensemble method\n",
        "* how to overcome overfitting after applying dropout, show model performance metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBQ_028yBy4C",
        "colab_type": "text"
      },
      "source": [
        "### 6. What to do next?\n",
        "* Challenge about dropout, search on net, 3 example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQETT-qaBy4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD2L1lkgBy4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}