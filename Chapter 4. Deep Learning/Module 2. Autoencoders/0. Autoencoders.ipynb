{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Architecture of Simple Autoencoder\n",
    "- Applications\n",
    "- Low-level Implementation in PyTorch (MNIST Digits dataset from torchvision for reconstruction)\n",
    "- Visualize the Latent Space\n",
    "- Different Types of Autoencoders: Convolutional, Variational, Deep, Denoising (Short overview - maybe more detailed description and implementation in following submodules)\n",
    "- High-level Implementation with OpenNMT for Machine Translation (Denoising AE) - maybe do this in another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Neural Networks (FFN)\n",
    "- Feature Extraction\n",
    "- Dimensionality Reduction\n",
    "- Data Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "An autoencoder is a type of neural network that learns a representation of the input. Both __output__ _r_ and __input__ _x_ therefore have the same dimensions. However, the output representation _r_ is not a straightforward copy of the input _x_. The autoencoder learns to represent the input by reducing its dimensions to a smaller size, also called __code__ _h_. This code contains the most important features that are needed to represent the input correctly. Hence an autoencoder is an __unsupervised__ feature learner, which learns the code _h_ using __mapping functions__ _f(x)_ and *g(h)*, also called the **encoder** and **decoder** respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "<img src=\"images/simple_AE.png\" title=\"Simple Autoencoder Architecture\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "Feature extraction, dimensionality reduction, machine translation, image reconstruction/denoising/generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Pytorch, MNIST, Undercomplete AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space\n",
    "Examine features in the bottle neck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Types of Autoencoders\n",
    "Variational, Convolutional, Denoising, Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Why is it better than other dimensionality reduction techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
