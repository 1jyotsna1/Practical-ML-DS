{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit2b4c68a66e834f80ac62a395bc4bcb45",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Optimisation for Deep Learning\n",
    "\n",
    "Learning outcomes\n",
    "- understand mathematically and intuitively the most common optimisation algorithms used for optimising deep models\n",
    "- implement your own optimiser in PyTorch\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient descent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SGD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## AdaGrad\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RMSProp\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Adam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## So which algorithm do I use?\n",
    "\n",
    "Well... as usual, it depends on your problem and your dataset.\n",
    "\n",
    "It's still a highly active field of research. But in general, **SGD with momentum or Adam** are the go to choices for optimising deep models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using these optimisation algorithms\n",
    "\n",
    "Let's set up the same neural network as in the previous module, and then switch out the optimiser for Adam and others and show how you can adapt it to use momentum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import NN, get_dataloaders\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "my_nn = NN([784, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "# HOW TO USE DIFFERENT OPTIMISERS PROVIDED BY PYTORCH\n",
    "optimiser = torch.optim.SGD(my_nn.parameters(), lr=learning_rate, momentum=0.1)\n",
    "# optimiser = torch.optim.Adagrad(NN.parameters(), lr=learning_rate)\n",
    "# optimiser = torch.optim.RMSprop(NN.parameters(), lr=learning_rate)\n",
    "optimiser = torch.optim.Adam(my_nn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "source": [
    "The stuff below is exactly the same as before!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "kward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 478 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 479 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 480 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 481 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 482 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 483 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 484 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 485 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 486 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 487 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 488 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 489 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 490 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 491 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 492 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 493 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 494 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 495 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 496 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 497 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 498 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 499 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 500 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 501 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 502 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 503 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 504 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 505 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 506 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 507 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 508 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 509 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 510 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 511 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 512 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 513 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 514 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 515 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 516 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 517 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 518 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 519 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 520 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 521 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 522 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 523 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 524 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 525 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 526 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 527 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 528 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 529 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 530 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 531 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 532 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 533 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 534 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 535 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 536 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 537 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 538 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 539 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 540 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 541 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 542 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 543 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 544 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 545 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 546 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 547 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 548 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 549 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 550 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 551 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 552 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 553 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 554 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 555 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 556 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 557 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 558 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 559 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 560 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 561 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 562 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 563 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 564 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 565 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 566 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 567 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 568 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 569 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 570 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 571 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 572 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 573 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 574 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 575 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 576 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 577 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 578 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 579 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 580 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 581 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 582 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 583 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 584 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 585 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 586 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 587 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 588 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 589 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 590 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 591 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 592 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 593 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 594 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 595 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 596 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 597 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 598 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 599 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 600 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 601 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 602 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 603 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 604 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 605 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 606 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 607 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 608 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 609 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 610 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 611 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 612 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 613 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 614 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 615 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 616 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 617 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 618 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 619 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 620 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 621 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 622 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 623 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\ntorch.Size([16, 1, 28, 28])\ntorch.Size([16, 10])\ntorch.Size([16])\nEpoch: 0 \tBatch: 624 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\n"
    }
   ],
   "source": [
    "# GET DATALOADERS\n",
    "test_loader, val_loader, train_loader = get_dataloaders()\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='../../runs')                            # we will use this to show our models performance on a graph\n",
    "\n",
    "# TRAINING LOOP\n",
    "def train(model, optimiser, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            print(inputs.shape)\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            print(prediction.shape)\n",
    "            print(labels.shape)\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "train(my_nn, optimiser)"
   ]
  },
  {
   "source": [
    "Let's visualise the training curves using some of the optimisers that we explained above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optmiser):\n",
    "    "
   ]
  },
  {
   "source": [
    "## Implementing our own PyTorch optimiser\n",
    "\n",
    "To understand a bit further what's happening under the hood, let's implement SGD from scratch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, model_params, learning_rate):\n",
    "        self.model_params = model_params\n",
    "        self.learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.model_params:\n",
    "            param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.model_params:\n",
    "            param.grad = torch.zeros_like(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = NN()\n",
    "optimiser = SGD()\n",
    "\n",
    "train(my_nn, optimiser)"
   ]
  }
 ]
}