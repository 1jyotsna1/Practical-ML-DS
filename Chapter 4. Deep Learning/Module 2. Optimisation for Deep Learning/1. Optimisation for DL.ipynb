{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('main': conda)",
   "display_name": "Python 3.8.5 64-bit ('main': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06c1e258a470a687113bfba03f207c092b27379067ada2d83b8b31269ab641fe"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Optimisation for Deep Learning\n",
    "\n",
    "Learning outcomes\n",
    "- understand mathematically and intuitively the most common optimisation algorithms used for optimising deep models\n",
    "- implement your own optimiser in PyTorch\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Challenges with optimising deep models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient Descent\n",
    "\n",
    "![](images/gradient_descent.jpg)\n",
    "\n",
    "## SGD\n",
    "\n",
    "![](images/SGD.jpg)\n",
    "\n",
    "## SGD with momentum\n",
    "\n",
    "![](images/momentum.jpg)\n",
    "\n",
    "## SGD with Nesterov momentum\n",
    "\n",
    "![](images/nesterov.jpg)\n",
    "\n",
    "## AdaGrad\n",
    "\n",
    "Is there a more systematic way to reduce the learning rate over time?\n",
    "\n",
    "AdaGrad assumes so, and reduces the learning rate\n",
    "\n",
    "![](images/adagrad.jpg)\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "The problem with AdaGrad is that the learning rate can never recover and increase to speed up optimisation once it has slowed down, it can only decrease further. So if a steep part of the loss surface is encountered before a flatter part, the learning rate for this parameter will be divided by the large loss surface gradient in the steep region and be too small to make meaningful progress in the flatter region.\n",
    "\n",
    "RMSProp is similar to AdaGrad except for how it accumulates the gradient to decay the learning rate for each parameter. Instead of continuuing to sum up the square of all of the gradients encountered in each given direction, it takes an *exponential moving average*. This gives the chance for the learning rate to increase if a steep gradient were not encountered recently, as the historical gradients encountered have an exponentially smaller influence on the learning rate with each optimisation step.\n",
    "\n",
    "![](images/rmsprop.jpg)\n",
    "\n",
    "## Adam\n",
    "\n",
    "![](images/adam.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## So which algorithm do I use?\n",
    "\n",
    "Well... as usual, it depends on your problem and your dataset.\n",
    "\n",
    "It's still a highly active field of research. But in general, **SGD with momentum or Adam** are the go to choices for optimising deep models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using these optimisation algorithms\n",
    "\n",
    "Let's set up the same neural network as in the previous module, and then switch out the optimiser for Adam and others and show how you can adapt it to use momentum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import NN, get_dataloaders\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# HOW TO USE DIFFERENT OPTIMISERS PROVIDED BY PYTORCH\n",
    "optimiser = torch.optim.SGD(my_nn.parameters(), lr=learning_rate, momentum=0.1)\n",
    "# optimiser = torch.optim.Adagrad(NN.parameters(), lr=learning_rate)\n",
    "# optimiser = torch.optim.RMSprop(NN.parameters(), lr=learning_rate)\n",
    "optimiser = torch.optim.Adam(my_nn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "source": [
    "The stuff below is exactly the same as before!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "100.1%Extracting MNIST-data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST-data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "113.5%Extracting MNIST-data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "100.4%Extracting MNIST-data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST-data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "180.4%Extracting MNIST-data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST-data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# GET DATALOADERS\n",
    "test_loader, val_loader, train_loader = get_dataloaders()\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TRAINING LOOP\n",
    "def train(model, optimiser, graph_name, epochs=1, tag='Loss/Train'):\n",
    "    writer = SummaryWriter(log_dir=f'../../runs/{tag}') # make a different writer for each tagged optimisation run\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar(f'Loss/{graph_name}', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "# train(my_nn, optimiser)"
   ]
  },
  {
   "source": [
    "Let's compare the training curves generated using some of the optimisers that we explained above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " \tLoss: tensor(2.1702, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 340 \tLoss: tensor(2.1566, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 341 \tLoss: tensor(2.1759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 342 \tLoss: tensor(2.0939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 343 \tLoss: tensor(2.0399, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 344 \tLoss: tensor(2.1666, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 345 \tLoss: tensor(2.1858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 346 \tLoss: tensor(2.1874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 347 \tLoss: tensor(2.1195, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 348 \tLoss: tensor(2.0071, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 349 \tLoss: tensor(2.1568, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 350 \tLoss: tensor(2.1484, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 351 \tLoss: tensor(2.1725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 352 \tLoss: tensor(2.0955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 353 \tLoss: tensor(2.0881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 354 \tLoss: tensor(2.1031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 355 \tLoss: tensor(2.1536, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 356 \tLoss: tensor(2.0849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 357 \tLoss: tensor(2.1607, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 358 \tLoss: tensor(2.0035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 359 \tLoss: tensor(2.1063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 360 \tLoss: tensor(2.1384, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 361 \tLoss: tensor(2.0944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 362 \tLoss: tensor(2.1385, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 363 \tLoss: tensor(2.2024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 364 \tLoss: tensor(2.1196, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 365 \tLoss: tensor(2.0837, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 366 \tLoss: tensor(2.1457, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 367 \tLoss: tensor(2.1957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 368 \tLoss: tensor(2.0733, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 369 \tLoss: tensor(2.0837, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 370 \tLoss: tensor(2.1182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 371 \tLoss: tensor(2.0920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 372 \tLoss: tensor(2.1845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 373 \tLoss: tensor(2.1306, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 374 \tLoss: tensor(2.0544, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 375 \tLoss: tensor(2.0401, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 376 \tLoss: tensor(2.1173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 377 \tLoss: tensor(2.0828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 378 \tLoss: tensor(2.1891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 379 \tLoss: tensor(1.9948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 380 \tLoss: tensor(2.1375, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 381 \tLoss: tensor(2.0742, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 382 \tLoss: tensor(2.0800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 383 \tLoss: tensor(2.1739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 384 \tLoss: tensor(2.0181, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 385 \tLoss: tensor(2.0662, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 386 \tLoss: tensor(2.1793, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 387 \tLoss: tensor(2.0318, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 388 \tLoss: tensor(2.0392, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 389 \tLoss: tensor(2.1631, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 390 \tLoss: tensor(2.1218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 391 \tLoss: tensor(2.0536, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 392 \tLoss: tensor(2.0279, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 393 \tLoss: tensor(2.0169, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 394 \tLoss: tensor(2.0874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 395 \tLoss: tensor(2.0780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 396 \tLoss: tensor(1.9988, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 397 \tLoss: tensor(2.0761, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 398 \tLoss: tensor(2.0997, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 399 \tLoss: tensor(2.1248, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 400 \tLoss: tensor(2.1530, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 401 \tLoss: tensor(2.0772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 402 \tLoss: tensor(2.1452, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 403 \tLoss: tensor(2.0496, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 404 \tLoss: tensor(1.9897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 405 \tLoss: tensor(1.9770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 406 \tLoss: tensor(2.1187, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 407 \tLoss: tensor(2.1221, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 408 \tLoss: tensor(2.0936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 409 \tLoss: tensor(2.1266, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 410 \tLoss: tensor(1.9929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 411 \tLoss: tensor(1.9910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 412 \tLoss: tensor(2.0417, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 413 \tLoss: tensor(2.0435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 414 \tLoss: tensor(2.0956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 415 \tLoss: tensor(2.1301, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 416 \tLoss: tensor(2.0795, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 417 \tLoss: tensor(2.0278, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 418 \tLoss: tensor(2.1742, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 419 \tLoss: tensor(2.0748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 420 \tLoss: tensor(2.0427, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 421 \tLoss: tensor(1.9705, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 422 \tLoss: tensor(2.0154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 423 \tLoss: tensor(2.0360, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 424 \tLoss: tensor(2.0652, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 425 \tLoss: tensor(2.0690, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 426 \tLoss: tensor(2.1437, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 427 \tLoss: tensor(2.0671, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 428 \tLoss: tensor(2.1542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 429 \tLoss: tensor(2.1058, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 430 \tLoss: tensor(1.9634, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 431 \tLoss: tensor(2.1125, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 432 \tLoss: tensor(2.0813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 433 \tLoss: tensor(2.1287, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 434 \tLoss: tensor(2.1701, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 435 \tLoss: tensor(2.0116, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 436 \tLoss: tensor(1.9960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 437 \tLoss: tensor(2.0853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 438 \tLoss: tensor(2.1285, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 439 \tLoss: tensor(2.1198, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 440 \tLoss: tensor(1.9769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 441 \tLoss: tensor(1.9377, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 442 \tLoss: tensor(2.0259, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 443 \tLoss: tensor(1.9942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 444 \tLoss: tensor(2.0333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 445 \tLoss: tensor(2.0447, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 446 \tLoss: tensor(2.0845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 447 \tLoss: tensor(2.0067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 448 \tLoss: tensor(1.9966, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 449 \tLoss: tensor(2.1854, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 450 \tLoss: tensor(2.0592, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 451 \tLoss: tensor(2.0184, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 452 \tLoss: tensor(2.0248, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 453 \tLoss: tensor(2.0038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 454 \tLoss: tensor(2.0939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 455 \tLoss: tensor(1.8866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 456 \tLoss: tensor(1.9255, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 457 \tLoss: tensor(1.9950, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 458 \tLoss: tensor(2.0878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 459 \tLoss: tensor(1.9932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 460 \tLoss: tensor(2.1625, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 461 \tLoss: tensor(2.0141, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 462 \tLoss: tensor(2.0426, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 463 \tLoss: tensor(2.0594, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 464 \tLoss: tensor(1.9862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 465 \tLoss: tensor(1.9529, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 466 \tLoss: tensor(1.9860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 467 \tLoss: tensor(1.9555, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 468 \tLoss: tensor(2.0453, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 469 \tLoss: tensor(2.0184, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 470 \tLoss: tensor(1.9756, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 471 \tLoss: tensor(2.0217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 472 \tLoss: tensor(2.0423, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 473 \tLoss: tensor(1.9843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 474 \tLoss: tensor(2.0114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 475 \tLoss: tensor(2.0801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 476 \tLoss: tensor(2.1073, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 477 \tLoss: tensor(2.0437, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 478 \tLoss: tensor(2.1138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 479 \tLoss: tensor(2.0566, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 480 \tLoss: tensor(2.1182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 481 \tLoss: tensor(2.0725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 482 \tLoss: tensor(2.0032, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 483 \tLoss: tensor(1.9677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 484 \tLoss: tensor(2.0010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 485 \tLoss: tensor(2.0719, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 486 \tLoss: tensor(1.9054, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 487 \tLoss: tensor(2.0819, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 488 \tLoss: tensor(2.0083, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 489 \tLoss: tensor(2.0148, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 490 \tLoss: tensor(1.9060, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 491 \tLoss: tensor(2.1462, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 492 \tLoss: tensor(1.9261, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 493 \tLoss: tensor(1.9155, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 494 \tLoss: tensor(1.9388, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 495 \tLoss: tensor(1.9817, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 496 \tLoss: tensor(1.9664, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 497 \tLoss: tensor(1.9841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 498 \tLoss: tensor(2.0723, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 499 \tLoss: tensor(1.9858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 500 \tLoss: tensor(1.9408, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 501 \tLoss: tensor(2.0066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 502 \tLoss: tensor(1.9486, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 503 \tLoss: tensor(2.0083, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 504 \tLoss: tensor(1.9927, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 505 \tLoss: tensor(2.0655, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 506 \tLoss: tensor(1.9189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 507 \tLoss: tensor(2.0762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 508 \tLoss: tensor(2.0319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 509 \tLoss: tensor(1.9473, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 510 \tLoss: tensor(2.1334, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 511 \tLoss: tensor(1.8808, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 512 \tLoss: tensor(2.0786, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 513 \tLoss: tensor(2.0427, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 514 \tLoss: tensor(2.1466, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 515 \tLoss: tensor(2.0400, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 516 \tLoss: tensor(2.0278, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 517 \tLoss: tensor(1.9765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 518 \tLoss: tensor(1.8443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 519 \tLoss: tensor(2.0107, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 520 \tLoss: tensor(1.9959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 521 \tLoss: tensor(1.9321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 522 \tLoss: tensor(1.8835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 523 \tLoss: tensor(2.0896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 524 \tLoss: tensor(2.0076, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 525 \tLoss: tensor(1.8998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 526 \tLoss: tensor(2.1301, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 527 \tLoss: tensor(2.0101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 528 \tLoss: tensor(1.9318, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 529 \tLoss: tensor(2.1238, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 530 \tLoss: tensor(1.8905, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 531 \tLoss: tensor(2.1233, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 532 \tLoss: tensor(2.0230, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 533 \tLoss: tensor(1.9939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 534 \tLoss: tensor(1.9485, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 535 \tLoss: tensor(1.8431, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 536 \tLoss: tensor(1.9791, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 537 \tLoss: tensor(1.9103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 538 \tLoss: tensor(2.0349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 539 \tLoss: tensor(1.8983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 540 \tLoss: tensor(1.8803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 541 \tLoss: tensor(1.8222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 542 \tLoss: tensor(1.9025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 543 \tLoss: tensor(1.8861, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 544 \tLoss: tensor(1.9900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 545 \tLoss: tensor(2.0331, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 546 \tLoss: tensor(1.7914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 547 \tLoss: tensor(1.9235, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 548 \tLoss: tensor(1.9217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 549 \tLoss: tensor(1.8111, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 550 \tLoss: tensor(1.9853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 551 \tLoss: tensor(2.1351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 552 \tLoss: tensor(2.1821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 553 \tLoss: tensor(2.0697, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 554 \tLoss: tensor(2.0126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 555 \tLoss: tensor(1.8849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 556 \tLoss: tensor(1.9782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 557 \tLoss: tensor(1.9678, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 558 \tLoss: tensor(1.9598, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 559 \tLoss: tensor(2.0660, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 560 \tLoss: tensor(2.0439, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 561 \tLoss: tensor(1.9073, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 562 \tLoss: tensor(1.9443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 563 \tLoss: tensor(2.0100, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 564 \tLoss: tensor(1.9563, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 565 \tLoss: tensor(1.8711, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 566 \tLoss: tensor(2.0151, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 567 \tLoss: tensor(2.0096, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 568 \tLoss: tensor(1.7912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 569 \tLoss: tensor(1.9868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 570 \tLoss: tensor(1.9647, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 571 \tLoss: tensor(1.9677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 572 \tLoss: tensor(2.0229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 573 \tLoss: tensor(2.0319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 574 \tLoss: tensor(2.0249, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 575 \tLoss: tensor(1.9097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 576 \tLoss: tensor(1.9710, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 577 \tLoss: tensor(1.9189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 578 \tLoss: tensor(1.8550, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 579 \tLoss: tensor(2.0677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 580 \tLoss: tensor(2.0595, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 581 \tLoss: tensor(1.9660, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 582 \tLoss: tensor(1.9773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 583 \tLoss: tensor(1.8089, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 584 \tLoss: tensor(1.9368, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 585 \tLoss: tensor(1.8975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 586 \tLoss: tensor(2.0667, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 587 \tLoss: tensor(1.8805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 588 \tLoss: tensor(1.9463, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 589 \tLoss: tensor(2.0204, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 590 \tLoss: tensor(2.0704, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 591 \tLoss: tensor(2.0108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 592 \tLoss: tensor(1.9192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 593 \tLoss: tensor(1.9289, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 594 \tLoss: tensor(1.9957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 595 \tLoss: tensor(2.0390, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 596 \tLoss: tensor(1.9084, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 597 \tLoss: tensor(2.0311, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 598 \tLoss: tensor(1.9858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 599 \tLoss: tensor(1.9891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 600 \tLoss: tensor(1.9689, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 601 \tLoss: tensor(1.7952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 602 \tLoss: tensor(1.8607, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 603 \tLoss: tensor(1.9972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 604 \tLoss: tensor(1.9215, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 605 \tLoss: tensor(1.9002, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 606 \tLoss: tensor(1.9850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 607 \tLoss: tensor(1.9302, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 608 \tLoss: tensor(1.9351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 609 \tLoss: tensor(2.0356, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 610 \tLoss: tensor(1.9479, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 611 \tLoss: tensor(1.9525, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 612 \tLoss: tensor(1.8545, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 613 \tLoss: tensor(1.8061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 614 \tLoss: tensor(2.1439, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 615 \tLoss: tensor(1.8140, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 616 \tLoss: tensor(1.7823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 617 \tLoss: tensor(1.8543, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 618 \tLoss: tensor(1.9716, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 619 \tLoss: tensor(1.9593, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 620 \tLoss: tensor(1.8652, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 621 \tLoss: tensor(1.9624, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 622 \tLoss: tensor(1.9913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 623 \tLoss: tensor(1.9094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 624 \tLoss: tensor(2.0756, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimisers = [\n",
    "    {\n",
    "        'optimiser_class': torch.optim.SGD, \n",
    "        'tag': 'SGD'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adam,\n",
    "        'tag': 'Adam'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adagrad,\n",
    "        'tag': 'Adagrad'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.RMSprop,\n",
    "        'tag': 'RMSProp'\n",
    "    }\n",
    "]\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for optimiser_obj in optimisers:   \n",
    "    for lr in learning_rates:\n",
    "        my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "        optimiser_class = optimiser_obj['optimiser_class']\n",
    "        optimiser = optimiser_class(my_nn.parameters(), lr=lr)\n",
    "        tag = optimiser_obj['tag']\n",
    "        train(my_nn, optimiser, graph_name=lr, epochs=1, tag=f'Loss/Train/{tag}')\n",
    "    "
   ]
  },
  {
   "source": [
    "## Implementing our own PyTorch optimiser\n",
    "\n",
    "To understand a bit further what's happening under the hood, let's implement SGD from scratch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, model_params, learning_rate):\n",
    "        self.model_params = list(model_params) # HACK turning to list prevents len model_params being zero\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.model_params:\n",
    "                param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.model_params:\n",
    "            if param.grad is None: # if not yet set (loss.backward() not yet called)\n",
    "                print('continuing')\n",
    "                continue\n",
    "            param.grad = torch.zeros_like(param.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ckward&gt;)\nEpoch: 0 \tBatch: 362 \tLoss: tensor(2.2908, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 363 \tLoss: tensor(2.2891, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 364 \tLoss: tensor(2.2966, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 365 \tLoss: tensor(2.2943, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 366 \tLoss: tensor(2.2945, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 367 \tLoss: tensor(2.2896, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 368 \tLoss: tensor(2.2937, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 369 \tLoss: tensor(2.2968, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 370 \tLoss: tensor(2.2864, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 371 \tLoss: tensor(2.2943, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 372 \tLoss: tensor(2.2943, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 373 \tLoss: tensor(2.2934, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 374 \tLoss: tensor(2.2931, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 375 \tLoss: tensor(2.2960, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 376 \tLoss: tensor(2.2935, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 377 \tLoss: tensor(2.2957, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 378 \tLoss: tensor(2.2960, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 379 \tLoss: tensor(2.2907, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 380 \tLoss: tensor(2.2880, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 381 \tLoss: tensor(2.2934, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 382 \tLoss: tensor(2.2911, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 383 \tLoss: tensor(2.2895, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 384 \tLoss: tensor(2.2939, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 385 \tLoss: tensor(2.2942, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 386 \tLoss: tensor(2.2960, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 387 \tLoss: tensor(2.2875, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 388 \tLoss: tensor(2.2947, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 389 \tLoss: tensor(2.2901, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 390 \tLoss: tensor(2.2904, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 391 \tLoss: tensor(2.2877, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 392 \tLoss: tensor(2.2934, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 393 \tLoss: tensor(2.2923, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 394 \tLoss: tensor(2.2982, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 395 \tLoss: tensor(2.2927, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 396 \tLoss: tensor(2.2933, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 397 \tLoss: tensor(2.2890, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 398 \tLoss: tensor(2.2938, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 399 \tLoss: tensor(2.2879, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 400 \tLoss: tensor(2.2856, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 401 \tLoss: tensor(2.2909, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 402 \tLoss: tensor(2.2945, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 403 \tLoss: tensor(2.2929, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 404 \tLoss: tensor(2.2921, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 405 \tLoss: tensor(2.2863, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 406 \tLoss: tensor(2.2950, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 407 \tLoss: tensor(2.2940, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 408 \tLoss: tensor(2.2834, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 409 \tLoss: tensor(2.2835, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 410 \tLoss: tensor(2.2955, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 411 \tLoss: tensor(2.2883, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 412 \tLoss: tensor(2.2874, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 413 \tLoss: tensor(2.2906, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 414 \tLoss: tensor(2.2909, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 415 \tLoss: tensor(2.2945, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 416 \tLoss: tensor(2.2958, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 417 \tLoss: tensor(2.2927, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 418 \tLoss: tensor(2.2979, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 419 \tLoss: tensor(2.2851, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 420 \tLoss: tensor(2.2950, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 421 \tLoss: tensor(2.2966, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 422 \tLoss: tensor(2.2931, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 423 \tLoss: tensor(2.2842, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 424 \tLoss: tensor(2.2920, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 425 \tLoss: tensor(2.2859, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 426 \tLoss: tensor(2.2832, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 427 \tLoss: tensor(2.2889, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 428 \tLoss: tensor(2.2917, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 429 \tLoss: tensor(2.2938, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 430 \tLoss: tensor(2.2892, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 431 \tLoss: tensor(2.2865, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 432 \tLoss: tensor(2.2890, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 433 \tLoss: tensor(2.2964, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 434 \tLoss: tensor(2.2758, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 435 \tLoss: tensor(2.2869, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 436 \tLoss: tensor(2.2802, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 437 \tLoss: tensor(2.2899, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 438 \tLoss: tensor(2.2881, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 439 \tLoss: tensor(2.2887, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 440 \tLoss: tensor(2.2895, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 441 \tLoss: tensor(2.2847, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 442 \tLoss: tensor(2.2912, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 443 \tLoss: tensor(2.2910, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 444 \tLoss: tensor(2.2837, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 445 \tLoss: tensor(2.2924, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 446 \tLoss: tensor(2.2934, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 447 \tLoss: tensor(2.2913, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 448 \tLoss: tensor(2.2786, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 449 \tLoss: tensor(2.2845, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 450 \tLoss: tensor(2.2920, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 451 \tLoss: tensor(2.2838, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 452 \tLoss: tensor(2.2821, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 453 \tLoss: tensor(2.2855, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 454 \tLoss: tensor(2.2907, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 455 \tLoss: tensor(2.2837, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 456 \tLoss: tensor(2.2864, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 457 \tLoss: tensor(2.2778, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 458 \tLoss: tensor(2.2879, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 459 \tLoss: tensor(2.2844, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 460 \tLoss: tensor(2.2766, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 461 \tLoss: tensor(2.2646, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 462 \tLoss: tensor(2.2990, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 463 \tLoss: tensor(2.2905, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 464 \tLoss: tensor(2.2861, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 465 \tLoss: tensor(2.2840, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 466 \tLoss: tensor(2.2878, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 467 \tLoss: tensor(2.2933, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 468 \tLoss: tensor(2.2834, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 469 \tLoss: tensor(2.2896, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 470 \tLoss: tensor(2.2901, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 471 \tLoss: tensor(2.2799, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 472 \tLoss: tensor(2.2821, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 473 \tLoss: tensor(2.3001, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 474 \tLoss: tensor(2.2924, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 475 \tLoss: tensor(2.2830, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 476 \tLoss: tensor(2.2877, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 477 \tLoss: tensor(2.2861, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 478 \tLoss: tensor(2.2909, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 479 \tLoss: tensor(2.2761, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 480 \tLoss: tensor(2.2835, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 481 \tLoss: tensor(2.2783, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 482 \tLoss: tensor(2.2771, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 483 \tLoss: tensor(2.2717, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 484 \tLoss: tensor(2.2735, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 485 \tLoss: tensor(2.2686, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 486 \tLoss: tensor(2.2568, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 487 \tLoss: tensor(2.2792, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 488 \tLoss: tensor(2.2832, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 489 \tLoss: tensor(2.2792, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 490 \tLoss: tensor(2.2641, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 491 \tLoss: tensor(2.2744, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 492 \tLoss: tensor(2.2853, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 493 \tLoss: tensor(2.2862, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 494 \tLoss: tensor(2.2895, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 495 \tLoss: tensor(2.2714, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 496 \tLoss: tensor(2.2723, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 497 \tLoss: tensor(2.2735, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 498 \tLoss: tensor(2.2597, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 499 \tLoss: tensor(2.2323, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 500 \tLoss: tensor(2.2704, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 501 \tLoss: tensor(2.2521, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 502 \tLoss: tensor(2.2627, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 503 \tLoss: tensor(2.2572, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 504 \tLoss: tensor(2.2835, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 505 \tLoss: tensor(2.2281, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 506 \tLoss: tensor(2.3003, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 507 \tLoss: tensor(2.2916, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 508 \tLoss: tensor(2.2605, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 509 \tLoss: tensor(2.2735, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 510 \tLoss: tensor(2.2705, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 511 \tLoss: tensor(2.2729, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 512 \tLoss: tensor(2.2965, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 513 \tLoss: tensor(2.2672, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 514 \tLoss: tensor(2.2647, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 515 \tLoss: tensor(2.2508, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 516 \tLoss: tensor(2.2775, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 517 \tLoss: tensor(2.3011, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 518 \tLoss: tensor(2.2698, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 519 \tLoss: tensor(2.2629, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 520 \tLoss: tensor(2.2781, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 521 \tLoss: tensor(2.2829, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 522 \tLoss: tensor(2.2733, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 523 \tLoss: tensor(2.2794, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 524 \tLoss: tensor(2.2576, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 525 \tLoss: tensor(2.2222, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 526 \tLoss: tensor(2.1885, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 527 \tLoss: tensor(2.2935, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 528 \tLoss: tensor(2.2613, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 529 \tLoss: tensor(2.2572, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 530 \tLoss: tensor(2.2236, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 531 \tLoss: tensor(2.2131, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 532 \tLoss: tensor(2.2160, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 533 \tLoss: tensor(2.2959, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 534 \tLoss: tensor(2.1887, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 535 \tLoss: tensor(2.2809, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 536 \tLoss: tensor(2.2922, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 537 \tLoss: tensor(2.2128, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 538 \tLoss: tensor(2.2193, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 539 \tLoss: tensor(2.1926, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 540 \tLoss: tensor(2.2445, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 541 \tLoss: tensor(2.2389, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 542 \tLoss: tensor(2.1699, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 543 \tLoss: tensor(2.2872, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 544 \tLoss: tensor(2.2564, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 545 \tLoss: tensor(2.2636, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 546 \tLoss: tensor(2.2808, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 547 \tLoss: tensor(2.1949, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 548 \tLoss: tensor(2.2767, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 549 \tLoss: tensor(2.2460, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 550 \tLoss: tensor(2.2591, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 551 \tLoss: tensor(2.2456, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 552 \tLoss: tensor(2.2309, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 553 \tLoss: tensor(2.1889, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 554 \tLoss: tensor(2.1722, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 555 \tLoss: tensor(2.2938, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 556 \tLoss: tensor(2.2451, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 557 \tLoss: tensor(2.2479, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 558 \tLoss: tensor(2.2351, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 559 \tLoss: tensor(2.2320, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 560 \tLoss: tensor(2.2237, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 561 \tLoss: tensor(2.1879, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 562 \tLoss: tensor(2.2162, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 563 \tLoss: tensor(2.2583, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 564 \tLoss: tensor(2.2624, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 565 \tLoss: tensor(2.1858, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 566 \tLoss: tensor(2.1129, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 567 \tLoss: tensor(2.2464, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 568 \tLoss: tensor(2.1551, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 569 \tLoss: tensor(2.2914, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 570 \tLoss: tensor(2.1501, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 571 \tLoss: tensor(2.1413, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 572 \tLoss: tensor(2.1987, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 573 \tLoss: tensor(2.1555, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 574 \tLoss: tensor(2.2329, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 575 \tLoss: tensor(2.2424, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 576 \tLoss: tensor(2.2537, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 577 \tLoss: tensor(2.2116, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 578 \tLoss: tensor(2.2009, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 579 \tLoss: tensor(2.1913, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 580 \tLoss: tensor(2.1839, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 581 \tLoss: tensor(2.1807, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 582 \tLoss: tensor(2.2490, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 583 \tLoss: tensor(2.1927, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 584 \tLoss: tensor(2.1731, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 585 \tLoss: tensor(2.2256, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 586 \tLoss: tensor(2.2263, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 587 \tLoss: tensor(2.2090, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 588 \tLoss: tensor(2.2882, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 589 \tLoss: tensor(2.1534, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 590 \tLoss: tensor(2.2222, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 591 \tLoss: tensor(2.2318, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 592 \tLoss: tensor(2.1922, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 593 \tLoss: tensor(2.1560, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 594 \tLoss: tensor(2.2604, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 595 \tLoss: tensor(2.1649, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 596 \tLoss: tensor(2.2253, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 597 \tLoss: tensor(2.2774, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 598 \tLoss: tensor(2.2510, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 599 \tLoss: tensor(2.2076, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 600 \tLoss: tensor(2.1264, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 601 \tLoss: tensor(2.1582, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 602 \tLoss: tensor(2.1554, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 603 \tLoss: tensor(2.2735, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 604 \tLoss: tensor(2.1559, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 605 \tLoss: tensor(2.1093, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 606 \tLoss: tensor(2.1781, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 607 \tLoss: tensor(2.1941, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 608 \tLoss: tensor(2.2486, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 609 \tLoss: tensor(2.2477, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 610 \tLoss: tensor(2.2291, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 611 \tLoss: tensor(2.2628, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 612 \tLoss: tensor(2.2495, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 613 \tLoss: tensor(2.1905, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 614 \tLoss: tensor(2.2226, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 615 \tLoss: tensor(2.2404, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 616 \tLoss: tensor(2.2172, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 617 \tLoss: tensor(2.1860, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 618 \tLoss: tensor(2.1722, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 619 \tLoss: tensor(2.2538, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 620 \tLoss: tensor(2.1173, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 621 \tLoss: tensor(2.1811, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 622 \tLoss: tensor(2.2066, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 623 \tLoss: tensor(2.1569, grad_fn=&lt;NllLossBackward&gt;)\nEpoch: 0 \tBatch: 624 \tLoss: tensor(2.1549, grad_fn=&lt;NllLossBackward&gt;)\n"
    }
   ],
   "source": [
    "my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "optimiser = SGD(my_nn.parameters(), learning_rate=0.1)\n",
    "\n",
    "train(my_nn, optimiser, 'custom_sgd')"
   ]
  },
  {
   "source": [
    "## Challenges\n",
    "- flash card match images with name of optimisation algorithm\n",
    "- roughly sketch the paths that different optimisation algorithms might take"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}