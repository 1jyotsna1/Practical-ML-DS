{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit2b4c68a66e834f80ac62a395bc4bcb45",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Optimisation for Deep Learning\n",
    "\n",
    "Learning outcomes\n",
    "- understand mathematically and intuitively the most common optimisation algorithms used for optimising deep models\n",
    "- implement your own optimiser in PyTorch\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient descent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SGD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## AdaGrad\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RMSProp\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Adam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## So which algorithm do I use?\n",
    "\n",
    "Well... as usual, it depends on your problem and your dataset.\n",
    "\n",
    "It's still a highly active field of research. But in general, **SGD with momentum or Adam** are the go to choices for optimising deep models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using these optimisation algorithms\n",
    "\n",
    "Let's set up the same neural network as in the previous module, and then switch out the optimiser for Adam and others and show how you can adapt it to use momentum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import NN, get_dataloaders\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# HOW TO USE DIFFERENT OPTIMISERS PROVIDED BY PYTORCH\n",
    "optimiser = torch.optim.SGD(my_nn.parameters(), lr=learning_rate, momentum=0.1)\n",
    "# optimiser = torch.optim.Adagrad(NN.parameters(), lr=learning_rate)\n",
    "# optimiser = torch.optim.RMSprop(NN.parameters(), lr=learning_rate)\n",
    "optimiser = torch.optim.Adam(my_nn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "source": [
    "The stuff below is exactly the same as before!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " \tLoss: tensor(1.9378, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 340 \tLoss: tensor(1.5862, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 341 \tLoss: tensor(1.6386, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 342 \tLoss: tensor(1.8936, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 343 \tLoss: tensor(1.8180, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 344 \tLoss: tensor(1.9209, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 345 \tLoss: tensor(1.7717, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 346 \tLoss: tensor(1.7141, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 347 \tLoss: tensor(1.6657, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 348 \tLoss: tensor(1.5850, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 349 \tLoss: tensor(1.6648, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 350 \tLoss: tensor(1.6003, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 351 \tLoss: tensor(1.8465, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 352 \tLoss: tensor(1.6780, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 353 \tLoss: tensor(1.7759, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 354 \tLoss: tensor(1.6478, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 355 \tLoss: tensor(1.5726, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 356 \tLoss: tensor(1.7110, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 357 \tLoss: tensor(1.7090, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 358 \tLoss: tensor(1.7113, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 359 \tLoss: tensor(1.6489, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 360 \tLoss: tensor(1.6053, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 361 \tLoss: tensor(1.7157, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 362 \tLoss: tensor(1.7689, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 363 \tLoss: tensor(1.8213, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 364 \tLoss: tensor(1.7398, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 365 \tLoss: tensor(1.7114, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 366 \tLoss: tensor(1.5984, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 367 \tLoss: tensor(1.4641, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 368 \tLoss: tensor(1.9088, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 369 \tLoss: tensor(1.8350, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 370 \tLoss: tensor(1.8441, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 371 \tLoss: tensor(1.7218, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 372 \tLoss: tensor(1.5801, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 373 \tLoss: tensor(1.7369, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 374 \tLoss: tensor(1.5863, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 375 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 376 \tLoss: tensor(1.5229, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 377 \tLoss: tensor(1.5903, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 378 \tLoss: tensor(1.7184, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 379 \tLoss: tensor(1.6947, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 380 \tLoss: tensor(1.7736, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 381 \tLoss: tensor(1.5962, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 382 \tLoss: tensor(1.7496, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 383 \tLoss: tensor(1.6425, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 384 \tLoss: tensor(1.5865, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 385 \tLoss: tensor(1.6487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 386 \tLoss: tensor(1.7150, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 387 \tLoss: tensor(1.7186, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 388 \tLoss: tensor(1.7981, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 389 \tLoss: tensor(1.6116, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 390 \tLoss: tensor(1.6486, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 391 \tLoss: tensor(1.6487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 392 \tLoss: tensor(1.4650, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 393 \tLoss: tensor(1.6111, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 394 \tLoss: tensor(1.6057, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 395 \tLoss: tensor(1.7108, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 396 \tLoss: tensor(1.6667, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 397 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 398 \tLoss: tensor(1.7715, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 399 \tLoss: tensor(1.6485, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 400 \tLoss: tensor(1.7752, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 401 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 402 \tLoss: tensor(1.8944, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 403 \tLoss: tensor(1.6100, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 404 \tLoss: tensor(1.6996, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 405 \tLoss: tensor(1.5406, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 406 \tLoss: tensor(1.7721, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 407 \tLoss: tensor(1.7741, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 408 \tLoss: tensor(1.7898, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 409 \tLoss: tensor(1.5395, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 410 \tLoss: tensor(1.7118, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 411 \tLoss: tensor(1.7092, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 412 \tLoss: tensor(1.7106, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 413 \tLoss: tensor(1.7791, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 414 \tLoss: tensor(1.8211, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 415 \tLoss: tensor(1.8798, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 416 \tLoss: tensor(1.5848, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 417 \tLoss: tensor(1.5329, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 418 \tLoss: tensor(1.7597, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 419 \tLoss: tensor(1.7155, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 420 \tLoss: tensor(1.5969, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 421 \tLoss: tensor(1.5852, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 422 \tLoss: tensor(1.6093, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 423 \tLoss: tensor(1.5293, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 424 \tLoss: tensor(1.8985, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 425 \tLoss: tensor(1.7970, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 426 \tLoss: tensor(1.5845, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 427 \tLoss: tensor(1.7091, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 428 \tLoss: tensor(1.9532, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 429 \tLoss: tensor(2.0603, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 430 \tLoss: tensor(1.7451, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 431 \tLoss: tensor(1.5883, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 432 \tLoss: tensor(1.6707, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 433 \tLoss: tensor(1.6515, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 434 \tLoss: tensor(1.5688, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 435 \tLoss: tensor(1.4658, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 436 \tLoss: tensor(1.7066, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 437 \tLoss: tensor(1.5227, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 438 \tLoss: tensor(1.5540, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 439 \tLoss: tensor(1.6510, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 440 \tLoss: tensor(1.6481, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 441 \tLoss: tensor(1.6485, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 442 \tLoss: tensor(1.4699, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 443 \tLoss: tensor(1.7627, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 444 \tLoss: tensor(1.7195, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 445 \tLoss: tensor(1.6084, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 446 \tLoss: tensor(1.8947, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 447 \tLoss: tensor(1.7734, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 448 \tLoss: tensor(1.5644, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 449 \tLoss: tensor(1.7120, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 450 \tLoss: tensor(1.6753, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 451 \tLoss: tensor(1.6484, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 452 \tLoss: tensor(1.7070, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 453 \tLoss: tensor(1.6959, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 454 \tLoss: tensor(1.7493, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 455 \tLoss: tensor(1.7062, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 456 \tLoss: tensor(1.6062, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 457 \tLoss: tensor(1.5835, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 458 \tLoss: tensor(1.8597, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 459 \tLoss: tensor(1.6797, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 460 \tLoss: tensor(1.7038, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 461 \tLoss: tensor(1.5845, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 462 \tLoss: tensor(1.6228, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 463 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 464 \tLoss: tensor(1.5150, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 465 \tLoss: tensor(1.6437, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 466 \tLoss: tensor(1.6481, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 467 \tLoss: tensor(1.5946, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 468 \tLoss: tensor(1.6561, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 469 \tLoss: tensor(1.5873, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 470 \tLoss: tensor(1.7340, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 471 \tLoss: tensor(1.6396, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 472 \tLoss: tensor(1.5863, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 473 \tLoss: tensor(1.5873, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 474 \tLoss: tensor(1.7660, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 475 \tLoss: tensor(1.6612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 476 \tLoss: tensor(1.7662, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 477 \tLoss: tensor(1.7108, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 478 \tLoss: tensor(1.6067, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 479 \tLoss: tensor(1.6281, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 480 \tLoss: tensor(1.5403, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 481 \tLoss: tensor(1.5862, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 482 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 483 \tLoss: tensor(1.6862, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 484 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 485 \tLoss: tensor(1.5237, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 486 \tLoss: tensor(1.7152, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 487 \tLoss: tensor(1.6010, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 488 \tLoss: tensor(1.6662, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 489 \tLoss: tensor(1.5863, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 490 \tLoss: tensor(1.7219, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 491 \tLoss: tensor(1.6515, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 492 \tLoss: tensor(1.7685, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 493 \tLoss: tensor(1.6509, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 494 \tLoss: tensor(1.7772, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 495 \tLoss: tensor(1.5211, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 496 \tLoss: tensor(1.7162, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 497 \tLoss: tensor(1.7520, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 498 \tLoss: tensor(1.6495, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 499 \tLoss: tensor(1.6710, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 500 \tLoss: tensor(1.8362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 501 \tLoss: tensor(1.6102, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 502 \tLoss: tensor(1.5866, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 503 \tLoss: tensor(1.6878, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 504 \tLoss: tensor(1.6377, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 505 \tLoss: tensor(1.8974, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 506 \tLoss: tensor(1.5978, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 507 \tLoss: tensor(1.6760, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 508 \tLoss: tensor(1.5863, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 509 \tLoss: tensor(1.5949, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 510 \tLoss: tensor(1.6764, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 511 \tLoss: tensor(1.7104, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 512 \tLoss: tensor(1.7518, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 513 \tLoss: tensor(1.8735, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 514 \tLoss: tensor(1.8960, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 515 \tLoss: tensor(1.6924, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 516 \tLoss: tensor(1.7644, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 517 \tLoss: tensor(1.9389, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 518 \tLoss: tensor(1.6808, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 519 \tLoss: tensor(1.7601, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 520 \tLoss: tensor(1.7227, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 521 \tLoss: tensor(1.6904, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 522 \tLoss: tensor(1.9487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 523 \tLoss: tensor(1.5708, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 524 \tLoss: tensor(1.7299, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 525 \tLoss: tensor(1.7188, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 526 \tLoss: tensor(1.7056, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 527 \tLoss: tensor(1.4616, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 528 \tLoss: tensor(1.8003, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 529 \tLoss: tensor(1.9334, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 530 \tLoss: tensor(1.6215, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 531 \tLoss: tensor(1.6495, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 532 \tLoss: tensor(1.6472, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 533 \tLoss: tensor(1.5557, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 534 \tLoss: tensor(1.5930, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 535 \tLoss: tensor(1.7078, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 536 \tLoss: tensor(1.4719, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 537 \tLoss: tensor(1.5254, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 538 \tLoss: tensor(1.7061, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 539 \tLoss: tensor(1.6473, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 540 \tLoss: tensor(1.7354, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 541 \tLoss: tensor(1.7728, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 542 \tLoss: tensor(1.8985, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 543 \tLoss: tensor(1.6999, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 544 \tLoss: tensor(1.8329, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 545 \tLoss: tensor(1.7150, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 546 \tLoss: tensor(1.5879, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 547 \tLoss: tensor(1.6478, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 548 \tLoss: tensor(1.7001, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 549 \tLoss: tensor(1.9155, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 550 \tLoss: tensor(1.9337, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 551 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 552 \tLoss: tensor(1.6391, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 553 \tLoss: tensor(1.5706, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 554 \tLoss: tensor(1.5925, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 555 \tLoss: tensor(1.6659, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 556 \tLoss: tensor(1.5325, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 557 \tLoss: tensor(1.5944, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 558 \tLoss: tensor(1.6488, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 559 \tLoss: tensor(1.7588, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 560 \tLoss: tensor(1.8529, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 561 \tLoss: tensor(1.6511, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 562 \tLoss: tensor(1.7101, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 563 \tLoss: tensor(1.7991, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 564 \tLoss: tensor(1.8460, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 565 \tLoss: tensor(1.7631, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 566 \tLoss: tensor(1.7427, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 567 \tLoss: tensor(1.6435, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 568 \tLoss: tensor(1.4721, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 569 \tLoss: tensor(1.7174, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 570 \tLoss: tensor(1.6776, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 571 \tLoss: tensor(1.4617, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 572 \tLoss: tensor(1.7093, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 573 \tLoss: tensor(1.8301, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 574 \tLoss: tensor(1.6496, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 575 \tLoss: tensor(1.5838, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 576 \tLoss: tensor(1.7093, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 577 \tLoss: tensor(1.7601, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 578 \tLoss: tensor(1.7106, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 579 \tLoss: tensor(1.7106, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 580 \tLoss: tensor(1.7080, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 581 \tLoss: tensor(1.8408, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 582 \tLoss: tensor(1.8484, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 583 \tLoss: tensor(1.7050, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 584 \tLoss: tensor(2.0230, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 585 \tLoss: tensor(1.6759, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 586 \tLoss: tensor(1.7362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 587 \tLoss: tensor(1.6453, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 588 \tLoss: tensor(1.5634, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 589 \tLoss: tensor(1.6509, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 590 \tLoss: tensor(1.6260, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 591 \tLoss: tensor(1.6487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 592 \tLoss: tensor(1.6519, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 593 \tLoss: tensor(1.5561, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 594 \tLoss: tensor(1.5071, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 595 \tLoss: tensor(1.5803, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 596 \tLoss: tensor(1.6036, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 597 \tLoss: tensor(1.5875, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 598 \tLoss: tensor(1.6060, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 599 \tLoss: tensor(1.7095, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 600 \tLoss: tensor(1.5075, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 601 \tLoss: tensor(1.5860, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 602 \tLoss: tensor(1.6349, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 603 \tLoss: tensor(1.7464, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 604 \tLoss: tensor(1.5831, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 605 \tLoss: tensor(1.5266, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 606 \tLoss: tensor(1.5898, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 607 \tLoss: tensor(1.8363, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 608 \tLoss: tensor(1.5819, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 609 \tLoss: tensor(1.7861, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 610 \tLoss: tensor(1.5954, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 611 \tLoss: tensor(1.6443, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 612 \tLoss: tensor(1.5950, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 613 \tLoss: tensor(1.5892, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 614 \tLoss: tensor(1.7635, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 615 \tLoss: tensor(1.7604, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 616 \tLoss: tensor(1.8208, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 617 \tLoss: tensor(1.5778, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 618 \tLoss: tensor(1.5862, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 619 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 620 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 621 \tLoss: tensor(1.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 622 \tLoss: tensor(1.5854, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 623 \tLoss: tensor(1.5521, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 624 \tLoss: tensor(1.5812, grad_fn=<NllLossBackward>)\n"
    }
   ],
   "source": [
    "# GET DATALOADERS\n",
    "test_loader, val_loader, train_loader = get_dataloaders()\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='../../runs')                            # we will use this to show our models performance on a graph\n",
    "\n",
    "# TRAINING LOOP\n",
    "def train(model, optimiser, epochs=1, tag='Loss/Train'):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "train(my_nn, optimiser)"
   ]
  },
  {
   "source": [
    "Let's compare the training curves generated using some of the optimisers that we explained above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Exception in thread Thread-12:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n    self.run()\n  File \"/home/ice/.local/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 238, in run\n    self._record_writer.write(data)\n  File \"/home/ice/.local/lib/python3.8/site-packages/tensorboard/summary/writer/record_writer.py\", line 40, in write\n    self._writer.write(header + header_crc + data + footer_crc)\n  File \"/home/ice/.local/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 535, in write\n    self.fs.append(self.filename, file_content, self.binary_mode)\n  File \"/home/ice/.local/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 158, in append\n    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n  File \"/home/ice/.local/lib/python3.8/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 162, in _write\n    with io.open(filename, mode, encoding=encoding) as f:\nFileNotFoundError: [Errno 2] No such file or directory: b'../../runs/events.out.tfevents.1601494414.TB1.36926.8'\nEpoch: 0 \tBatch: 0 \tLoss: tensor(2.3025, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 1 \tLoss: tensor(2.3026, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 2 \tLoss: tensor(2.3026, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 3 \tLoss: tensor(2.3026, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 4 \tLoss: tensor(2.3040, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 5 \tLoss: tensor(2.3031, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 6 \tLoss: tensor(2.3026, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 7 \tLoss: tensor(2.3027, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 8 \tLoss: tensor(2.3031, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 9 \tLoss: tensor(2.3024, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 10 \tLoss: tensor(2.3028, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 11 \tLoss: tensor(2.3028, grad_fn=<NllLossBackward>)\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cbfb5410f7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Loss/Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-23758e3616d9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimiser, epochs, tag)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                        \u001b[0;31m# backward pass to compute and set all of the model param's gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;31m# update the model's parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# write loss to a graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_caffe2_blob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mscalar_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         self._get_file_writer().add_summary(\n\u001b[0m\u001b[1;32m    344\u001b[0m             scalar(tag, scalar_value), global_step, walltime)\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_summary\u001b[0;34m(self, summary, global_step, walltime)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_profile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_event\u001b[0;34m(self, event, step, walltime)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# since protobuf might not convert depending on version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py\u001b[0m in \u001b[0;36madd_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;34m\" but got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             )\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_async_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorboard/summary/writer/event_file_writer.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, bytestring)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Writer is closed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_byte_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, item, block, timeout)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimisers = [\n",
    "    {\n",
    "        'optimiser_class': torch.optim.SGD, \n",
    "        'tag': 'SGD'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adam,\n",
    "        'tag': 'Adam'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adagrad,\n",
    "        'tag': 'Adagrad'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.RMSprop,\n",
    "        'tag': 'RMSProp'\n",
    "    }\n",
    "]\n",
    "\n",
    "for optimiser_obj in optimisers:    \n",
    "    my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "    optimiser_class = optimiser_obj['optimiser_class']\n",
    "    optimiser = optimiser_class(my_nn.parameters(), lr=0.01)\n",
    "    tag = optimiser_obj['tag']\n",
    "    train(my_nn, optimiser, epochs=1, tag=f'Loss/Train/{tag}')\n",
    "    "
   ]
  },
  {
   "source": [
    "## Implementing our own PyTorch optimiser\n",
    "\n",
    "To understand a bit further what's happening under the hood, let's implement SGD from scratch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, model_params, learning_rate):\n",
    "        self.model_params = model_params\n",
    "        self.learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.model_params:\n",
    "            param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.model_params:\n",
    "            param.grad = torch.zeros_like(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = NN()\n",
    "optimiser = SGD()\n",
    "\n",
    "train(my_nn, optimiser)"
   ]
  }
 ]
}