{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit2b4c68a66e834f80ac62a395bc4bcb45",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Optimisation for Deep Learning\n",
    "\n",
    "Learning outcomes\n",
    "- understand mathematically and intuitively the most common optimisation algorithms used for optimising deep models\n",
    "- implement your own optimiser in PyTorch\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient descent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SGD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## AdaGrad\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RMSProp\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Adam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## So which algorithm do I use?\n",
    "\n",
    "Well... as usual, it depends on your problem and your dataset.\n",
    "\n",
    "It's still a highly active field of research. But in general, **SGD with momentum or Adam** are the go to choices for optimising deep models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using these optimisation algorithms\n",
    "\n",
    "Let's set up the same neural network as in the previous module, and then switch out the optimiser for Adam and others and show how you can adapt it to use momentum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import NN, get_dataloaders\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# HOW TO USE DIFFERENT OPTIMISERS PROVIDED BY PYTORCH\n",
    "optimiser = torch.optim.SGD(my_nn.parameters(), lr=learning_rate, momentum=0.1)\n",
    "# optimiser = torch.optim.Adagrad(NN.parameters(), lr=learning_rate)\n",
    "# optimiser = torch.optim.RMSprop(NN.parameters(), lr=learning_rate)\n",
    "optimiser = torch.optim.Adam(my_nn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "source": [
    "The stuff below is exactly the same as before!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 0 \tBatch: 0 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 1 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 2 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 3 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 4 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 5 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 6 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 7 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 8 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 9 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 10 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 11 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 12 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 13 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 14 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 15 \tLoss: tensor(2.1487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 16 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 17 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 18 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 19 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 20 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 21 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 22 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 23 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 24 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 25 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 26 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 27 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 28 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 29 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 30 \tLoss: tensor(2.2111, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 31 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 32 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 33 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 34 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 35 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 36 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 37 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 38 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 39 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 40 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 41 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 42 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 43 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 44 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 45 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 46 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 47 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 48 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 49 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 50 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 51 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 52 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 53 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 54 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 55 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 56 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 57 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 58 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 59 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 60 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 61 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 62 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 63 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 64 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 65 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 66 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 67 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 68 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 69 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 70 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 71 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 72 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 73 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 74 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 75 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 76 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 77 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 78 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 79 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 80 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 81 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 82 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 83 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 84 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 85 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 86 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 87 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 88 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 89 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 90 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 91 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 92 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 93 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 94 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 95 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 96 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 97 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 98 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 99 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 100 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 101 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 102 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 103 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 104 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 105 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 106 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 107 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 108 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 109 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 110 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 111 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 112 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 113 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 114 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 115 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 116 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 117 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 118 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 119 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 120 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 121 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 122 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d5053585c56e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# write loss to a graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-d5053585c56e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimiser, epochs, tag)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'../../runs/{tag}'\u001b[0m\u001b[0;34m)\u001b[0m                            \u001b[0;31m# we will use this to show our models performance on a graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# pass the data forward through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RGBA\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcffi\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mUSE_CFFI_ACCESS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyaccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GET DATALOADERS\n",
    "test_loader, val_loader, train_loader = get_dataloaders()\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TRAINING LOOP\n",
    "def train(model, optimiser, epochs=1, tag='Loss/Train'):\n",
    "    writer = SummaryWriter(log_dir=f'../../runs/{tag}')                            # we will use this to show our models performance on a graph\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar('Loss', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "train(my_nn, optimiser)"
   ]
  },
  {
   "source": [
    "Let's compare the training curves generated using some of the optimisers that we explained above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 340 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 341 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 342 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 343 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 344 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 345 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 346 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 347 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 348 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 349 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 350 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 351 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 352 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 353 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 354 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 355 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 356 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 357 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 358 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 359 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 360 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 361 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 362 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 363 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 364 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 365 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 366 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 367 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 368 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 369 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 370 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 371 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 372 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 373 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 374 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 375 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 376 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 377 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 378 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 379 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 380 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 381 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 382 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 383 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 384 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 385 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 386 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 387 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 388 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 389 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 390 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 391 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 392 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 393 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 394 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 395 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 396 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 397 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 398 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 399 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 400 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 401 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 402 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 403 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 404 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 405 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 406 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 407 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 408 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 409 \tLoss: tensor(2.1487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 410 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 411 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 412 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 413 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 414 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 415 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 416 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 417 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 418 \tLoss: tensor(2.2111, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 419 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 420 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 421 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 422 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 423 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 424 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 425 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 426 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 427 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 428 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 429 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 430 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 431 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 432 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 433 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 434 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 435 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 436 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 437 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 438 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 439 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 440 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 441 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 442 \tLoss: tensor(2.1487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 443 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 444 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 445 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 446 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 447 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 448 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 449 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 450 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 451 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 452 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 453 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 454 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 455 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 456 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 457 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 458 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 459 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 460 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 461 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 462 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 463 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 464 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 465 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 466 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 467 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 468 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 469 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 470 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 471 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 472 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 473 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 474 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 475 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 476 \tLoss: tensor(2.1487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 477 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 478 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 479 \tLoss: tensor(2.1487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 480 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 481 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 482 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 483 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 484 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 485 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 486 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 487 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 488 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 489 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 490 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 491 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 492 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 493 \tLoss: tensor(2.2111, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 494 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 495 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 496 \tLoss: tensor(2.1487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 497 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 498 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 499 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 500 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 501 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 502 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 503 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 504 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 505 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 506 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 507 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 508 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 509 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 510 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 511 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 512 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 513 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 514 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 515 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 516 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 517 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 518 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 519 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 520 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 521 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 522 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 523 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 524 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 525 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 526 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 527 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 528 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 529 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 530 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 531 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 532 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 533 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 534 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 535 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 536 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 537 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 538 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 539 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 540 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 541 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 542 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 543 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 544 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 545 \tLoss: tensor(2.1487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 546 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 547 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 548 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 549 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 550 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 551 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 552 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 553 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 554 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 555 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 556 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 557 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 558 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 559 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 560 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 561 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 562 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 563 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 564 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 565 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 566 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 567 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 568 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 569 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 570 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 571 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 572 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 573 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 574 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 575 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 576 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 577 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 578 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 579 \tLoss: tensor(2.1487, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 580 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 581 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 582 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 583 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 584 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 585 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 586 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 587 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 588 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 589 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 590 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 591 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 592 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 593 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 594 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 595 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 596 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 597 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 598 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 599 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 600 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 601 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 602 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 603 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 604 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 605 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 606 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 607 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 608 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 609 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 610 \tLoss: tensor(2.4612, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 611 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 612 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 613 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 614 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 615 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 616 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 617 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 618 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 619 \tLoss: tensor(2.2112, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 620 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 621 \tLoss: tensor(2.2737, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 622 \tLoss: tensor(2.3362, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 623 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\nEpoch: 0 \tBatch: 624 \tLoss: tensor(2.3987, grad_fn=<NllLossBackward>)\n"
    }
   ],
   "source": [
    "optimisers = [\n",
    "    {\n",
    "        'optimiser_class': torch.optim.SGD, \n",
    "        'tag': 'SGD'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adam,\n",
    "        'tag': 'Adam'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.Adagrad,\n",
    "        'tag': 'Adagrad'\n",
    "    },\n",
    "    {\n",
    "        'optimiser_class': torch.optim.RMSprop,\n",
    "        'tag': 'RMSProp'\n",
    "    }\n",
    "]\n",
    "\n",
    "for optimiser_obj in optimisers:    \n",
    "    my_nn = NN([784, 1024, 1024, 512, 10], distribution=True, flatten_input=True)\n",
    "    optimiser_class = optimiser_obj['optimiser_class']\n",
    "    optimiser = optimiser_class(my_nn.parameters(), lr=0.01)\n",
    "    tag = optimiser_obj['tag']\n",
    "    train(my_nn, optimiser, epochs=1, tag=f'Loss/Train-{tag}')\n",
    "    "
   ]
  },
  {
   "source": [
    "## Implementing our own PyTorch optimiser\n",
    "\n",
    "To understand a bit further what's happening under the hood, let's implement SGD from scratch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, model_params, learning_rate):\n",
    "        self.model_params = model_params\n",
    "        self.learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.model_params:\n",
    "            param -= self.learning_rate * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.model_params:\n",
    "            param.grad = torch.zeros_like(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn = NN()\n",
    "optimiser = SGD()\n",
    "\n",
    "train(my_nn, optimiser)"
   ]
  }
 ]
}